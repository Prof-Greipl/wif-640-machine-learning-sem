{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IML_309_Data Normalization.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prof-Greipl/wif-640-machine-learning-sem/blob/master/IML_309_Data_Normalization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JasRRcvdrj9",
        "colab_type": "text"
      },
      "source": [
        "# Our Standardizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53AWSfT08K_1",
        "colab_type": "code",
        "outputId": "629b3d89-82e6-4ef3-ee32-3b7bdda97e2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        }
      },
      "source": [
        "#added\n",
        "class standardizer:\n",
        "  def __init__(self):\n",
        "      self.row_scale_params = []\n",
        "      \n",
        "  def standardize(self, matrix):\n",
        "    N = matrix.shape[0]\n",
        "    result = np.empty( matrix.shape )    \n",
        "    # run through the lines\n",
        "    for i in range( N ):\n",
        "      row = matrix[i,:]\n",
        "\n",
        "      # Compute scaling parameters...\n",
        "      mean = np.mean(row)\n",
        "      std = np.std(row)\n",
        "\n",
        "      print(f\"Row {i} : Row = {row} Mean = {mean} Std = {std}\")\n",
        "      # Handle std = 0 case\n",
        "      if std < 1E-6:\n",
        "        std = 1\n",
        "\n",
        "      # and save\n",
        "      self.row_scale_params.append( [mean, std] )\n",
        "           \n",
        "      # add the row to the output\n",
        "      result[i, :] = np.array( (row - mean) / std  )\n",
        "\n",
        "    return result\n",
        "\n",
        "  def check(self, matrix):\n",
        "    N = matrix.shape[0]\n",
        "   \n",
        "    for i in range( N ):\n",
        "      row = matrix[i,:]\n",
        "\n",
        "      # Compute scaling parameters...\n",
        "      mean = np.mean(row)\n",
        "      std = np.std(row)\n",
        "      print(f\"Row {i} : Mean = {mean}, Std = {std}\")\n",
        "\n",
        "# Test\n",
        "X  = np.array([[1], [1]])\n",
        "X  = np.array( [[0.,1.], [0.,1.]] ) \n",
        "X  = np.array( [[0.,1., 2.0], [0.,1., 2.0]] )\n",
        "standardizer = standardizer()\n",
        "\n",
        "M = standardizer.standardize( X )\n",
        "print(f\"In \\n {X}\")\n",
        "print(f\"Out\\n {M}\")\n",
        "\n",
        "standardizer.check( M )\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-55b6d4e9e28c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# Test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mX\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0mX\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mX\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRXZ2JSFmXlI",
        "colab_type": "text"
      },
      "source": [
        "# Extend Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_27n1bKPiJf",
        "colab_type": "code",
        "outputId": "4bb3d951-4013-4d16-fcf6-2d9da3ccf333",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "class standardizer:\n",
        "  def __init__(self):\n",
        "      self.row_scale_params = []\n",
        "      \n",
        "  def standardize(self, matrix):\n",
        "    N = matrix.shape[0]\n",
        "    result = np.empty( matrix.shape )    \n",
        "    # run through the lines\n",
        "    for i in range( N ):\n",
        "      row = matrix[i,:]\n",
        "\n",
        "      # Compute scaling parameters...\n",
        "      mean = np.mean(row)\n",
        "      std = np.std(row)\n",
        "\n",
        "      print(f\"Row {i} : Row = {row} Mean = {mean} Std = {std}\")\n",
        "      # Handle std = 0 case\n",
        "      if std < 1E-6:\n",
        "        std = 1\n",
        "\n",
        "      # and save\n",
        "      self.row_scale_params.append( [mean, std] )\n",
        "           \n",
        "      # add the row to the output\n",
        "      result[i, :] = np.array( (row - mean) / std  )\n",
        "\n",
        "    return result\n",
        "\n",
        "  def check(self, matrix):\n",
        "    N = matrix.shape[0]\n",
        "   \n",
        "    for i in range( N ):\n",
        "      row = matrix[i,:]\n",
        "\n",
        "      # Compute scaling parameters...\n",
        "      mean = np.mean(row)\n",
        "      std = np.std(row)\n",
        "      print(f\"Row {i} : Mean = {mean}, Std = {std}\")\n",
        "\n",
        "def get_data( data_variant ):\n",
        "  if (data_variant == \"D2_N1_Y1\"):\n",
        "    X  = np.array([[1], [1]])\n",
        "    y_true =np.array( [[22]] )\n",
        "\n",
        "  elif (data_variant == \"D2_N2_Y1\"):\n",
        "    X  = np.array( [[0.,1.], [0.,1.]] ) \n",
        "    y_true = np.array( [[0., 7.]] )\n",
        "\n",
        "  elif (data_variant == \"D2_N3_Y1\"):\n",
        "    X  = np.array( [[0.,1., 2.0], [0.1, 0.2, 0.3]] ) \n",
        "    y_true = np.array( [[10., -11., -15.0]] )\n",
        "\n",
        "  elif (data_variant == \"D2_N3_Y1_UNSCALED\"):\n",
        "    X  = np.array( [[0.,10., 200.0], [0.,1., 2.0]] ) \n",
        "    y_true = np.array( [[10., -11., -15.0]] )\n",
        "\n",
        "  else: \n",
        "    raise Exception(f'Unkown datasource:  {data_variant}')\n",
        "\n",
        "  return(X, y_true)\n",
        "\n",
        "def activation(z, act_func):\n",
        "    global _activation\n",
        "    if act_func == 'relu':\n",
        "       return np.maximum(z, np.zeros(z.shape))\n",
        "    \n",
        "    elif act_func == 'sigmoid':\n",
        "      return 1.0/(1.0 + np.exp( -z ))\n",
        "\n",
        "    elif act_func == 'linear':\n",
        "        return z\n",
        "    else:\n",
        "        raise Exception('Activation function is not defined.')\n",
        "\n",
        "\n",
        "def get_dactivation(A, act_func):\n",
        "    if act_func == 'relu':\n",
        "        return np.maximum(np.sign(A), np.zeros(A.shape)) # 1 if backward input >0, 0 otherwise; then diaganolize\n",
        "\n",
        "    elif act_func == 'sigmoid':\n",
        "        h = activation(A, 'sigmoid')\n",
        "        return h *(1-h)\n",
        "\n",
        "    elif act_func == 'linear':\n",
        "        return np.ones(A.shape)\n",
        "\n",
        "    else:\n",
        "        raise Exception('Activation function is not defined.')\n",
        "\n",
        "def loss(y_true, y_predicted, loss_function='mse'):\n",
        "   if loss_function == 'mse':\n",
        "      return metrics.mean_squared_error( y_true, y_predicted)\n",
        "   else:\n",
        "      raise Exception('Loss metric is not defined.')\n",
        "\n",
        "\n",
        "def get_dZ_from_loss(y, y_predicted, metric):\n",
        "    if metric == 'mse':\n",
        "        return y_predicted - y\n",
        "    else:\n",
        "        raise Exception('Loss metric is not defined.')\n",
        "           \n",
        "class layer:\n",
        "  def __init__(self,input_dim, output_dim, activation='relu'):    \n",
        "    self.activation = activation\n",
        "    self.input_dim = input_dim\n",
        "    self.output_dim = output_dim # is this needed?? TODO\n",
        "    if input_dim > 0:\n",
        "      self.b = np.ones( (output_dim,1) )       \n",
        "      self.W = np.ones( (output_dim, input_dim) )\n",
        "      #self.W = np.random.randn(output_dim, input_dim) * np.sqrt(2/input_dim) \n",
        "    \n",
        "    self.A = np.zeros( (output_dim,1) ) # added: we temp. store for A\n",
        "  \n",
        "  def setWeight(self, W ):\n",
        "    self.W = W\n",
        "    \n",
        "  def setBias(self, b ):\n",
        "    self.b = b\n",
        "    \n",
        "  def setActivation(self, A ): \n",
        "    self.Z =  np.add( np.dot(self.W, A), self.b)\n",
        "    self.A =  activation(self.Z, self.activation)\n",
        "  \n",
        "  \n",
        "  def print(self, layer_name=\"\"):\n",
        "    print(f\"Layer {layer_name}: Begin of Summary\")\n",
        "    if self.input_dim > 0:\n",
        "      print(f\"Layer {layer_name}: input_dim = {self.input_dim}\")\n",
        "      print(f\"Layer {layer_name}: output_dim = {self.output_dim}\")\n",
        "      print(f\"Layer {layer_name}: Activation = {self.activation}\")\n",
        "      print(f\"W = \")\n",
        "      print(self.W)\n",
        "      print(f\"A = \")\n",
        "      print(self.A)\n",
        "      print(f\"b = \")\n",
        "      print(self.b)\n",
        "    else:\n",
        "      print(f\"Layer {layer_name}: This is an input layer..... \")\n",
        "      print(f\"A = \")\n",
        "      print(self.A)\n",
        "  \n",
        "    print(f\"Layer {layer_name}: End of Summary\")\n",
        "  \n",
        "\n",
        "class ModelNet:\n",
        "  def __init__(self, input_dim):  \n",
        "    self.history = []\n",
        "    self.neural_net = []\n",
        "    self.neural_net.append(layer(0 , input_dim, 'irrelevant'))\n",
        "    \n",
        "\n",
        "  def addLayer(self, nr_neurons, activation='relu'):    \n",
        "    layer_index = len(self.neural_net)\n",
        "    input_dim = self.neural_net[layer_index - 1].output_dim\n",
        "    new_layer = layer( input_dim, nr_neurons, activation)\n",
        "    self.neural_net.append( new_layer )\n",
        "    \n",
        "  #added  \n",
        "  def get_history(self):\n",
        "     return pd.DataFrame(\n",
        "         self.history, \n",
        "         columns=['epoch', 'loss']\n",
        "     )         \n",
        "\n",
        "  def forward_propagation(self, input_vec ):\n",
        "    self.neural_net[0].A = input_vec\n",
        "    for layer_index in range(1,len(self.neural_net)):    \n",
        "      _A_Prev = self.neural_net[layer_index-1].A                       \n",
        "      self.neural_net[layer_index].setActivation( _A_Prev )\n",
        "      \n",
        "    return  self.neural_net[layer_index].A\n",
        "    \n",
        "    \n",
        "  def fit(self, input_vec, y_true, max_epoch, early_stop=1, learning_rate=0.01, verbose=1 ):\n",
        "    print(f\"Start training for input_vec:\")\n",
        "    print( input_vec)\n",
        "    print(f\"Feature set entries: {input_vec.shape[1]}\")\n",
        "    \n",
        "    self.learning_rate = learning_rate\n",
        "    self.history = []  # Reset History Array\n",
        "    num_train_datum = input_vec.shape[1]\n",
        "    global _activation\n",
        "    _activation = 0\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(1,max_epoch+1): \n",
        "\n",
        "      # Generate y_predicted\n",
        "      y_predicted = self.forward_propagation( input_vec )\n",
        "\n",
        "      # Do Backpropagation\n",
        "      bp_verbose = verbose - 1\n",
        "      if (epoch == 1000):\n",
        "        bp_verbose=1\n",
        "        self.summary()\n",
        "\n",
        "      self.backward_propagation( y_true, y_predicted,  num_train_datum, verbose = bp_verbose )\n",
        "\n",
        "      #calculate intermediate loss\n",
        "      cost = loss(y_true, y_predicted)\n",
        "\n",
        "      # Update history\n",
        "      self.history.append([epoch, cost])\n",
        "\n",
        "      # Update the weights an biases\n",
        "      self.update( learning_rate )\n",
        "\n",
        "      # added: early stopping\n",
        "      if (epoch > 7)  and (early_stop == 1):\n",
        "        actual_loss = self.history[epoch-1][1] # epochs start with one!\n",
        "        past_loss   = self.history[epoch-6][1] \n",
        "        if (abs(actual_loss - past_loss)) < 1E-3:\n",
        "          print(f\"Early stop in after epoch {epoch} with loss  {actual_loss}\")\n",
        "          print(f\"   Prev Loss ({epoch-5}) : {past_loss} [Delta: { abs(actual_loss-past_loss) }]\")\n",
        "\n",
        "          break\n",
        "      \n",
        "\n",
        "      if (verbose > 0):\n",
        "        #print(f\"Epoch {epoch}: Y-True = {y_true}\")\n",
        "        #print(f\"Epoch {epoch}: Y-Pred = {y_predicted}\")\n",
        "        print(f\"Epoch {epoch}: Loss   = { cost }\")\n",
        "        #print(f\"Epoch {epoch}: Finished\")\n",
        "\n",
        "    print(f\"Epoch {epoch}: Y-True = {y_true}\")\n",
        "    print(f\"Epoch {epoch}: Y-Pred = {y_predicted}\")\n",
        "    print(f\"Epoch {epoch}: Loss = {loss(y_true, y_predicted)}\")    \n",
        "    print(f\"Epoch {epoch}: Finished\")\n",
        "      \n",
        "      \n",
        "  def backward_propagation(self, y, y_predicted, num_train_datum, metric='mse', verbose=0):   \n",
        "    nr_layers = len(self.neural_net)\n",
        "    for layer_index in range(nr_layers-1,0,-1):\n",
        "        if layer_index+1 == nr_layers: # if output layer\n",
        "            dZ = get_dZ_from_loss(y, y_predicted, metric)\n",
        "        else: \n",
        "            dZ = np.multiply(\n",
        "                   np.dot(\n",
        "                       self.neural_net[layer_index+1].W.T, \n",
        "                       dZ), \n",
        "                   get_dactivation(\n",
        "                         self.neural_net[layer_index].A, \n",
        "                         self.neural_net[layer_index].activation)\n",
        "                   )\n",
        "           \n",
        "        \n",
        "        dW = np.dot(dZ, self.neural_net[layer_index-1].A.T) / num_train_datum\n",
        "        db = np.sum(dZ, axis=1, keepdims=True) / num_train_datum\n",
        "        \n",
        "        self.neural_net[layer_index].dW = dW\n",
        "        self.neural_net[layer_index].db = db\n",
        "        if (verbose > 0):\n",
        "          print(f\"\\n\\n====== Backward Propagation Layer {layer_index} =======\")\n",
        "          print(f\"dZ      =  {dZ}\")          \n",
        "          print(f\"dW      =  {dW}\")\n",
        "          print(f\"A-1     = {self.neural_net[layer_index-1].A}\") \n",
        "          print(f\"\\nb     =  {db}\")\n",
        "             \n",
        "  # added\n",
        "  def update( self, learning_rate ):\n",
        "    nr_layers = len(self.neural_net)\n",
        "    for layer_index in range(1,nr_layers):        # update (W,b)\n",
        "      self.neural_net[layer_index].W = self.neural_net[layer_index].W - learning_rate * self.neural_net[layer_index].dW  \n",
        "      self.neural_net[layer_index].b = self.neural_net[layer_index].b - learning_rate * self.neural_net[layer_index].db\n",
        "\n",
        "  def summary(self):\n",
        "      print(\"MODEL SUMMARY\")\n",
        "      for layer_index in range(len(self.neural_net)):        \n",
        "        self.neural_net[layer_index].print(layer_index)\n",
        "        \n",
        "      print(\"FINISHED MODEL SUMMARY\")\n",
        "      \n",
        "        \n",
        "#Testing---------------------------------   \n",
        "_activation = 0     \n",
        "input_dim = 2\n",
        "output_dim = 1\n",
        "model = ModelNet( input_dim )\n",
        "model.addLayer( 2, 'relu' )\n",
        "model.addLayer( output_dim, 'linear' )\n",
        "\n",
        "(X, y_true ) = get_data( \"D2_N3_Y1_UNSCALED\")\n",
        "\n",
        "\n",
        "max_epoch = 400\n",
        "learning_rate = 0.01\n",
        "\n",
        "print(\"Training with scaled data----------------\")\n",
        "standardizer = standardizer()\n",
        "X_S = standardizer.standardize( X )\n",
        "model.fit( X_S, y_true, max_epoch, learning_rate = learning_rate , verbose=0)\n",
        "history_scaled = model.get_history()\n",
        "\n",
        "print(\"Training with unscaled data----------------\")\n",
        "model.fit( X, y_true, max_epoch, learning_rate = learning_rate , early_stop = 0, verbose=0)\n",
        "history_unscaled = model.get_history()\n",
        "\n",
        "#Plotting\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Mean Square Error [MSE]')\n",
        "plt.plot(history_scaled['epoch'], history_scaled['loss'], label='Loss (Scaled Data)')\n",
        "plt.plot(history_unscaled['loss'], label='Loss (Unscaled Data)')\n",
        "#plt.ylim([0,50])  \n",
        "plt.legend()\n",
        "plt.show()\n",
        "      \n",
        "print(\"Unscaled History \")\n",
        "display ( history_unscaled.head ( 10 ))\n",
        "print(\"Scaled History \")\n",
        "display ( history_scaled.head ( 10 ))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training with scaled data----------------\n",
            "Row 0 : Row = [  0.  10. 200.] Mean = 70.0 Std = 92.01449161228173\n",
            "Row 1 : Row = [0. 1. 2.] Mean = 1.0 Std = 0.816496580927726\n",
            "Standardized X \n",
            " [[-0.76074973 -0.6520712   1.41282093]\n",
            " [-1.22474487  0.          1.22474487]]\n",
            "Start training for input_vec:\n",
            "[[-0.76074973 -0.6520712   1.41282093]\n",
            " [-1.22474487  0.          1.22474487]]\n",
            "Feature set entries: 3\n",
            "Epoch 400: Y-True = [[ 10. -11. -15.]]\n",
            "Epoch 400: Y-Pred = [[  6.79732779 -10.82027108 -15.02119763]]\n",
            "Epoch 400: Loss = 3.4299537097746353\n",
            "Epoch 400: Finished\n",
            "Activations : 0\n",
            "Training with unscaled data----------------\n",
            "Start training for input_vec:\n",
            "[[  0.  10. 200.]\n",
            " [  0.   1.   2.]]\n",
            "Feature set entries: 3\n",
            "Epoch 400: Y-True = [[ 10. -11. -15.]]\n",
            "Epoch 400: Y-Pred = [[-5.10499906 -5.10499906 -5.10499906]]\n",
            "Epoch 400: Loss = 120.27435876382555\n",
            "Epoch 400: Finished\n",
            "Activations : 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIIwxxkRkCcIYY0xEliCMMcZE9P8BrGNmxE2DlYEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Unscaled History \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>epoch</th>\n",
              "      <th>loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>352.317378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>335.650833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>322.582179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>311.962673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>303.073162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>295.443755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>288.756410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>282.789390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>277.384141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>272.424783</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   epoch        loss\n",
              "0      1  352.317378\n",
              "1      2  335.650833\n",
              "2      3  322.582179\n",
              "3      4  311.962673\n",
              "4      5  303.073162\n",
              "5      6  295.443755\n",
              "6      7  288.756410\n",
              "7      8  282.789390\n",
              "8      9  277.384141\n",
              "9     10  272.424783"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Scaled History \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>epoch</th>\n",
              "      <th>loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>261.305517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>218.511338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>192.920370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>175.429138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>162.183713</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>151.266530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>141.626628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>132.664414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>124.050052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>115.635494</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   epoch        loss\n",
              "0      1  261.305517\n",
              "1      2  218.511338\n",
              "2      3  192.920370\n",
              "3      4  175.429138\n",
              "4      5  162.183713\n",
              "5      6  151.266530\n",
              "6      7  141.626628\n",
              "7      8  132.664414\n",
              "8      9  124.050052\n",
              "9     10  115.635494"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}