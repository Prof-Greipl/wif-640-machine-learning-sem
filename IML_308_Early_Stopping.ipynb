{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IML_308_Early Stopping.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prof-Greipl/wif-640-machine-learning-sem/blob/master/IML_308_Early_Stopping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRXZ2JSFmXlI",
        "colab_type": "text"
      },
      "source": [
        "# Early Stopping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_27n1bKPiJf",
        "colab_type": "code",
        "outputId": "9e8a2e1f-961a-4e50-e5cd-a182c35656cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "def activation(z, act_func):\n",
        "    global _activation\n",
        "    if act_func == 'relu':\n",
        "       return np.maximum(z, np.zeros(z.shape))\n",
        "    \n",
        "    elif act_func == 'sigmoid':\n",
        "      return 1.0/(1.0 + np.exp( -z ))\n",
        "\n",
        "    elif act_func == 'linear':\n",
        "        return z\n",
        "    else:\n",
        "        raise Exception('Activation function is not defined.')\n",
        "\n",
        "\n",
        "def get_dactivation(A, act_func):\n",
        "    if act_func == 'relu':\n",
        "        return np.maximum(np.sign(A), np.zeros(A.shape)) # 1 if backward input >0, 0 otherwise; then diaganolize\n",
        "\n",
        "    elif act_func == 'sigmoid':\n",
        "        h = activation(A, 'sigmoid')\n",
        "        return h *(1-h)\n",
        "\n",
        "    elif act_func == 'linear':\n",
        "        return np.ones(A.shape)\n",
        "\n",
        "    else:\n",
        "        raise Exception('Activation function is not defined.')\n",
        "\n",
        "def loss(y_true, y_predicted, loss_function='mse'):\n",
        "   if loss_function == 'mse':\n",
        "      return metrics.mean_squared_error( y_true, y_predicted)\n",
        "   else:\n",
        "      raise Exception('Loss metric is not defined.')\n",
        "\n",
        "\n",
        "def get_dZ_from_loss(y, y_predicted, metric):\n",
        "    if metric == 'mse':\n",
        "        return y_predicted - y\n",
        "    else:\n",
        "        raise Exception('Loss metric is not defined.')\n",
        "\n",
        "        \n",
        "class layer:\n",
        "  def __init__(self,input_dim, output_dim, activation='relu'):    \n",
        "    self.activation = activation\n",
        "    self.input_dim = input_dim\n",
        "    self.output_dim = output_dim # is this needed?? TODO\n",
        "    if input_dim > 0:\n",
        "      self.b = np.ones( (output_dim,1) )       \n",
        "      self.W = np.ones( (output_dim, input_dim) )\n",
        "      #self.W = np.random.randn(output_dim, input_dim) * np.sqrt(2/input_dim) \n",
        "    \n",
        "    self.A = np.zeros( (output_dim,1) ) # added: we temp. store for A\n",
        "  \n",
        "  def setWeight(self, W ):\n",
        "    self.W = W\n",
        "    \n",
        "  def setBias(self, b ):\n",
        "    self.b = b\n",
        "    \n",
        "  def setActivation(self, A ): \n",
        "    self.Z =  np.add( np.dot(self.W, A), self.b)\n",
        "    self.A =  activation(self.Z, self.activation)\n",
        "  \n",
        "  \n",
        "  def print(self, layer_name=\"\"):\n",
        "    print(f\"Layer {layer_name}: Begin of Summary\")\n",
        "    if self.input_dim > 0:\n",
        "      print(f\"Layer {layer_name}: input_dim = {self.input_dim}\")\n",
        "      print(f\"Layer {layer_name}: output_dim = {self.output_dim}\")\n",
        "      print(f\"Layer {layer_name}: Activation = {self.activation}\")\n",
        "      print(f\"W = \")\n",
        "      print(self.W)\n",
        "      print(f\"A = \")\n",
        "      print(self.A)\n",
        "      print(f\"b = \")\n",
        "      print(self.b)\n",
        "    else:\n",
        "      print(f\"Layer {layer_name}: This is an input layer..... \")\n",
        "      print(f\"A = \")\n",
        "      print(self.A)\n",
        "  \n",
        "    print(f\"Layer {layer_name}: End of Summary\")\n",
        "  \n",
        "\n",
        "class ModelNet:\n",
        "  def __init__(self, input_dim):  \n",
        "    self.history = []\n",
        "    self.neural_net = []\n",
        "    self.neural_net.append(layer(0 , input_dim, 'irrelevant'))\n",
        "    \n",
        "\n",
        "  def addLayer(self, nr_neurons, activation='relu'):    \n",
        "    layer_index = len(self.neural_net)\n",
        "    input_dim = self.neural_net[layer_index - 1].output_dim\n",
        "    new_layer = layer( input_dim, nr_neurons, activation)\n",
        "    self.neural_net.append( new_layer )\n",
        "    \n",
        "  #added  \n",
        "  def get_history(self):\n",
        "     return pd.DataFrame(\n",
        "         self.history, \n",
        "         columns=['epoch', 'loss']\n",
        "     )         \n",
        "\n",
        "  def forward_propagation(self, input_vec ):\n",
        "    self.neural_net[0].A = input_vec\n",
        "    for layer_index in range(1,len(self.neural_net)):    \n",
        "      _A_Prev = self.neural_net[layer_index-1].A                       \n",
        "      self.neural_net[layer_index].setActivation( _A_Prev )\n",
        "      \n",
        "    return  self.neural_net[layer_index].A\n",
        "    \n",
        "    \n",
        "  def fit(self, input_vec, y_true, max_epoch, learning_rate=0.01, verbose=1 ):\n",
        "    print(f\"Start training for input_vec:\")\n",
        "    print( input_vec)\n",
        "    print(f\"Feature set entries: {input_vec.shape[1]}\")\n",
        "    \n",
        "    self.learning_rate = learning_rate\n",
        "    self.history = []  # Reset History Array\n",
        "    num_train_datum = input_vec.shape[1]\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(1,max_epoch+1): \n",
        "\n",
        "      # Generate y_predicted\n",
        "      y_predicted = self.forward_propagation( input_vec )\n",
        "\n",
        "      # Do Backpropagation\n",
        "      self.backward_propagation( y_true, y_predicted,  num_train_datum, verbose = verbose - 1 )\n",
        "\n",
        "      #calculate intermediate loss\n",
        "      cost = loss(y_true, y_predicted)\n",
        "\n",
        "      # Update history\n",
        "      self.history.append([epoch, cost])\n",
        "\n",
        "      # Update the weights an biases\n",
        "      self.update( learning_rate )\n",
        "\n",
        "      # added: early stopping\n",
        "      if epoch > 7:\n",
        "        actual_loss = self.history[epoch-1][1] # epochs start with one!\n",
        "        past_loss   = self.history[epoch-6][1] \n",
        "        if (abs(actual_loss - past_loss)) < 1E-3:\n",
        "          print(f\"Early stop in after epoch {epoch} with loss  {actual_loss}\")\n",
        "          print(f\"   Prev Loss ({epoch-5}) : {past_loss} [Delta: { abs(actual_loss-past_loss) }]\")\n",
        "\n",
        "          break\n",
        "      \n",
        "\n",
        "      if (verbose > 0):\n",
        "        #print(f\"Epoch {epoch}: Y-True = {y_true}\")\n",
        "        #print(f\"Epoch {epoch}: Y-Pred = {y_predicted}\")\n",
        "        print(f\"Epoch {epoch}: Loss   = { cost }\")\n",
        "        #print(f\"Epoch {epoch}: Finished\")\n",
        "\n",
        "    print(f\"Epoch {epoch}: Y-True = {y_true}\")\n",
        "    print(f\"Epoch {epoch}: Y-Pred = {y_predicted}\")\n",
        "    print(f\"Epoch {epoch}: Loss = {loss(y_true, y_predicted)}\")    \n",
        "    print(f\"Epoch {epoch}: Finished\")\n",
        "      \n",
        "      \n",
        "  def backward_propagation(self, y, y_predicted, num_train_datum, metric='mse', verbose=0):\n",
        "    nr_layers = len(self.neural_net)\n",
        "    for layer_index in range(nr_layers-1,0,-1):\n",
        "        if layer_index+1 == nr_layers: # if output layer\n",
        "            dZ = get_dZ_from_loss(y, y_predicted, metric)\n",
        "        else: \n",
        "            dZ = np.multiply(\n",
        "                   np.dot(\n",
        "                       self.neural_net[layer_index+1].W.T, \n",
        "                       dZ), \n",
        "                   get_dactivation(\n",
        "                         self.neural_net[layer_index].A, \n",
        "                         self.neural_net[layer_index].activation)\n",
        "                   )\n",
        "           \n",
        "        \n",
        "        dW = np.dot(dZ, self.neural_net[layer_index-1].A.T) / num_train_datum\n",
        "        db = np.sum(dZ, axis=1, keepdims=True) / num_train_datum\n",
        "        \n",
        "        self.neural_net[layer_index].dW = dW\n",
        "        self.neural_net[layer_index].db = db\n",
        "        if (verbose > 0):\n",
        "          print(f\"\\n\\n====== Backward Propagation Layer {layer_index} =======\")\n",
        "          print(f\"dZ      =  {dZ}\")          \n",
        "          print(f\"dW      =  {dW}\")\n",
        "          print(f\"A-1     = {self.neural_net[layer_index-1].A}\") \n",
        "          print(f\"\\nb     =  {db}\")\n",
        "             \n",
        "  # added\n",
        "  def update( self, learning_rate ):\n",
        "    nr_layers = len(self.neural_net)\n",
        "    for layer_index in range(1,nr_layers):        # update (W,b)\n",
        "      self.neural_net[layer_index].W = self.neural_net[layer_index].W - learning_rate * self.neural_net[layer_index].dW  \n",
        "      self.neural_net[layer_index].b = self.neural_net[layer_index].b - learning_rate * self.neural_net[layer_index].db\n",
        "\n",
        "  def summary(self):\n",
        "      print(\"MODEL SUMMARY\")\n",
        "      for layer_index in range(len(self.neural_net)):        \n",
        "        self.neural_net[layer_index].print(layer_index)\n",
        "        \n",
        "      print(\"FINISHED MODEL SUMMARY\")\n",
        "      \n",
        "        \n",
        "#Testing---------------------------------        \n",
        "input_dim = 2\n",
        "output_dim = 1\n",
        "model = ModelNet( input_dim )\n",
        "model.addLayer( 2, 'relu' )\n",
        "model.addLayer( output_dim, 'linear' )\n",
        "\n",
        "# Play with different feature set lengths..\n",
        "# N=1\n",
        "#X  = np.array([[1], [1]])\n",
        "#y_true =np.array( [[22]] )\n",
        "\n",
        "# N=2\n",
        "#X  = np.array( [[0.,1.], [0.,1.]] ) \n",
        "#y_true = np.array( [[0., 7.]] )\n",
        "\n",
        "\n",
        "# N=3\n",
        "X  = np.array( [[0.,1., 2.0], [0.,1., 2.0]] ) \n",
        "y_true = np.array( [[-10., -11., -15.0]] )\n",
        "\n",
        "\n",
        "\n",
        "model.fit( X, y_true, 1000, verbose=1)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start training for input_vec:\n",
            "[[0. 1. 2.]\n",
            " [0. 1. 2.]]\n",
            "Feature set entries: 3\n",
            "Epoch 1: Loss   = 389.6666666666667\n",
            "Epoch 2: Loss   = 216.82547069020575\n",
            "Epoch 3: Loss   = 161.41326835349574\n",
            "Epoch 4: Loss   = 125.54600853022914\n",
            "Epoch 5: Loss   = 95.54288048982966\n",
            "Epoch 6: Loss   = 68.98978969633946\n",
            "Epoch 7: Loss   = 47.118711362661145\n",
            "Epoch 8: Loss   = 31.594309707528513\n",
            "Epoch 9: Loss   = 22.421439984317143\n",
            "Epoch 10: Loss   = 17.839100922606516\n",
            "Epoch 11: Loss   = 15.72896679169825\n",
            "Epoch 12: Loss   = 14.67006037744386\n",
            "Epoch 13: Loss   = 13.992744484595002\n",
            "Epoch 14: Loss   = 13.446110354144063\n",
            "Epoch 15: Loss   = 12.948387913746416\n",
            "Epoch 16: Loss   = 12.474782726545124\n",
            "Epoch 17: Loss   = 12.017861843211229\n",
            "Epoch 18: Loss   = 11.575222892279465\n",
            "Epoch 19: Loss   = 11.145899726277724\n",
            "Epoch 20: Loss   = 10.729347309245876\n",
            "Epoch 21: Loss   = 10.325157779059358\n",
            "Epoch 22: Loss   = 9.932979986280385\n",
            "Epoch 23: Loss   = 9.55249537819732\n",
            "Epoch 24: Loss   = 9.183409595219244\n",
            "Epoch 25: Loss   = 8.825448563174799\n",
            "Epoch 26: Loss   = 8.478355961525544\n",
            "Epoch 27: Loss   = 8.141891183237634\n",
            "Epoch 28: Loss   = 7.815827525825735\n",
            "Epoch 29: Loss   = 7.49995053075704\n",
            "Epoch 30: Loss   = 7.194056441040438\n",
            "Epoch 31: Loss   = 6.897950763614084\n",
            "Epoch 32: Loss   = 6.611446929243518\n",
            "Epoch 33: Loss   = 6.334365045325974\n",
            "Epoch 34: Loss   = 6.066530738451728\n",
            "Epoch 35: Loss   = 5.807774084494105\n",
            "Epoch 36: Loss   = 5.5579286246259265\n",
            "Epoch 37: Loss   = 5.316830466083876\n",
            "Epoch 38: Loss   = 5.084317466764607\n",
            "Epoch 39: Loss   = 4.8602285028614265\n",
            "Epoch 40: Loss   = 4.644402818754249\n",
            "Epoch 41: Loss   = 4.4366794582626286\n",
            "Epoch 42: Loss   = 4.236896776176212\n",
            "Epoch 43: Loss   = 4.044892028702887\n",
            "Epoch 44: Loss   = 3.8605010411369762\n",
            "Epoch 45: Loss   = 3.6835579506632397\n",
            "Epoch 46: Loss   = 3.513895021792088\n",
            "Epoch 47: Loss   = 3.3513425314831533\n",
            "Epoch 48: Loss   = 3.195728720572751\n",
            "Epoch 49: Loss   = 3.046879807690219\n",
            "Epoch 50: Loss   = 2.90462006144218\n",
            "Epoch 51: Loss   = 2.7687719262744803\n",
            "Epoch 52: Loss   = 2.639156197099474\n",
            "Epoch 53: Loss   = 2.5155922375105266\n",
            "Epoch 54: Loss   = 2.3978982362028907\n",
            "Epoch 55: Loss   = 2.2858914960855086\n",
            "Epoch 56: Loss   = 2.1793887505048515\n",
            "Epoch 57: Loss   = 2.0782065010101847\n",
            "Epoch 58: Loss   = 1.9821613711689112\n",
            "Epoch 59: Loss   = 1.8910704710878115\n",
            "Epoch 60: Loss   = 1.8047517675065956\n",
            "Epoch 61: Loss   = 1.7230244545983657\n",
            "Epoch 62: Loss   = 1.6457093209303384\n",
            "Epoch 63: Loss   = 1.5726291083995623\n",
            "Epoch 64: Loss   = 1.503608859354072\n",
            "Epoch 65: Loss   = 1.4384762485311018\n",
            "Epoch 66: Loss   = 1.3770618968819972\n",
            "Epoch 67: Loss   = 1.3191996647996502\n",
            "Epoch 68: Loss   = 1.2647269227107447\n",
            "Epoch 69: Loss   = 1.2134847974340628\n",
            "Epoch 70: Loss   = 1.1653183931310733\n",
            "Epoch 71: Loss   = 1.1200769860799784\n",
            "Epoch 72: Loss   = 1.0776141928843803\n",
            "Epoch 73: Loss   = 1.037788112078813\n",
            "Epoch 74: Loss   = 1.0004614394123954\n",
            "Epoch 75: Loss   = 0.9655015573769012\n",
            "Epoch 76: Loss   = 0.9327805997950657\n",
            "Epoch 77: Loss   = 0.9021754924989654\n",
            "Epoch 78: Loss   = 0.8735679713067691\n",
            "Epoch 79: Loss   = 0.846844578650172\n",
            "Epoch 80: Loss   = 0.821896640316094\n",
            "Epoch 81: Loss   = 0.7986202238462218\n",
            "Epoch 82: Loss   = 0.776916080189544\n",
            "Epoch 83: Loss   = 0.7566895702282807\n",
            "Epoch 84: Loss   = 0.737850577799434\n",
            "Epoch 85: Loss   = 0.720313410815337\n",
            "Epoch 86: Loss   = 0.7039966920498603\n",
            "Epoch 87: Loss   = 0.688823241104957\n",
            "Epoch 88: Loss   = 0.674719949007923\n",
            "Epoch 89: Loss   = 0.6616176468152152\n",
            "Epoch 90: Loss   = 0.6494509695166794\n",
            "Epoch 91: Loss   = 0.6381582164463692\n",
            "Epoch 92: Loss   = 0.6276812093149963\n",
            "Epoch 93: Loss   = 0.6179651488859488\n",
            "Epoch 94: Loss   = 0.6089584712235309\n",
            "Epoch 95: Loss   = 0.600612704349536\n",
            "Epoch 96: Loss   = 0.5928823260540199\n",
            "Epoch 97: Loss   = 0.5857246235187552\n",
            "Epoch 98: Loss   = 0.5790995553281076\n",
            "Epoch 99: Loss   = 0.5729696163628227\n",
            "Epoch 100: Loss   = 0.567299705997403\n",
            "Epoch 101: Loss   = 0.5620569999521369\n",
            "Epoch 102: Loss   = 0.5572108260862735\n",
            "Epoch 103: Loss   = 0.552732544359465\n",
            "Epoch 104: Loss   = 0.5485954311345979\n",
            "Epoch 105: Loss   = 0.5447745679460355\n",
            "Epoch 106: Loss   = 0.5412467348133786\n",
            "Epoch 107: Loss   = 0.537990308141577\n",
            "Epoch 108: Loss   = 0.5349851632136009\n",
            "Epoch 109: Loss   = 0.5322125812515176\n",
            "Epoch 110: Loss   = 0.5296551609955257\n",
            "Epoch 111: Loss   = 0.5272967347278925\n",
            "Epoch 112: Loss   = 0.5251222886496995\n",
            "Epoch 113: Loss   = 0.5231178875023081\n",
            "Epoch 114: Loss   = 0.5212706033125007\n",
            "Epoch 115: Loss   = 0.5195684481297527\n",
            "Epoch 116: Loss   = 0.5180003106160994\n",
            "Epoch 117: Loss   = 0.5165558963430549\n",
            "Epoch 118: Loss   = 0.5152256716459348\n",
            "Epoch 119: Loss   = 0.5140008108834863\n",
            "Epoch 120: Loss   = 0.5128731469496813\n",
            "Epoch 121: Loss   = 0.5118351248847325\n",
            "Epoch 122: Loss   = 0.5108797584336373\n",
            "Epoch 123: Loss   = 0.5100005894027445\n",
            "Epoch 124: Loss   = 0.5091916496677407\n",
            "Epoch 125: Loss   = 0.5084474256899943\n",
            "Epoch 126: Loss   = 0.5077628254022327\n",
            "Epoch 127: Loss   = 0.5071331473290174\n",
            "Epoch 128: Loss   = 0.5065540518121598\n",
            "Epoch 129: Loss   = 0.5060215342162842\n",
            "Epoch 130: Loss   = 0.5055318999947934\n",
            "Epoch 131: Loss   = 0.5050817415018029\n",
            "Epoch 132: Loss   = 0.504667916440774\n",
            "Epoch 133: Loss   = 0.5042875278459146\n",
            "Epoch 134: Loss   = 0.5039379054975565\n",
            "Epoch 135: Loss   = 0.5036165886778733\n",
            "Epoch 136: Loss   = 0.5033213101783163\n",
            "Epoch 137: Loss   = 0.5030499814750294\n",
            "Epoch 138: Loss   = 0.5028006789932263\n",
            "Epoch 139: Loss   = 0.5025716313861396\n",
            "Epoch 140: Loss   = 0.5023612077585168\n",
            "Epoch 141: Loss   = 0.5021679067689028\n",
            "Epoch 142: Loss   = 0.5019903465489796\n",
            "Early stop in after epoch 143 with loss  0.5018272553821242\n",
            "   Prev Loss (138) : 0.5028006789932263 [Delta: 0.0009734236111020866]\n",
            "Epoch 143: Y-True = [[-10. -11. -15.]]\n",
            "Epoch 143: Y-Pred = [[ -9.4350603  -11.98320201 -14.53134373]]\n",
            "Epoch 143: Loss = 0.5018272553821242\n",
            "Epoch 143: Finished\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}