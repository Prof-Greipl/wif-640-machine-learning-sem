{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"IML_201_MNIST with LANET.ipynb","version":"0.3.2","provenance":[{"file_id":"1zClpCmXXCfora-ioa9zwinKfTVU18IfP","timestamp":1567502389065},{"file_id":"1vu7AD-mBYlX580KZEQ0uzGUVKQYtYKe9","timestamp":1567449859050},{"file_id":"1IYojz_9oaDwM3Od6Ztub2bZrxaRqDT_3","timestamp":1567448809939},{"file_id":"1mW4xs4JJTPA6jm7hIRjOhjfR67gHm1Fi","timestamp":1567416857042},{"file_id":"1KbqMz92YgwPjWMafBVhJCXj9r64TsYMZ","timestamp":1567245502602},{"file_id":"1NIIvnmjnRo0DWk3LEQ0CTSXjPOdW_onv","timestamp":1567242361448},{"file_id":"1hqH2_nPaRpEsg1z65rglN-vd4OPo10Id","timestamp":1567239664353},{"file_id":"1610xqRo2kOx8mfvzK3fWft-wNrwBBGPL","timestamp":1566992017998},{"file_id":"1qO3v9wS3vNQRhzyWfw2MJGAuX3TtVw0J","timestamp":1566987247685},{"file_id":"1S9bdyiAEoSoFnub1i90tniJjGu86Y4wu","timestamp":1566986399582}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"RluyvAxqWLKI","colab_type":"text"},"source":["# Shuffling"]},{"cell_type":"code","metadata":{"id":"hNxiSRMbWNzy","colab_type":"code","outputId":"ea303a82-01d1-479e-a3d3-36f818cbacf3","executionInfo":{"status":"ok","timestamp":1567585920811,"user_tz":-120,"elapsed":537,"user":{"displayName":"Dieter Greipl","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCQkt4LlXZTbr43bodLOYJA_AMigYzivO_n1PBIcw=s64","userId":"09839720196215486633"}},"colab":{"base_uri":"https://localhost:8080/","height":268}},"source":["X = np.array([[1., 10., 100], [2., 20., 200], [3., 30.,300]])\n","y = np.array([[0,10,20], [1,11,21], [2,12,22]])\n","print( X.shape )\n","print( y.shape )\n","print( X  )\n","print( y )\n","from sklearn.utils import shuffle\n","X, y = shuffle(X.T, y.T)\n","print(\"After Shuffle\")\n","print( X.T  )\n","print( y.T )\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(3, 3)\n","(3, 3)\n","[[  1.  10. 100.]\n"," [  2.  20. 200.]\n"," [  3.  30. 300.]]\n","[[ 0 10 20]\n"," [ 1 11 21]\n"," [ 2 12 22]]\n","After Shuffle\n","[[100.   1.  10.]\n"," [200.   2.  20.]\n"," [300.   3.  30.]]\n","[[20  0 10]\n"," [21  1 11]\n"," [22  2 12]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GCl2WWLvbBxO","colab_type":"code","outputId":"6b711c91-ff64-4126-9628-7bfe0d8a9d34","executionInfo":{"status":"ok","timestamp":1567587691767,"user_tz":-120,"elapsed":559,"user":{"displayName":"Dieter Greipl","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCQkt4LlXZTbr43bodLOYJA_AMigYzivO_n1PBIcw=s64","userId":"09839720196215486633"}},"colab":{"base_uri":"https://localhost:8080/","height":167}},"source":["# Our Situation: N = 4\n","import random\n","X = np.array([[1., 10., 100, 1000], [2., 20., 200, 2000], [3., 30.,300, 3000]])\n","y = np.array([[0,10,20,30], [1,11,21,31], [2,12,22,32] ])\n","print( X.shape )\n","print( y.shape )\n","\n","permutation = np.random.permutation( X.shape[1])\n","print( permutation)\n","X_s = X.T[permutation]\n","y_s = y.T[permutation]\n","print( X_s.T )\n","print( y_s.T )\n","\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(3, 4)\n","(3, 4)\n","[0 1 2 3]\n","[[1.e+00 1.e+01 1.e+02 1.e+03]\n"," [2.e+00 2.e+01 2.e+02 2.e+03]\n"," [3.e+00 3.e+01 3.e+02 3.e+03]]\n","[[ 0 10 20 30]\n"," [ 1 11 21 31]\n"," [ 2 12 22 32]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nRXZ2JSFmXlI","colab_type":"text"},"source":["# Adding Validation Loss"]},{"cell_type":"code","metadata":{"id":"B_27n1bKPiJf","colab_type":"code","outputId":"d20dfaa0-40e6-41b0-cfd5-42eefbc42726","executionInfo":{"status":"ok","timestamp":1567588627122,"user_tz":-120,"elapsed":30118,"user":{"displayName":"Dieter Greipl","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCQkt4LlXZTbr43bodLOYJA_AMigYzivO_n1PBIcw=s64","userId":"09839720196215486633"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import numpy as np \n","import pandas as pd\n","from sklearn.utils import shuffle\n","import sys\n","\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","\n","from sklearn import metrics\n","\n","def load_data( data_variant ):\n","  if (data_variant == \"D2_N1_Y1\"):\n","    X  = np.array([[1], [1]])\n","    y_true =np.array( [[22]] )\n","\n","  elif (data_variant == \"D2_N2_Y1\"):\n","    X  = np.array( [[0.,1.], [0.,1.]] ) \n","    y_true = np.array( [[0., 7.]] )\n","\n","  elif (data_variant == \"D2_N3_Y1\"):\n","    X  = np.array( [[0.,1., 2.0], [0.1, 0.2, 0.3]] ) \n","    y_true = np.array( [[10., -11., -15.0]] )\n","\n","  elif (data_variant == \"D2_N3_Y1_UNSCALED\"):\n","    X  = np.array( [[0.,10., 200.0], [0.,1., 2.0]] ) \n","    y_true = np.array( [[10., -11., -15.0]] )\n","\n","  elif (data_variant == \"D2_N100_Y1_UNSCALED\"):\n","    X = np.arange(1,201).reshape(2,100) + 10*np.random.rand(2,100)\n","    y_true = np.arange(1,101).reshape(1,100) +500\n","\n","  elif (data_variant == \"MNIST\"):\n","      X = np.loadtxt(\"sample_data/mnist_test.csv\", delimiter=\",\")\n","      \n","      # first col contains labels; do one-hot-encoding\n","      y = X[:,:1]\n","      lr = np.arange(10)  # we have 10 labels\n","      y_true = (lr==y).astype(np.float)\n","      y_true[ y_true == 1] = 0.99\n","      y_true[ y_true == 0] = 0.01\n","\n","      # remaining cols contain pixel values\n","      X = X[:,1:]\n","      print( X.shape  )\n","      return (X.T, y_true.T, )\n","\n","  else: \n","    raise Exception(f'Unkown datasource:  {data_variant}')\n","\n","  return(X, y_true)\n","\n","class standardizer:\n","  def __init__(self):\n","      self.row_scale_params = []\n","      \n","  def standardize(self, matrix):\n","    N = matrix.shape[0]\n","    result = np.empty( matrix.shape )    \n","    # run through the lines\n","    for i in range( N ):\n","      row = matrix[i,:]\n","\n","      # Compute scaling parameters...\n","      mean = np.mean(row)\n","      std = np.std(row)\n","\n","      # Handle std = 0 case\n","      if std < 1E-6:\n","        std = 1\n","\n","      # and save\n","      self.row_scale_params.append( [mean, std] )\n","           \n","      # add the row to the output\n","      result[i, :] = np.array( (row - mean) / std  )\n","\n","    return result\n","\n","  def check(self, matrix):\n","    N = matrix.shape[0]\n","   \n","    for i in range( N ):\n","      row = matrix[i,:]\n","\n","      # Compute scaling parameters...\n","      mean = np.mean(row)\n","      std = np.std(row)\n","      print(f\"Row {i} : Mean = {mean}, Std = {std}\")\n","\n","\n","def load_auto_mpg_data(samples = -1):\n","  df = pd.read_csv(\n","      \"https://data.heatonresearch.com/data/t81-558/auto-mpg.csv\", \n","      na_values=['NA', '?'])\n","\n","\n","  print( df.describe() )\n","\n","  # Handle the missing values\n","  df['horsepower'] = df['horsepower'].fillna(df['horsepower'].median())\n","\n","  nCyl = df['cylinders']\n","  nDis = df['displacement']\n","  nHor = df['horsepower']\n","  nWei = df['weight']\n","  nAcc = df['acceleration']\n","  nYea = df['year']\n","  nOri = df['origin']\n","\n","  X = pd.concat( [nCyl, nDis, nHor, nWei, nAcc, nYea, nOri], axis = 'columns').values;\n","  y_true = np.array( [ df['mpg'].values ] ) # regression\n","\n","  if (samples < 0 ):\n","    return (X.T, y_true)\n","  else:\n","    return ( X.T[:, 0:samples], y_true[:, 0:samples])\n","\n","def activation(z, act_func):\n","    global _activation\n","    if act_func == 'relu':\n","        h = np.maximum(z , np.zeros(z.shape))\n","        if not np.array_equal(h,z):\n","          _activation = _activation + 1;\n","          \n","        return np.maximum(z, np.zeros(input_.shape))\n","    \n","    elif act_func == 'sigmoid':\n","      return 1.0/(1.0 + np.exp( -z ))\n","\n","    elif act_func == 'linear':\n","        return z\n","    else:\n","        raise Exception('Activation function is not defined.')\n","\n","def loss(y_true, y_predicted, loss_function='mse'):\n","   if loss_function == 'mse':\n","      return metrics.mean_squared_error( y_true, y_predicted)\n","   else:\n","      raise Exception('Loss metric is not defined.')\n","\n","\n","def get_dZ_from_loss(y, y_predicted, metric):\n","    if metric == 'mse':\n","        return (y_predicted - y)\n","\n","    else:\n","        raise Exception('Loss metric is not defined.')\n","   \n","def get_dactivation(A, act_func):\n","    if act_func == 'relu':\n","        return np.maximum(np.sign(A), np.zeros(A.shape)) # 1 if backward input >0, 0 otherwise; then diaganolize\n","\n","    elif act_func == 'sigmoid':\n","        h = activation(A, 'sigmoid')\n","        return h *(1-h)\n","\n","    elif act_func == 'linear':\n","        return np.ones(A.shape)\n","\n","    else:\n","        raise Exception('Activation function is not defined.')\n","        \n","class layer:\n","  def __init__(self,input_dim, output_dim, activation='relu'):    \n","    self.activation = activation\n","    self.input_dim = input_dim\n","    self.output_dim = output_dim # is this needed?? TODO\n","    if input_dim > 0:\n","      self.b = np.ones( (output_dim,1) )       \n","      self.W = np.random.randn( output_dim, input_dim )\n","      self.W = np.random.randn(output_dim, input_dim) * np.sqrt(2/input_dim) \n","    \n","    self.A = np.zeros( (output_dim,1) ) # added: we temp. store for A\n","  \n","  def setWeight(self, W ):\n","    self.W = W\n","    \n","  def setBias(self, b ):\n","    self.b = b\n","    \n","  def setActivation(self, A ): \n","    self.Z =  np.add( np.dot(self.W, A), self.b)\n","    self.A =  activation(self.Z, self.activation)\n","  \n","  \n","  def print(self, layer_name=\"\"):\n","    print(f\"Layer {layer_name}: Begin of Summary\")\n","    if self.input_dim > 0:\n","      print(f\"Layer {layer_name}: input_dim = {self.input_dim}\")\n","      print(f\"Layer {layer_name}: output_dim = {self.output_dim}\")\n","      print(f\"Layer {layer_name}: Activation = {self.activation}\")\n","      print(f\"W = \")\n","      print(self.W)\n","      print(f\"A = \")\n","      print(self.A)\n","      print(f\"b = \")\n","      print(self.b)\n","    else:\n","      print(f\"Layer {layer_name}: This is an input layer..... \")\n","      print(f\"A = \")\n","      print(self.A)\n","  \n","    print(f\"Layer {layer_name}: End of Summary\")\n","  \n","\n","class ModelNet:\n","  def __init__(self, input_dim):  \n","    self.history = []\n","    self.neural_net = []\n","    self.neural_net.append(layer(0 , input_dim, 'irrelevant'))\n","    \n","\n","  def addLayer(self, nr_neurons, activation='relu'):    \n","    layer_index = len(self.neural_net)\n","    input_dim = self.neural_net[layer_index - 1].output_dim\n","    new_layer = layer( input_dim, nr_neurons, activation)\n","    self.neural_net.append( new_layer )\n","    \n","  \n","  def get_history(self):\n","     return pd.DataFrame(\n","         self.history, \n","         columns=['epoch', 'train_loss', 'val_loss']\n","     )         \n","\n","  def forward_propagation(self, input_vec ):\n","    self.neural_net[0].A = input_vec\n","    for layer_index in range(1,len(self.neural_net)):    \n","      _A_Prev = self.neural_net[layer_index-1].A                       \n","      self.neural_net[layer_index].setActivation( _A_Prev )\n","      \n","    return  self.neural_net[layer_index].A\n","    \n","    \n","  def fit(self, training_data, epochs, val_data = None, early_stop=1, doShuffle=True, batch_size=0, learning_rate=0.01, verbose=1 ):\n","    self.learning_rate = learning_rate\n","    self.history = []  # Reset History Array\n","    \n","    X_train, y_train = training_data\n","\n","    if val_data:\n","      X_valid, y_valid = val_data\n","\n","    num_train_datum = X_train.shape[1]\n","\n","    print(f\"\\n\\nStart training for following parameters :\")\n","    print(f\" N  =  {num_train_datum}\")\n","    print(f\" Shape of X     =  {X_train.shape}\")\n","    print(f\" Shape of y     =  {y_train.shape}\")\n","    print(f\" Batchsize      =  {batch_size}\")\n","    print(f\" Learning rate  =  {learning_rate}\")\n","    if val_data:\n","      print(f\" N for Val Data  = {X_valid.shape[1]}\")\n","    else:\n","      printf(\" No validation data provided\")  \n","    \n","    # Training Loop\n","    for epoch in range(1,epochs+1): \n","      # Shuffle the training data\n","       \n","      # Step 1: Do forward propagation and calculate losses ----------------\n","      # ... for validation data\n","      if val_data:\n","        y_valid_predicted = self.forward_propagation( X_valid )\n","        validation_loss = loss(y_valid, y_valid_predicted)\n","      else:\n","        validation_loss = -1\n","\n","      \n","      # .. for training data\n","      y_train_predicted = self.forward_propagation( X_train )\n","      training_loss   = loss(y_train, y_train_predicted)\n","      \n","      # ... and update history\n","      self.history.append([epoch, training_loss, validation_loss])\n","\n","      # Step 2: Check for early stopping ----------------------------------\n","       # added: early stopping\n","      if (epoch > 7)  and (early_stop == 1):\n","        actual_train_loss = self.history[epoch-1][1] # epochs start with one!\n","        actual_val_loss   = self.history[epoch-1][2] # epochs start with one!\n","        past_loss   = self.history[epoch-6][1] \n","        if (abs(actual_train_loss - past_loss)) < 1E-3:\n","          print(f\"Early stop in after epoch {epoch} with training-loss  {actual_train_loss}\")\n","          print(f\"Early stop in after epoch {epoch} with validation-loss  {actual_val_loss}\")          \n","          print(f\"   Prev Loss ({epoch-5}) : {past_loss} [Delta: { abs(actual_train_loss-past_loss) }]\")\n","\n","          break   \n","      \n","      # Print before update\n","      if (verbose > 0):\n","        print(f\"Epoch {epoch}: Train.-Loss   = { training_loss  }\")\n","        print(f\"Epoch {epoch}: Val-Loss      = { validation_loss  }\")\n","        print(f\"Epoch {epoch}: Finished\")\n","\n","      # Step 2: Learning Step\n","      # Start to prepare the batches ...\n","      if batch_size:\n","        # Shuffle, IMPROVE, as this creats a copy..\n","        if doShuffle:\n","          X_shuffle, y_shuffle = shuffle(X_train.T, y_train.T)\n","        else:\n","          X_shuffle, y_shuffle = (X_train.T, y_train.T)\n","          \n","        # Note the transpose, to undo shuffel transpose\n","        X_train_batches_list = [   \n","            X_shuffle.T[: , k:k+batch_size]\n","            for k in range(0, num_train_datum, batch_size)]  \n","\n","        y_train_batches_list = [   \n","            y_shuffle.T[:, k:k+batch_size]\n","            for k in range(0, num_train_datum, batch_size)]  \n","\n","        nr_batches = len(X_train_batches_list)      \n","        for i in range( nr_batches ):\n","            X_train_batch = X_train_batches_list[i]\n","            y_train_batch = y_train_batches_list[i]\n","\n","            y_train_batch_predicted = model.forward_propagation( X_train_batch )\n","            self.backward_propagation( y_train_batch, y_train_batch_predicted, 1, verbose = verbose - 1 )\n","\n","            # Update the weights an biases\n","            self.update( learning_rate )\n","\n","      # Training with full data set\n","      else:\n","          y_train_predicted = model.forward_propagation( X_train )\n","          self.backward_propagation( y_train, y_train_predicted, 1, verbose = verbose - 1 )\n","\n","          # Update the weights an biases\n","          self.update( learning_rate )\n","\n","    print(f\"\\nEpoch {epoch}: Training   Loss = {training_loss}\")    \n","    print(f\"Epoch {epoch}: Validation Loss = {validation_loss}\")    \n","      \n","  def backward_propagation(self, y, y_predicted, num_train_datum, metric='mse', verbose=0):   \n","    nr_layers = len(self.neural_net)\n","    for layer_index in range(nr_layers-1,0,-1):\n","        if layer_index+1 == nr_layers: # if output layer\n","\n","            # This is  outpu layer with activation!\n","            dZ = np.multiply(get_dZ_from_loss(y, y_predicted, metric), \n","                             get_dactivation(\n","                                self.neural_net[layer_index].A, \n","                                self.neural_net[layer_index].activation)\n","            )\n","#           dZ = get_dZ_from_loss(y, y_predicted, metric)\n","        else: \n","            dZ = np.multiply(\n","                   np.dot(\n","                       self.neural_net[layer_index+1].W.T, \n","                       dZ), \n","                   get_dactivation(\n","                         self.neural_net[layer_index].A, \n","                         self.neural_net[layer_index].activation)\n","                   )\n","           \n","        \n","        dW = np.dot(dZ, self.neural_net[layer_index-1].A.T) / num_train_datum\n","        db = np.sum(dZ, axis=1, keepdims=True) / num_train_datum\n","        \n","        self.neural_net[layer_index].dW = dW\n","        self.neural_net[layer_index].db = db\n","        if (verbose > 0):\n","          print(f\"\\n\\n====== Backward Propagation Layer {layer_index} =======\")\n","          print(f\"dZ      =  {dZ}\")          \n","          print(f\"dW      =  {dW}\")\n","          print(f\"db     =  {db}\")\n","          print(f\"A           = {self.neural_net[layer_index].A}\") \n","          print(f\"A prev lay  = {self.neural_net[layer_index-1].A}\") \n","             \n","  # added\n","  def update( self, learning_rate ):\n","    nr_layers = len(self.neural_net)\n","    for layer_index in range(1,nr_layers):        # update (W,b)\n","      self.neural_net[layer_index].W = self.neural_net[layer_index].W - learning_rate * self.neural_net[layer_index].dW  \n","      self.neural_net[layer_index].b = self.neural_net[layer_index].b - learning_rate * self.neural_net[layer_index].db\n","\n","  def summary(self):\n","      print(\"MODEL SUMMARY\")\n","      for layer_index in range(len(self.neural_net)):        \n","        self.neural_net[layer_index].print(layer_index)\n","        \n","      print(\"FINISHED MODEL SUMMARY\")\n","      \n","        \n","#Testing---------------------------------   \n","_activation = 0     \n","input_dim = 784\n","output_dim = 10\n","model = ModelNet( input_dim )\n","model.addLayer( 60, 'sigmoid' )\n","#model.addLayer( 4, 'relu' )\n","model.addLayer( output_dim, 'sigmoid' )\n","\n","(X, y_true ) = load_data(\"MNIST\")\n","\n","print( X.shape, y_true.shape)\n","\n","\n","\n","#Scaling\n","standardizer = standardizer()\n","X_S = standardizer.standardize( X )\n","\n","X_train, X_test, y_train, y_test = train_test_split(    \n","    X_S.T, y_true.T, test_size=1, random_state=42)\n","\n","print(f\"X_train shape : {X_train.shape}\")\n","print(f\"y_train shape :{y_train.shape}\")\n","\n","\n","\n","model.fit( (X_train.T, y_train.T) , \n","          30, \n","          (X_test.T, y_test.T), \n","          early_stop = 0, \n","          learning_rate = 0.1,\n","          batch_size = 10,\n","          doShuffle = False,\n","          verbose=1)\n","history = model.get_history()\n","plt.plot(history['train_loss'], label='Training Loss')\n","plt.plot(history['val_loss'], label='Validation Loss')\n","\n","\n","# We do the plot by ourselves\n","plt.xlabel('Epoch')\n","plt.ylabel('Mean Square Error [MSE]')\n","plt.ylim([0,0.1])  \n","plt.legend()\n","plt.show()\n","\n","# Test for a single image\n","print (f\"Testing-Loop :  {X_test.shape[0]}\")\n","\n","count = 0\n","test_cases = X_test.shape[0]\n","#test_cases = 2\n","for test_id in range(test_cases):\n","  input_vec = np.array( [ X_test.T[:, test_id ] ] ).T\n","  y_predicted = model.forward_propagation( input_vec )\n","  nr_pred = y_predicted.argmax()\n","  nr_real = y_test[test_id].argmax() \n","  if (nr_real == nr_pred ):\n","    count = count + 1\n","\n","  if (test_id < 0):\n","    print(\"\\ny_predicted\")\n","    print( y_predicted.argmax() )\n","    print( y_predicted)\n","    \n","    print(\"y_true\")\n","    print( y_test[test_id].argmax() )\n","    print( y_test[test_id])\n","\n","\n","print( count )\n","print( count / test_cases )\n","\n","\n","# Same loop for training data\n","count = 0\n","test_cases = X_train.shape[0]\n","#test_cases = 2\n","for test_id in range(test_cases):\n","  input_vec = np.array( [ X_train.T[:, test_id ] ] ).T\n","  y_predicted = model.forward_propagation( input_vec )\n","  nr_pred = y_predicted.argmax()\n","  nr_real = y_train[test_id].argmax() \n","  if (nr_real == nr_pred ):\n","    count = count + 1\n","\n","  if (test_id < 0):\n","    print(\"\\ny_predicted\")\n","    print( y_predicted.argmax() )\n","    print( y_predicted)\n","    \n","    print(\"y_true\")\n","    print( y_test[test_id].argmax() )\n","    print( y_test[test_id])\n","\n","print( count )\n","print( count / test_cases )\n","\n","#model.summary()\n","\n","\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(10000, 784)\n","(784, 10000) (10, 10000)\n","X_train shape : (9999, 784)\n","y_train shape :(9999, 10)\n","\n","\n","Start training for following parameters :\n"," N  =  9999\n"," Shape of X     =  (784, 9999)\n"," Shape of y     =  (10, 9999)\n"," Batchsize      =  10\n"," Learning rate  =  0.1\n"," N for Val Data  = 1\n","Epoch 1: Train.-Loss   = 0.390591435074341\n","Epoch 1: Val-Loss      = 0.4135421806927194\n","Epoch 1: Finished\n","Epoch 2: Train.-Loss   = 0.01520608961330204\n","Epoch 2: Val-Loss      = 0.00028086919252460276\n","Epoch 2: Finished\n","Epoch 3: Train.-Loss   = 0.013398639129426616\n","Epoch 3: Val-Loss      = 0.00019222298318678356\n","Epoch 3: Finished\n","Epoch 4: Train.-Loss   = 0.01254664154398285\n","Epoch 4: Val-Loss      = 0.00022112392570528696\n","Epoch 4: Finished\n","Epoch 5: Train.-Loss   = 0.012426531426591295\n","Epoch 5: Val-Loss      = 0.0003010875984363078\n","Epoch 5: Finished\n","Epoch 6: Train.-Loss   = 0.012337877781054312\n","Epoch 6: Val-Loss      = 0.0003201055983463149\n","Epoch 6: Finished\n","Epoch 7: Train.-Loss   = 0.01232974627345157\n","Epoch 7: Val-Loss      = 0.00027679952895438826\n","Epoch 7: Finished\n","Epoch 8: Train.-Loss   = 0.012316369905813964\n","Epoch 8: Val-Loss      = 0.00020469626511449376\n","Epoch 8: Finished\n","Epoch 9: Train.-Loss   = 0.012032912238961573\n","Epoch 9: Val-Loss      = 0.00016060310432730704\n","Epoch 9: Finished\n","Epoch 10: Train.-Loss   = 0.011937224330901157\n","Epoch 10: Val-Loss      = 0.00016759672244801574\n","Epoch 10: Finished\n","Epoch 11: Train.-Loss   = 0.011830524804890125\n","Epoch 11: Val-Loss      = 0.00019589110090096747\n","Epoch 11: Finished\n","Epoch 12: Train.-Loss   = 0.011718246801140657\n","Epoch 12: Val-Loss      = 0.0002225442794967585\n","Epoch 12: Finished\n","Epoch 13: Train.-Loss   = 0.011688616730667165\n","Epoch 13: Val-Loss      = 0.0002368878436911133\n","Epoch 13: Finished\n","Epoch 14: Train.-Loss   = 0.011612441437205792\n","Epoch 14: Val-Loss      = 0.00019327905975433782\n","Epoch 14: Finished\n","Epoch 15: Train.-Loss   = 0.011509918621729023\n","Epoch 15: Val-Loss      = 0.00017518649680857505\n","Epoch 15: Finished\n","Epoch 16: Train.-Loss   = 0.011485882694231455\n","Epoch 16: Val-Loss      = 0.00014718229979601827\n","Epoch 16: Finished\n","Epoch 17: Train.-Loss   = 0.011427195755555836\n","Epoch 17: Val-Loss      = 0.00011645622808873485\n","Epoch 17: Finished\n","Epoch 18: Train.-Loss   = 0.011386286868421814\n","Epoch 18: Val-Loss      = 6.623387468020778e-05\n","Epoch 18: Finished\n","Epoch 19: Train.-Loss   = 0.01135943756989442\n","Epoch 19: Val-Loss      = 5.219284856448091e-05\n","Epoch 19: Finished\n","Epoch 20: Train.-Loss   = 0.011390475692294667\n","Epoch 20: Val-Loss      = 5.2037898377998095e-05\n","Epoch 20: Finished\n","Epoch 21: Train.-Loss   = 0.011515930220652972\n","Epoch 21: Val-Loss      = 5.9489205671545556e-05\n","Epoch 21: Finished\n","Epoch 22: Train.-Loss   = 0.011709673277600146\n","Epoch 22: Val-Loss      = 6.6604472320863e-05\n","Epoch 22: Finished\n","Epoch 23: Train.-Loss   = 0.011809150095142393\n","Epoch 23: Val-Loss      = 7.453642288962695e-05\n","Epoch 23: Finished\n","Epoch 24: Train.-Loss   = 0.011878033449119061\n","Epoch 24: Val-Loss      = 8.882074385692837e-05\n","Epoch 24: Finished\n","Epoch 25: Train.-Loss   = 0.011876382948739252\n","Epoch 25: Val-Loss      = 0.00011927717784993494\n","Epoch 25: Finished\n","Epoch 26: Train.-Loss   = 0.01188326152415496\n","Epoch 26: Val-Loss      = 0.00014582410588575813\n","Epoch 26: Finished\n","Epoch 27: Train.-Loss   = 0.01188274855311548\n","Epoch 27: Val-Loss      = 0.00016942470594081307\n","Epoch 27: Finished\n","Epoch 28: Train.-Loss   = 0.011796910651808172\n","Epoch 28: Val-Loss      = 0.00016524820028328483\n","Epoch 28: Finished\n","Epoch 29: Train.-Loss   = 0.0117874075793896\n","Epoch 29: Val-Loss      = 0.00015951790908355783\n","Epoch 29: Finished\n","Epoch 30: Train.-Loss   = 0.011794021167457753\n","Epoch 30: Val-Loss      = 0.00015092636240666732\n","Epoch 30: Finished\n","\n","Epoch 30: Training   Loss = 0.011794021167457753\n","Epoch 30: Validation Loss = 0.00015092636240666732\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Testing-Loop :  1\n","1\n","1.0\n","9353\n","0.9353935393539354\n"],"name":"stdout"}]}]}