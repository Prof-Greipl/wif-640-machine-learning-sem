{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"IML_304_Adding a Loss Function.ipynb","version":"0.3.2","provenance":[{"file_id":"1S9bdyiAEoSoFnub1i90tniJjGu86Y4wu","timestamp":1566986399582}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"PNlKyoxuZa_a","colab_type":"text"},"source":["# Prep: Analyze `metrics.mean_squared_error()`"]},{"cell_type":"code","metadata":{"id":"jUJtDTswYN5j","colab_type":"code","outputId":"a7c090de-c725-45b9-f792-74b600d4864d","executionInfo":{"status":"ok","timestamp":1567501939901,"user_tz":-120,"elapsed":1361,"user":{"displayName":"Dieter Greipl","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCQkt4LlXZTbr43bodLOYJA_AMigYzivO_n1PBIcw=s64","userId":"09839720196215486633"}},"colab":{"base_uri":"https://localhost:8080/","height":117}},"source":["import numpy as np \n","from sklearn import metrics\n","y_true = np.array( [[0,1,2], [3,4,5]] )\n","y_predicted = np.array( [[1,1,1], [2,2,2]] )\n","error = metrics.mean_squared_error( y_true, y_predicted)\n","print (error)\n","\n","diff =  y_true  - y_predicted\n","print( diff )\n","diff = diff * diff\n","print( diff )\n","print (16/6)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["2.6666666666666665\n","[[-1  0  1]\n"," [ 1  2  3]]\n","[[1 0 1]\n"," [1 4 9]]\n","2.6666666666666665\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rT-PAzAbZlBf","colab_type":"text"},"source":["# Adding a Loss Function"]},{"cell_type":"markdown","metadata":{"id":"nRXZ2JSFmXlI","colab_type":"text"},"source":["We add a loss function."]},{"cell_type":"code","metadata":{"id":"B_27n1bKPiJf","colab_type":"code","colab":{}},"source":["import numpy as np \n","from sklearn import metrics\n","\n","def activation(z, act_func):\n","    global _activation\n","    if act_func == 'relu':\n","       return np.maximum(z, np.zeros(z.shape))\n","    \n","    elif act_func == 'sigmoid':\n","      return 1.0/(1.0 + np.exp( -z ))\n","\n","    elif act_func == 'linear':\n","        return z\n","    else:\n","        raise Exception('Activation function is not defined.')\n","\n","def loss(y_true, y_predicted, loss_function='mse'):\n","   if loss_function == 'mse':\n","      return metrics.mean_squared_error( y_true, y_predicted)\n","   else:\n","      raise Exception('Loss metric is not defined.')\n","        \n","        \n","class layer:\n","  def __init__(self,input_dim, output_dim, activation='relu'):    \n","    self.activation = activation\n","    self.input_dim = input_dim\n","    self.output_dim = output_dim \n","    if input_dim > 0:\n","      self.b = np.ones( (output_dim,1) ) \n","      self.W = np.ones( (output_dim, input_dim) )\n","    \n","    self.A = np.zeros( (output_dim,1)) # added: we temp. store for A\n","  \n","  def setWeight(self, W ):\n","    self.W = W\n","    \n","  def setBias(self, b ):\n","    self.b = b\n","    \n","  def setActivation(self, A ): # changed\n","    self.Z =  np.add( np.dot(self.W, A), self.b)\n","    self.A =  activation(self.Z, self.activation)\n","  \n","  def print(self, layer_name=\"\"):\n","    print(f\"Konfiguration of Layer {layer_name} ------\")\n","    if self.input_dim > 0:\n","      print(f\"input_dim = {self.input_dim}\")\n","      print(f\"output_dim = {self.output_dim}\")\n","      print(f\"Activation = {self.activation}\")\n","      print(f\"W = \")\n","      print(self.W)\n","      print(f\"b = \")\n","      print(self.b)\n","    else:\n","      print(\"This is an input layer..... \")\n","    #print(\"-----Finished Layer Config.\")\n","  \n","\n","class ModelNet:\n","  def __init__(self, input_dim):  \n","    \n","    self.neural_net = []\n","    self.neural_net.append(layer(0 , input_dim, 'irrelevant'))\n","    \n","  def addLayer(self, nr_neurons, activation='relu'):    \n","    layer_index = len(self.neural_net)\n","    input_dim = self.neural_net[layer_index - 1].output_dim\n","    new_layer = layer( input_dim, nr_neurons, activation)\n","    self.neural_net.append( new_layer)\n","    \n","  \n","  def forward_propagation(self, input_vec ):\n","    self.neural_net[0].A = input_vec\n","    for layer_index in range(1,len(self.neural_net)):    \n","       _A_Prev = self.neural_net[layer_index-1].A                       \n","       self.neural_net[layer_index].setActivation( _A_Prev )\n","    return  self.neural_net[layer_index].A\n","    \n","                             \n","  def summary(self):      \n","      for layer_index in range(len(self.neural_net)):        \n","        self.neural_net[layer_index].print(layer_index)\n","        \n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KgksdlgetaHg","colab_type":"text"},"source":["# Test"]},{"cell_type":"code","metadata":{"id":"2ch_ypOytbVi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":420},"outputId":"4f0efef2-671e-459d-a84c-9b87561fc735","executionInfo":{"status":"ok","timestamp":1567876333647,"user_tz":-120,"elapsed":541,"user":{"displayName":"Dieter Greipl","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCQkt4LlXZTbr43bodLOYJA_AMigYzivO_n1PBIcw=s64","userId":"09839720196215486633"}}},"source":["#Test        \n","input_dim = 2\n","output_dim = 1\n","model = ModelNet( input_dim )\n","model.addLayer( 2, 'relu' )\n","model.addLayer( output_dim, 'linear' )\n","model.summary()\n","\n","\n","# Testing Loss for one feature vev (N=1)\n","X  = [[1], [1]]\n","y_true =[[2]]\n","y_predicted = model.forward_propagation( X )\n","cost = loss( y_predicted, y_true)\n","print(f\"Loss = {cost}\")\n","\n","# Testing for more more features (N=2)\n","X  = [[1,2], [1,2]]\n","y_true =np.array( [[2, 3]] )\n","y_predicted = model.forward_propagation( X )\n","print(f\" Result of propagation {y_predicted}\")\n","print(f\" Shape of y_true {y_true}\")\n","\n","cost = loss( y_predicted, y_true)\n","print(f\"Loss = {cost}\")"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Konfiguration of Layer 0 ------\n","This is an input layer..... \n","Konfiguration of Layer 1 ------\n","input_dim = 2\n","output_dim = 2\n","Activation = relu\n","W = \n","[[1. 1.]\n"," [1. 1.]]\n","b = \n","[[1.]\n"," [1.]]\n","Konfiguration of Layer 2 ------\n","input_dim = 2\n","output_dim = 1\n","Activation = linear\n","W = \n","[[1. 1.]]\n","b = \n","[[1.]]\n","Loss = 25.0\n"," Result of propagation [[ 7. 11.]]\n"," Shape of y_true [[2 3]]\n","Loss = 44.5\n"],"name":"stdout"}]}]}