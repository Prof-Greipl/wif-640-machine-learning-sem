{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"IML_308_Early Stopping.ipynb","version":"0.3.2","provenance":[{"file_id":"1NIIvnmjnRo0DWk3LEQ0CTSXjPOdW_onv","timestamp":1567242361448},{"file_id":"1hqH2_nPaRpEsg1z65rglN-vd4OPo10Id","timestamp":1567239664353},{"file_id":"1610xqRo2kOx8mfvzK3fWft-wNrwBBGPL","timestamp":1566992017998},{"file_id":"1qO3v9wS3vNQRhzyWfw2MJGAuX3TtVw0J","timestamp":1566987247685},{"file_id":"1S9bdyiAEoSoFnub1i90tniJjGu86Y4wu","timestamp":1566986399582}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"nRXZ2JSFmXlI","colab_type":"text"},"source":["# Early Stopping"]},{"cell_type":"code","metadata":{"id":"B_27n1bKPiJf","colab_type":"code","colab":{}},"source":["import numpy as np \n","import pandas as pd\n","\n","import matplotlib.pyplot as plt\n","\n","from sklearn import metrics\n","\n","\n","def activation(z, act_func):\n","    global _activation\n","    if act_func == 'relu':\n","       return np.maximum(z, np.zeros(z.shape))\n","    \n","    elif act_func == 'sigmoid':\n","      return 1.0/(1.0 + np.exp( -z ))\n","\n","    elif act_func == 'linear':\n","        return z\n","    else:\n","        raise Exception('Activation function is not defined.')\n","\n","\n","def get_dactivation(A, act_func):\n","    if act_func == 'relu':\n","        return np.maximum(np.sign(A), np.zeros(A.shape)) # 1 if backward input >0, 0 otherwise; then diaganolize\n","\n","    elif act_func == 'sigmoid':\n","        h = activation(A, 'sigmoid')\n","        return h *(1-h)\n","\n","    elif act_func == 'linear':\n","        return np.ones(A.shape)\n","\n","    else:\n","        raise Exception('Activation function is not defined.')\n","\n","def loss(y_true, y_predicted, loss_function='mse'):\n","   if loss_function == 'mse':\n","      return metrics.mean_squared_error( y_true, y_predicted)\n","   else:\n","      raise Exception('Loss metric is not defined.')\n","\n","\n","def get_dZ_from_loss(y, y_predicted, metric):\n","    if metric == 'mse':\n","        return y_predicted - y\n","    else:\n","        raise Exception('Loss metric is not defined.')\n","\n","        \n","class layer:\n","  def __init__(self,input_dim, output_dim, activation='relu'):    \n","    self.activation = activation\n","    self.input_dim = input_dim\n","    self.output_dim = output_dim # is this needed?? TODO\n","    if input_dim > 0:\n","      self.b = np.ones( (output_dim,1) )       \n","      self.W = np.ones( (output_dim, input_dim) )\n","      #self.W = np.random.randn(output_dim, input_dim) * np.sqrt(2/input_dim) \n","    \n","    self.A = np.zeros( (output_dim,1) ) # added: we temp. store for A\n","  \n","  def setWeight(self, W ):\n","    self.W = W\n","    \n","  def setBias(self, b ):\n","    self.b = b\n","    \n","  def setActivation(self, A ): \n","    self.Z =  np.add( np.dot(self.W, A), self.b)\n","    self.A =  activation(self.Z, self.activation)\n","  \n","  \n","  def print(self, layer_name=\"\"):\n","    print(f\"Layer {layer_name}: Begin of Summary\")\n","    if self.input_dim > 0:\n","      print(f\"Layer {layer_name}: input_dim = {self.input_dim}\")\n","      print(f\"Layer {layer_name}: output_dim = {self.output_dim}\")\n","      print(f\"Layer {layer_name}: Activation = {self.activation}\")\n","      print(f\"W = \")\n","      print(self.W)\n","      print(f\"A = \")\n","      print(self.A)\n","      print(f\"b = \")\n","      print(self.b)\n","    else:\n","      print(f\"Layer {layer_name}: This is an input layer..... \")\n","      print(f\"A = \")\n","      print(self.A)\n","  \n","    print(f\"Layer {layer_name}: End of Summary\")\n","  \n","\n","class ModelNet:\n","  def __init__(self, input_dim):  \n","    self.history = []\n","    self.neural_net = []\n","    self.neural_net.append(layer(0 , input_dim, 'irrelevant'))\n","    \n","\n","  def addLayer(self, nr_neurons, activation='relu'):    \n","    layer_index = len(self.neural_net)\n","    input_dim = self.neural_net[layer_index - 1].output_dim\n","    new_layer = layer( input_dim, nr_neurons, activation)\n","    self.neural_net.append( new_layer )\n","    \n","  #added  \n","  def get_history(self):\n","     return pd.DataFrame(\n","         self.history, \n","         columns=['epoch', 'loss']\n","     )         \n","\n","  def forward_propagation(self, input_vec ):\n","    self.neural_net[0].A = input_vec\n","    for layer_index in range(1,len(self.neural_net)):    \n","      _A_Prev = self.neural_net[layer_index-1].A                       \n","      self.neural_net[layer_index].setActivation( _A_Prev )\n","      \n","    return  self.neural_net[layer_index].A\n","    \n","    \n","  def fit(self, input_vec, y_true, max_epoch, learning_rate=0.01, verbose=1 ):\n","    print(f\"Start training for input_vec:\")\n","    print( input_vec)\n","    print(f\"Feature set entries: {input_vec.shape[1]}\")\n","    \n","    self.learning_rate = learning_rate\n","    self.history = []  # Reset History Array\n","    num_train_datum = input_vec.shape[1]\n","\n","    # Training Loop\n","    for epoch in range(1,max_epoch+1): \n","\n","      # Generate y_predicted\n","      y_predicted = self.forward_propagation( input_vec )\n","\n","      # Do Backpropagation\n","      self.backward_propagation( y_true, y_predicted,  num_train_datum, verbose = verbose - 1 )\n","\n","      #calculate intermediate loss\n","      cost = loss(y_true, y_predicted)\n","\n","      # Update history\n","      self.history.append([epoch, cost])\n","\n","      # Update the weights an biases\n","      self.update( learning_rate )\n","\n","      # added: early stopping\n","      if epoch > 7:\n","        actual_loss = self.history[epoch-1][1] # epochs start with one!\n","        past_loss   = self.history[epoch-6][1] \n","        if (abs(actual_loss - past_loss)) < 1E-3:\n","          print(f\"Early stop in after epoch {epoch} with loss  {actual_loss}\")\n","          print(f\"   Prev Loss ({epoch-5}) : {past_loss} [Delta: { abs(actual_loss-past_loss) }]\")\n","\n","          break\n","      \n","\n","      if (verbose > 0):\n","        #print(f\"Epoch {epoch}: Y-True = {y_true}\")\n","        #print(f\"Epoch {epoch}: Y-Pred = {y_predicted}\")\n","        print(f\"Epoch {epoch}: Loss   = { cost }\")\n","        #print(f\"Epoch {epoch}: Finished\")\n","\n","    print(f\"Epoch {epoch}: Y-True = {y_true}\")\n","    print(f\"Epoch {epoch}: Y-Pred = {y_predicted}\")\n","    print(f\"Epoch {epoch}: Loss = {loss(y_true, y_predicted)}\")    \n","    print(f\"Epoch {epoch}: Finished\")\n","      \n","      \n","  def backward_propagation(self, y, y_predicted, num_train_datum, metric='mse', verbose=0):\n","    nr_layers = len(self.neural_net)\n","    for layer_index in range(nr_layers-1,0,-1):\n","        if layer_index+1 == nr_layers: # if output layer\n","            dZ = get_dZ_from_loss(y, y_predicted, metric)\n","        else: \n","            dZ = np.multiply(\n","                   np.dot(\n","                       self.neural_net[layer_index+1].W.T, \n","                       dZ), \n","                   get_dactivation(\n","                         self.neural_net[layer_index].A, \n","                         self.neural_net[layer_index].activation)\n","                   )\n","           \n","        \n","        dW = np.dot(dZ, self.neural_net[layer_index-1].A.T) / num_train_datum\n","        db = np.sum(dZ, axis=1, keepdims=True) / num_train_datum\n","        \n","        self.neural_net[layer_index].dW = dW\n","        self.neural_net[layer_index].db = db\n","        if (verbose > 0):\n","          print(f\"\\n\\n====== Backward Propagation Layer {layer_index} =======\")\n","          print(f\"dZ      =  {dZ}\")          \n","          print(f\"dW      =  {dW}\")\n","          print(f\"A-1     = {self.neural_net[layer_index-1].A}\") \n","          print(f\"\\nb     =  {db}\")\n","             \n","  # added\n","  def update( self, learning_rate ):\n","    nr_layers = len(self.neural_net)\n","    for layer_index in range(1,nr_layers):        # update (W,b)\n","      self.neural_net[layer_index].W = self.neural_net[layer_index].W - learning_rate * self.neural_net[layer_index].dW  \n","      self.neural_net[layer_index].b = self.neural_net[layer_index].b - learning_rate * self.neural_net[layer_index].db\n","\n","  def summary(self):\n","      print(\"MODEL SUMMARY\")\n","      for layer_index in range(len(self.neural_net)):        \n","        self.neural_net[layer_index].print(layer_index)\n","        \n","      print(\"FINISHED MODEL SUMMARY\")\n","      \n","        \n","\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ub8vzrWXRXoe","colab_type":"text"},"source":["# Test"]},{"cell_type":"code","metadata":{"id":"TGkJxaDjReyX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":185},"outputId":"66fe760c-b818-4aa9-b14a-3993902c68ca","executionInfo":{"status":"ok","timestamp":1567936167420,"user_tz":-120,"elapsed":549,"user":{"displayName":"Dieter Greipl","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCQkt4LlXZTbr43bodLOYJA_AMigYzivO_n1PBIcw=s64","userId":"09839720196215486633"}}},"source":["#Test        \n","input_dim = 2\n","output_dim = 1\n","model = ModelNet( input_dim )\n","model.addLayer( 2, 'relu' )\n","model.addLayer( output_dim, 'linear' )\n","\n","# Play with different feature set lengths..\n","# N=1\n","#X  = np.array([[1], [1]])\n","#y_true =np.array( [[22]] )\n","\n","# N=2\n","#X  = np.array( [[0.,1.], [0.,1.]] ) \n","#y_true = np.array( [[0., 7.]] )\n","\n","\n","# N=3\n","X  = np.array( [[0.,1., 2.0], [0.,1., 2.0]] ) \n","y_true = np.array( [[-10., -11., -15.0]] )\n","\n","model.fit( X, y_true, 1000, verbose=0)\n","\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Start training for input_vec:\n","[[0. 1. 2.]\n"," [0. 1. 2.]]\n","Feature set entries: 3\n","Early stop in after epoch 143 with loss  0.5018272553821242\n","   Prev Loss (138) : 0.5028006789932263 [Delta: 0.0009734236111020866]\n","Epoch 143: Y-True = [[-10. -11. -15.]]\n","Epoch 143: Y-Pred = [[ -9.4350603  -11.98320201 -14.53134373]]\n","Epoch 143: Loss = 0.5018272553821242\n","Epoch 143: Finished\n"],"name":"stdout"}]}]}