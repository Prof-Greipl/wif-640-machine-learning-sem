{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"IML_303_Neural Network with Forward Propagation (v0.2).ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"heuUWpNClPg3","colab_type":"text"},"source":["# Formal Notation\n","\n"]},{"cell_type":"markdown","metadata":{"id":"36YNlVltDLpP","colab_type":"text"},"source":["## Definition\n","A **Neural Network (NN)** is a (of course finite) sequence of layers \n","$L^1, L^2,... L^M $ (The superscript serves like an index and does not denote any power-function). If $M > 1$ then\n","$$\n","k^{l-1} = d^{l}, \\quad \\text{for} \\quad l \\in \\{2,..., M\\}  \n","$$\n","\n","\n","This means, that the output-dimension of Layer $l-1$ is equal to the input-dimension of Layer $l$. So Layer $l$ can use the outgoing activation of Layer $l-1$ as its incoming activation. The first layer will take the feature vector $x$ as incoming activation, compute the outgoing activation and transfer the  the result to the next layer, which uses it as its incoming activation. The outgoing activation of the last layer is the result of the network for input $x$. This process is called _forward propagation_. "]},{"cell_type":"markdown","metadata":{"id":"Rk3VmiAYB6f2","colab_type":"text"},"source":["## Input dimension and output dimension of a network\n","\n","The input dimension of the first layer called **input dimension of the network** of the network. It must be equal to the dimenson of our feature vectors.\n","\n","The output-dimension of the last layer is called **output dimension of the network**. It must be equal to the dimension of our label-vectors."]},{"cell_type":"markdown","metadata":{"id":"mQs6oTVmDDv5","colab_type":"text"},"source":["## Forward Propagation\n","Let $\\text{NN} = (L^1, L^1,\\dots,  L^M)$ be a Neural Network with input dimension $d$, then for  any feature vector $x \\in \\mathbb{R}^{d}$\n","\n","$$\n","NN(x) = (L_M \\circ L_{M-1} \\circ \\ldots \\circ  L_1)(x)\n","$$\n","\n","The process of calculating $NN(x)$ is called **forward-propagation** of the input $x$."]},{"cell_type":"markdown","metadata":{"id":"UJFttpl5sy0I","colab_type":"text"},"source":["# LANET 0.2: Neural Network with Forward Propagation"]},{"cell_type":"markdown","metadata":{"id":"3cjC271-Fd6K","colab_type":"text"},"source":["## Notes on the implementation\n","- The activation of each layer is stored in the layer in the variable `A`.\n","- To reduce computations we also store the $z$ vector for each layer.\n","- For simplicity we will add a Layer 0, just to hold the feature vector $x$ in variable `A`. Then we can do forward propagation in a simple loop over all layers\n","- we return the content of `A` of the last layer as result\n"]},{"cell_type":"markdown","metadata":{"id":"lQQFoZGXHZpr","colab_type":"text"},"source":["## Updated Code"]},{"cell_type":"code","metadata":{"id":"B_27n1bKPiJf","colab_type":"code","colab":{}},"source":["import numpy as np \n","\n","def sigma(z, act_func):\n","    global _activation\n","    if act_func == 'relu':\n","       return np.maximum(z, np.zeros(z.shape))\n","    \n","    elif act_func == 'sigmoid':\n","      return 1.0/(1.0 + np.exp( -z ))\n","\n","    elif act_func == 'linear':\n","        return z\n","    else:\n","        raise Exception('Activation function is not defined.')\n","\n","class Layer:\n","    def __init__(self,input_dim, output_dim, activation_function='linear'):    \n","        self.activation = activation_function\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim \n","        if input_dim > 0:\n","            #self.b = np.random.randn( output_dim, 1 )       \n","            #self.W = np.random.randn( output_dim, input_dim )\n","            #self.dW = np.random.randn( output_dim, input_dim )\n","            #self.db = np.random.randn( output_dim, 1 )\n","            self.b  = np.ones( (output_dim, 1) )       \n","            self.W  = np.ones( (output_dim, input_dim) )\n","        self.a = np.zeros( (output_dim,1) )\n","\n","    \n","    def set_weight(self, W ):\n","        self.W = W\n","      \n","    def set_bias(self, b ):\n","        self.b = b\n","  \n","    def compute_activation(self, a ): \n","        self.z =  np.add( np.dot(self.W, a), self.b)\n","        self.a =  sigma(self.z, self.activation)\n","    \n","    def print( self ):      \n","        print(f\"\\n====== Layer Info =======\")\n","        print(f\"a    = {self.a}\")\n","        if self.input_dim > 0: \n","          print(f\"W   =  {self.W}\")          \n","          print(f\"b   =  {self.b}\")  \n","    \n","\n","class Model:\n","  def __init__(self, input_dim):  \n","      self.neural_net = []\n","      self.neural_net.append(Layer(0 , input_dim, 'irrelevant'))    \n","\n","\n","  def add_layer(self, nr_neurons, activation='relu'):    \n","      layer_index = len(self.neural_net)\n","      input_dim = self.neural_net[layer_index - 1].output_dim\n","      new_layer = Layer( input_dim, nr_neurons, activation)\n","      self.neural_net.append( new_layer )\n","\n","\n","  def forward_propagation(self, input_vec ):\n","      self.neural_net[0].a = input_vec\n","      for layer_index in range(1,len(self.neural_net)):    \n","        _A_Prev = self.neural_net[layer_index-1].a                       \n","        self.neural_net[layer_index].compute_activation( _A_Prev )\n","        \n","      return  self.neural_net[layer_index].a\n","  \n","\n","  def summary(self):\n","      print(\"MODEL SUMMARY\")\n","      for layer_index in range(len(self.neural_net)):        \n","        self.neural_net[layer_index].print()\n","        \n","      print(\"FINISHED MODEL SUMMARY\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qdWo9h8Xsp51","colab_type":"text"},"source":["# Test"]},{"cell_type":"code","metadata":{"id":"lAHtv2VasuF6","colab_type":"code","outputId":"aca83d7b-d510-4bc3-a797-1698a2860687","executionInfo":{"status":"ok","timestamp":1568299166945,"user_tz":-120,"elapsed":524,"user":{"displayName":"Dieter Greipl","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCQkt4LlXZTbr43bodLOYJA_AMigYzivO_n1PBIcw=s64","userId":"09839720196215486633"}},"colab":{"base_uri":"https://localhost:8080/","height":504}},"source":["#Testing        \n","input_dim = 2\n","output_dim = 1\n","model = Model( input_dim )\n","model.add_layer( 2, 'relu' )\n","model.add_layer( output_dim, 'linear' )\n","model.summary()\n","\n","\n","# Testing for one feature  (N=1)\n","X  = np.array([[1,1]])\n","y_predicted = model.forward_propagation( X.T )\n","print(\"\\nExample 1\")\n","print(f\" Shape of X  {X.shape}\")\n","print(f\" Result of propagation {y_predicted}\")\n","print(f\" Shape of Result of propagation {y_predicted.shape}\")\n","\n","# Testing for more more features (N=2)\n","print(\"\\nExample 1\")\n","X  = np.array( [[1,1], [2,2]] )\n","y_predicted = model.forward_propagation( X.T )\n","print(f\" Shape of X  {X.shape}\")\n","print(f\" Result of propagation {y_predicted}\")\n","print(f\" Shape of Result of propagation {y_predicted.shape}\")\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["MODEL SUMMARY\n","\n","====== Layer Info =======\n","a    = [[0.]\n"," [0.]]\n","\n","====== Layer Info =======\n","a    = [[0.]\n"," [0.]]\n","W   =  [[1. 1.]\n"," [1. 1.]]\n","b   =  [[1.]\n"," [1.]]\n","\n","====== Layer Info =======\n","a    = [[0.]]\n","W   =  [[1. 1.]]\n","b   =  [[1.]]\n","FINISHED MODEL SUMMARY\n","\n","Example 1\n"," Shape of X  (1, 2)\n"," Result of propagation [[7.]]\n"," Shape of Result of propagation (1, 1)\n","\n","Example 1\n"," Shape of X  (2, 2)\n"," Result of propagation [[ 7. 11.]]\n"," Shape of Result of propagation (1, 2)\n"],"name":"stdout"}]}]}