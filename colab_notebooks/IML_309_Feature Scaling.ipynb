{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"IML_309_Feature Scaling.ipynb","version":"0.3.2","provenance":[{"file_id":"1KbqMz92YgwPjWMafBVhJCXj9r64TsYMZ","timestamp":1567245502602},{"file_id":"1NIIvnmjnRo0DWk3LEQ0CTSXjPOdW_onv","timestamp":1567242361448},{"file_id":"1hqH2_nPaRpEsg1z65rglN-vd4OPo10Id","timestamp":1567239664353},{"file_id":"1610xqRo2kOx8mfvzK3fWft-wNrwBBGPL","timestamp":1566992017998},{"file_id":"1qO3v9wS3vNQRhzyWfw2MJGAuX3TtVw0J","timestamp":1566987247685},{"file_id":"1S9bdyiAEoSoFnub1i90tniJjGu86Y4wu","timestamp":1566986399582}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"3JasRRcvdrj9","colab_type":"text"},"source":["# Prep: Class `Standardizer`"]},{"cell_type":"code","metadata":{"id":"53AWSfT08K_1","colab_type":"code","outputId":"8074025a-b7f0-41d1-f93d-5c1c22da1382","executionInfo":{"status":"ok","timestamp":1568116866872,"user_tz":-120,"elapsed":668,"user":{"displayName":"Dieter Greipl","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCQkt4LlXZTbr43bodLOYJA_AMigYzivO_n1PBIcw=s64","userId":"09839720196215486633"}},"colab":{"base_uri":"https://localhost:8080/","height":185}},"source":["import numpy as np\n","#added\n","class standardizer:\n","  def __init__(self):\n","      self.row_scale_params = []\n","      \n","  def standardize(self, matrix):\n","    N = matrix.shape[0]\n","    result = np.empty( matrix.shape )    \n","    # run through the lines\n","    for i in range( N ):\n","      row = matrix[i,:]\n","\n","      # Compute scaling parameters...\n","      mean = np.mean(row)\n","      std = np.std(row)\n","\n","      print(f\"Row {i} : Row = {row} Mean = {mean} Std = {std}\")\n","      # Handle std = 0 case\n","      if std < 1E-6:\n","        std = 1\n","\n","      # and save\n","      self.row_scale_params.append( [mean, std] )\n","           \n","      # add the row to the output\n","      result[i, :] = np.array( (row - mean) / std  )\n","\n","    return result\n","\n","  def check(self, matrix):\n","    N = matrix.shape[0]\n","   \n","    for i in range( N ):\n","      row = matrix[i,:]\n","\n","      # Compute scaling parameters...\n","      mean = np.mean(row)\n","      std = np.std(row)\n","      print(f\"Row {i} : Mean = {mean}, Std = {std}\")\n","\n","# Test\n","X  = np.array([[1], [1]])\n","X  = np.array( [[0.,1.], [0.,1.]] ) \n","X  = np.array( [[0.,1., 2.0], [0.,1., 2.0]] )\n","standardizer = standardizer()\n","\n","M = standardizer.standardize( X )\n","print(f\"In \\n {X}\")\n","print(f\"Out\\n {M}\")\n","\n","standardizer.check( M )\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Row 0 : Row = [0. 1. 2.] Mean = 1.0 Std = 0.816496580927726\n","Row 1 : Row = [0. 1. 2.] Mean = 1.0 Std = 0.816496580927726\n","In \n"," [[0. 1. 2.]\n"," [0. 1. 2.]]\n","Out\n"," [[-1.22474487  0.          1.22474487]\n"," [-1.22474487  0.          1.22474487]]\n","Row 0 : Mean = 0.0, Std = 0.9999999999999999\n","Row 1 : Mean = 0.0, Std = 0.9999999999999999\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nRXZ2JSFmXlI","colab_type":"text"},"source":["# LANET: Add Standardizer\n","We just *add* the standardizer to our LANET code. No changes on other code."]},{"cell_type":"code","metadata":{"id":"B_27n1bKPiJf","colab_type":"code","colab":{}},"source":["import numpy as np \n","import pandas as pd\n","\n","import matplotlib.pyplot as plt\n","\n","from sklearn import metrics\n","\n","class standardizer:\n","  def __init__(self):\n","      self.row_scale_params = []\n","      \n","  def standardize(self, matrix):\n","    N = matrix.shape[0]\n","    result = np.empty( matrix.shape )    \n","    # run through the lines\n","    for i in range( N ):\n","      row = matrix[i,:]\n","\n","      # Compute scaling parameters...\n","      mean = np.mean(row)\n","      std = np.std(row)\n","\n","      print(f\"Row {i} : Row = {row} Mean = {mean} Std = {std}\")\n","      # Handle std = 0 case\n","      if std < 1E-6:\n","        std = 1\n","\n","      # and save\n","      self.row_scale_params.append( [mean, std] )\n","           \n","      # add the row to the output\n","      result[i, :] = np.array( (row - mean) / std  )\n","\n","    return result\n","\n","  def check(self, matrix):\n","    N = matrix.shape[0]\n","   \n","    for i in range( N ):\n","      row = matrix[i,:]\n","\n","      # Compute scaling parameters...\n","      mean = np.mean(row)\n","      std = np.std(row)\n","      print(f\"Row {i} : Mean = {mean}, Std = {std}\")\n","\n","def get_data( data_variant ):\n","  if (data_variant == \"D2_N1_Y1\"):\n","    X  = np.array([[1], [1]])\n","    y_true =np.array( [[22]] )\n","\n","  elif (data_variant == \"D2_N2_Y1\"):\n","    X  = np.array( [[0.,1.], [0.,1.]] ) \n","    y_true = np.array( [[0., 7.]] )\n","\n","  elif (data_variant == \"D2_N3_Y1\"):\n","    X  = np.array( [[0.,1., 2.0], [0.1, 0.2, 0.3]] ) \n","    y_true = np.array( [[10., -11., -15.0]] )\n","\n","  elif (data_variant == \"D2_N3_Y1_UNSCALED\"):\n","    X  = np.array( [[0.,10., 200.0], [0.,1., 2.0]] ) \n","    y_true = np.array( [[10., -11., -15.0]] )\n","\n","  else: \n","    raise Exception(f'Unkown datasource:  {data_variant}')\n","\n","  return(X, y_true)\n","\n","def activation(z, act_func):\n","    global _activation\n","    if act_func == 'relu':\n","       return np.maximum(z, np.zeros(z.shape))\n","    \n","    elif act_func == 'sigmoid':\n","      return 1.0/(1.0 + np.exp( -z ))\n","\n","    elif act_func == 'linear':\n","        return z\n","    else:\n","        raise Exception('Activation function is not defined.')\n","\n","\n","def get_dactivation(A, act_func):\n","    if act_func == 'relu':\n","        return np.maximum(np.sign(A), np.zeros(A.shape)) # 1 if backward input >0, 0 otherwise; then diaganolize\n","\n","    elif act_func == 'sigmoid':\n","        h = activation(A, 'sigmoid')\n","        return h *(1-h)\n","\n","    elif act_func == 'linear':\n","        return np.ones(A.shape)\n","\n","    else:\n","        raise Exception('Activation function is not defined.')\n","\n","def loss(y_true, y_predicted, loss_function='mse'):\n","   if loss_function == 'mse':\n","      return metrics.mean_squared_error( y_true, y_predicted)\n","   else:\n","      raise Exception('Loss metric is not defined.')\n","\n","\n","def get_dZ_from_loss(y, y_predicted, metric):\n","    if metric == 'mse':\n","        return y_predicted - y\n","    else:\n","        raise Exception('Loss metric is not defined.')\n","           \n","class layer:\n","  def __init__(self,input_dim, output_dim, activation='relu'):    \n","    self.activation = activation\n","    self.input_dim = input_dim\n","    self.output_dim = output_dim # is this needed?? TODO\n","    if input_dim > 0:\n","      self.b = np.ones( (output_dim,1) )       \n","      self.W = np.ones( (output_dim, input_dim) )\n","      #self.W = np.random.randn(output_dim, input_dim) * np.sqrt(2/input_dim) \n","    \n","    self.A = np.zeros( (output_dim,1) ) # added: we temp. store for A\n","  \n","  def setWeight(self, W ):\n","    self.W = W\n","    \n","  def setBias(self, b ):\n","    self.b = b\n","    \n","  def setActivation(self, A ): \n","    self.Z =  np.add( np.dot(self.W, A), self.b)\n","    self.A =  activation(self.Z, self.activation)\n","  \n","  \n","  def print(self, layer_name=\"\"):\n","    print(f\"Layer {layer_name}: Begin of Summary\")\n","    if self.input_dim > 0:\n","      print(f\"Layer {layer_name}: input_dim = {self.input_dim}\")\n","      print(f\"Layer {layer_name}: output_dim = {self.output_dim}\")\n","      print(f\"Layer {layer_name}: Activation = {self.activation}\")\n","      print(f\"W = \")\n","      print(self.W)\n","      print(f\"A = \")\n","      print(self.A)\n","      print(f\"b = \")\n","      print(self.b)\n","    else:\n","      print(f\"Layer {layer_name}: This is an input layer..... \")\n","      print(f\"A = \")\n","      print(self.A)\n","  \n","    print(f\"Layer {layer_name}: End of Summary\")\n","  \n","\n","class ModelNet:\n","  def __init__(self, input_dim):  \n","    self.history = []\n","    self.neural_net = []\n","    self.neural_net.append(layer(0 , input_dim, 'irrelevant'))\n","    \n","\n","  def addLayer(self, nr_neurons, activation='relu'):    \n","    layer_index = len(self.neural_net)\n","    input_dim = self.neural_net[layer_index - 1].output_dim\n","    new_layer = layer( input_dim, nr_neurons, activation)\n","    self.neural_net.append( new_layer )\n","    \n","  #added  \n","  def get_history(self):\n","     return pd.DataFrame(\n","         self.history, \n","         columns=['epoch', 'loss']\n","     )         \n","\n","  def forward_propagation(self, input_vec ):\n","    self.neural_net[0].A = input_vec\n","    for layer_index in range(1,len(self.neural_net)):    \n","      _A_Prev = self.neural_net[layer_index-1].A                       \n","      self.neural_net[layer_index].setActivation( _A_Prev )\n","      \n","    return  self.neural_net[layer_index].A\n","    \n","    \n","  def fit(self, input_vec, y_true, max_epoch, early_stop=1, learning_rate=0.01, verbose=1 ):\n","    print(f\"Start training for input_vec:\")\n","    print( input_vec)\n","    print(f\"Feature set entries: {input_vec.shape[1]}\")\n","    \n","    self.learning_rate = learning_rate\n","    self.history = []  # Reset History Array\n","    num_train_datum = input_vec.shape[1]\n","    global _activation\n","    _activation = 0\n","\n","    # Training Loop\n","    for epoch in range(1,max_epoch+1): \n","\n","      # Generate y_predicted\n","      y_predicted = self.forward_propagation( input_vec )\n","\n","      # Do Backpropagation\n","      bp_verbose = verbose - 1\n","      if (epoch == 1000):\n","        bp_verbose=1\n","        self.summary()\n","\n","      self.backward_propagation( y_true, y_predicted,  num_train_datum, verbose = bp_verbose )\n","\n","      #calculate intermediate loss\n","      cost = loss(y_true, y_predicted)\n","\n","      # Update history\n","      self.history.append([epoch, cost])\n","\n","      # Update the weights an biases\n","      self.update( learning_rate )\n","\n","      # added: early stopping\n","      if (epoch > 7)  and (early_stop == 1):\n","        actual_loss = self.history[epoch-1][1] # epochs start with one!\n","        past_loss   = self.history[epoch-6][1] \n","        if (abs(actual_loss - past_loss)) < 1E-3:\n","          print(f\"Early stop in after epoch {epoch} with loss  {actual_loss}\")\n","          print(f\"   Prev Loss ({epoch-5}) : {past_loss} [Delta: { abs(actual_loss-past_loss) }]\")\n","\n","          break\n","      \n","\n","      if (verbose > 0):\n","        #print(f\"Epoch {epoch}: Y-True = {y_true}\")\n","        #print(f\"Epoch {epoch}: Y-Pred = {y_predicted}\")\n","        print(f\"Epoch {epoch}: Loss   = { cost }\")\n","        #print(f\"Epoch {epoch}: Finished\")\n","\n","    print(f\"Epoch {epoch}: Y-True = {y_true}\")\n","    print(f\"Epoch {epoch}: Y-Pred = {y_predicted}\")\n","    print(f\"Epoch {epoch}: Loss = {loss(y_true, y_predicted)}\")    \n","    print(f\"Epoch {epoch}: Finished\")\n","      \n","      \n","  def backward_propagation(self, y, y_predicted, num_train_datum, metric='mse', verbose=0):   \n","    nr_layers = len(self.neural_net)\n","    for layer_index in range(nr_layers-1,0,-1):\n","        if layer_index+1 == nr_layers: # if output layer\n","            dZ = get_dZ_from_loss(y, y_predicted, metric)\n","        else: \n","            dZ = np.multiply(\n","                   np.dot(\n","                       self.neural_net[layer_index+1].W.T, \n","                       dZ), \n","                   get_dactivation(\n","                         self.neural_net[layer_index].A, \n","                         self.neural_net[layer_index].activation)\n","                   )\n","           \n","        \n","        dW = np.dot(dZ, self.neural_net[layer_index-1].A.T) / num_train_datum\n","        db = np.sum(dZ, axis=1, keepdims=True) / num_train_datum\n","        \n","        self.neural_net[layer_index].dW = dW\n","        self.neural_net[layer_index].db = db\n","        if (verbose > 0):\n","          print(f\"\\n\\n====== Backward Propagation Layer {layer_index} =======\")\n","          print(f\"dZ      =  {dZ}\")          \n","          print(f\"dW      =  {dW}\")\n","          print(f\"A-1     = {self.neural_net[layer_index-1].A}\") \n","          print(f\"\\nb     =  {db}\")\n","             \n","  # added\n","  def update( self, learning_rate ):\n","    nr_layers = len(self.neural_net)\n","    for layer_index in range(1,nr_layers):        # update (W,b)\n","      self.neural_net[layer_index].W = self.neural_net[layer_index].W - learning_rate * self.neural_net[layer_index].dW  \n","      self.neural_net[layer_index].b = self.neural_net[layer_index].b - learning_rate * self.neural_net[layer_index].db\n","\n","  def summary(self):\n","      print(\"MODEL SUMMARY\")\n","      for layer_index in range(len(self.neural_net)):        \n","        self.neural_net[layer_index].print(layer_index)\n","        \n","      print(\"FINISHED MODEL SUMMARY\")\n","      \n","        \n","\n","\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v1X3arZ6VFHM","colab_type":"text"},"source":["# Test"]},{"cell_type":"code","metadata":{"id":"ijR4dBiIVKuv","colab_type":"code","outputId":"8ba3d207-6854-47b4-c40d-2d6aeb2d06c1","executionInfo":{"status":"ok","timestamp":1567937542468,"user_tz":-120,"elapsed":2268,"user":{"displayName":"Dieter Greipl","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCQkt4LlXZTbr43bodLOYJA_AMigYzivO_n1PBIcw=s64","userId":"09839720196215486633"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["#Testing---------------------------------   \n","_activation = 0     \n","input_dim = 2\n","output_dim = 1\n","model = ModelNet( input_dim )\n","model.addLayer( 2, 'relu' )\n","model.addLayer( output_dim, 'linear' )\n","\n","(X, y_true ) = get_data( \"D2_N3_Y1_UNSCALED\")\n","\n","max_epoch = 4000\n","learning_rate = 0.01\n","\n","print(\"Training with scaled data----------------\")\n","standardizer = standardizer()\n","X_S = standardizer.standardize( X )\n","model.fit( X_S, y_true, max_epoch, learning_rate = learning_rate , verbose=0)\n","history_scaled = model.get_history()\n","\n","print(\"Training with unscaled data----------------\")\n","model.fit( X, y_true, max_epoch, learning_rate = learning_rate , early_stop = 1, verbose=0)\n","history_unscaled = model.get_history()\n","\n","#Plotting\n","plt.xlabel('Epoch')\n","plt.ylabel('Mean Square Error [MSE]')\n","plt.plot(history_scaled['epoch'], history_scaled['loss'], label='Loss (Scaled Data)')\n","plt.plot(history_unscaled['loss'], label='Loss (Unscaled Data)')\n","#plt.ylim([0,50])  \n","plt.legend()\n","plt.show()\n","      \n","print(\"Unscaled History \")\n","display ( history_unscaled.head ( 10 ))\n","print(\"Scaled History \")\n","display ( history_scaled.head ( 10 ))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Training with scaled data----------------\n","Row 0 : Row = [  0.  10. 200.] Mean = 70.0 Std = 92.01449161228173\n","Row 1 : Row = [0. 1. 2.] Mean = 1.0 Std = 0.816496580927726\n","Start training for input_vec:\n","[[-0.76074973 -0.6520712   1.41282093]\n"," [-1.22474487  0.          1.22474487]]\n","Feature set entries: 3\n","Early stop in after epoch 840 with loss  0.20405508068577413\n","   Prev Loss (835) : 0.20487918811489145 [Delta: 0.0008241074291173156]\n","Epoch 840: Y-True = [[ 10. -11. -15.]]\n","Epoch 840: Y-Pred = [[  9.22883773 -10.90042604 -15.0869426 ]]\n","Epoch 840: Loss = 0.20405508068577413\n","Epoch 840: Finished\n","Training with unscaled data----------------\n","Start training for input_vec:\n","[[  0.  10. 200.]\n"," [  0.   1.   2.]]\n","Feature set entries: 3\n","Early stop in after epoch 504 with loss  120.23154683802146\n","   Prev Loss (499) : 120.2325327049893 [Delta: 0.0009858669678379783]\n","Epoch 504: Y-True = [[ 10. -11. -15.]]\n","Epoch 504: Y-Pred = [[-5.23676928 -5.23676928 -5.23676928]]\n","Epoch 504: Loss = 120.23154683802146\n","Epoch 504: Finished\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Unscaled History \n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>epoch</th>\n","      <th>loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>412.509363</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>394.681072</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>380.758204</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>369.437769</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>359.923022</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>6</td>\n","      <td>351.703799</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>7</td>\n","      <td>344.440894</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>8</td>\n","      <td>337.901571</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>9</td>\n","      <td>331.921827</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>10</td>\n","      <td>326.383413</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   epoch        loss\n","0      1  412.509363\n","1      2  394.681072\n","2      3  380.758204\n","3      4  369.437769\n","4      5  359.923022\n","5      6  351.703799\n","6      7  344.440894\n","7      8  337.901571\n","8      9  331.921827\n","9     10  326.383413"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Scaled History \n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>epoch</th>\n","      <th>loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>261.305517</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>218.511338</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>192.920370</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>175.429138</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>162.183713</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>6</td>\n","      <td>151.266530</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>7</td>\n","      <td>141.626628</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>8</td>\n","      <td>132.664414</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>9</td>\n","      <td>124.050052</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>10</td>\n","      <td>115.635494</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   epoch        loss\n","0      1  261.305517\n","1      2  218.511338\n","2      3  192.920370\n","3      4  175.429138\n","4      5  162.183713\n","5      6  151.266530\n","6      7  141.626628\n","7      8  132.664414\n","8      9  124.050052\n","9     10  115.635494"]},"metadata":{"tags":[]}}]}]}