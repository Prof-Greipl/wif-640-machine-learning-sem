{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"IML_304_LANET (v0.3): Adding a Loss Function.ipynb","version":"0.3.2","provenance":[{"file_id":"1S9bdyiAEoSoFnub1i90tniJjGu86Y4wu","timestamp":1566986399582}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"PNlKyoxuZa_a","colab_type":"text"},"source":["# Prep: Analyze `metrics.mean_squared_error()`"]},{"cell_type":"code","metadata":{"id":"jUJtDTswYN5j","colab_type":"code","outputId":"a7c090de-c725-45b9-f792-74b600d4864d","executionInfo":{"status":"ok","timestamp":1567501939901,"user_tz":-120,"elapsed":1361,"user":{"displayName":"Dieter Greipl","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCQkt4LlXZTbr43bodLOYJA_AMigYzivO_n1PBIcw=s64","userId":"09839720196215486633"}},"colab":{"base_uri":"https://localhost:8080/","height":117}},"source":["import numpy as np \n","from sklearn import metrics\n","y_true = np.array( [[0,1,2], [3,4,5]] )\n","y_predicted = np.array( [[1,1,1], [2,2,2]] )\n","error = metrics.mean_squared_error( y_true, y_predicted)\n","print (error)\n","\n","diff =  y_true  - y_predicted\n","print( diff )\n","diff = diff * diff\n","print( diff )\n","print (16/6)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["2.6666666666666665\n","[[-1  0  1]\n"," [ 1  2  3]]\n","[[1 0 1]\n"," [1 4 9]]\n","2.6666666666666665\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rDumVfWgUS2c","colab_type":"text"},"source":["#Prep: Caluclate L2-Norm of Vector"]},{"cell_type":"code","metadata":{"id":"KXTsi6RPUZ9E","colab_type":"code","outputId":"d2364f35-cb50-4910-e021-9ae6e57499fb","executionInfo":{"status":"ok","timestamp":1567953718522,"user_tz":-120,"elapsed":655,"user":{"displayName":"Dieter Greipl","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCQkt4LlXZTbr43bodLOYJA_AMigYzivO_n1PBIcw=s64","userId":"09839720196215486633"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["v = np.array([-1,2,3])\n","print (np.square(v).sum())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["14\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rT-PAzAbZlBf","colab_type":"text"},"source":["# Adding a Loss Function"]},{"cell_type":"markdown","metadata":{"id":"nRXZ2JSFmXlI","colab_type":"text"},"source":["We add a loss function."]},{"cell_type":"code","metadata":{"id":"B_27n1bKPiJf","colab_type":"code","colab":{}},"source":["import numpy as np \n","from sklearn import metrics\n","\n","def loss(y_true, y_predicted, loss_function='mse'):\n","    if loss_function == 'mse':       \n","        return metrics.mean_squared_error( y_true, y_predicted)\n","    else:\n","        raise Exception('Loss metric is not defined.')\n","        \n","def sigma(z, act_func):\n","    global _activation\n","    if act_func == 'relu':\n","       return np.maximum(z, np.zeros(z.shape))\n","    \n","    elif act_func == 'sigmoid':\n","      return 1.0/(1.0 + np.exp( -z ))\n","\n","    elif act_func == 'linear':\n","        return z\n","    else:\n","        raise Exception('Activation function is not defined.')\n","\n","class Layer:\n","    def __init__(self,input_dim, output_dim, activation_function='linear'):    \n","        self.activation = activation_function\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim \n","        if input_dim > 0:\n","            #self.b = np.random.randn( output_dim, 1 )       \n","            #self.W = np.random.randn( output_dim, input_dim )\n","            #self.dW = np.random.randn( output_dim, input_dim )\n","            #self.db = np.random.randn( output_dim, 1 )\n","            self.b  = np.ones( (output_dim, 1) )       \n","            self.W  = np.ones( (output_dim, input_dim) )\n","        self.a = np.zeros( (output_dim,1) )\n","\n","    \n","    def set_weight(self, W ):\n","        self.W = W\n","      \n","    def set_bias(self, b ):\n","        self.b = b\n","  \n","    def compute_activation(self, a ): \n","        self.z =  np.add( np.dot(self.W, a), self.b)\n","        self.a =  sigma(self.z, self.activation)\n","    \n","    def print( self ):      \n","        print(f\"\\n====== Layer Info =======\")\n","        print(f\"a    = {self.a}\")\n","        if self.input_dim > 0: \n","          print(f\"W   =  {self.W}\")          \n","          print(f\"b   =  {self.b}\")  \n","    \n","\n","class Model:\n","  def __init__(self, input_dim):  \n","      self.neural_net = []\n","      self.neural_net.append(Layer(0 , input_dim, 'irrelevant'))    \n","\n","\n","  def add_layer(self, nr_neurons, activation='relu'):    \n","      layer_index = len(self.neural_net)\n","      input_dim = self.neural_net[layer_index - 1].output_dim\n","      new_layer = Layer( input_dim, nr_neurons, activation)\n","      self.neural_net.append( new_layer )\n","\n","\n","  def forward_propagation(self, input_vec ):\n","      self.neural_net[0].a = input_vec\n","      for layer_index in range(1,len(self.neural_net)):    \n","        _A_Prev = self.neural_net[layer_index-1].a                       \n","        self.neural_net[layer_index].compute_activation( _A_Prev )\n","        \n","      return  self.neural_net[layer_index].a\n","  \n","\n","  def summary(self):\n","      print(\"MODEL SUMMARY\")\n","      for layer_index in range(len(self.neural_net)):        \n","        self.neural_net[layer_index].print()\n","        \n","      print(\"FINISHED MODEL SUMMARY\")\n","        \n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KgksdlgetaHg","colab_type":"text"},"source":["# Test"]},{"cell_type":"code","metadata":{"id":"2ch_ypOytbVi","colab_type":"code","outputId":"6e9bbb46-898c-4cb7-ab7d-861b5b5746cf","executionInfo":{"status":"ok","timestamp":1568299547163,"user_tz":-120,"elapsed":496,"user":{"displayName":"Dieter Greipl","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCQkt4LlXZTbr43bodLOYJA_AMigYzivO_n1PBIcw=s64","userId":"09839720196215486633"}},"colab":{"base_uri":"https://localhost:8080/","height":403}},"source":["#Test        \n","input_dim = 2\n","output_dim = 1\n","model = Model( input_dim )\n","model.add_layer( 2, 'relu' )\n","model.add_layer( output_dim, 'linear' )\n","model.summary()\n","\n","\n","# Testing Loss for one feature vev (N=1)\n","X      = np.array([[1,1]])\n","y_true = np.array([[2]])\n","y_predicted = model.forward_propagation( X.T )\n","cost = loss( y_predicted, y_true)\n","print(f\"Loss = {cost}\")\n","\n","# Testing for more more features (N=2)\n","X       = np.array( [[1,1], [2,2]] )\n","y_true  = np.array( [[2, 3]] )\n","y_predicted = model.forward_propagation( X.T )\n","print(f\" Result of propagation {y_predicted}\")\n","print(f\" Shape of y_true {y_true}\")\n","\n","cost = loss( y_predicted, y_true)\n","print(f\"Loss = {cost}\")"],"execution_count":5,"outputs":[{"output_type":"stream","text":["MODEL SUMMARY\n","\n","====== Layer Info =======\n","a    = [[0.]\n"," [0.]]\n","\n","====== Layer Info =======\n","a    = [[0.]\n"," [0.]]\n","W   =  [[1. 1.]\n"," [1. 1.]]\n","b   =  [[1.]\n"," [1.]]\n","\n","====== Layer Info =======\n","a    = [[0.]]\n","W   =  [[1. 1.]]\n","b   =  [[1.]]\n","FINISHED MODEL SUMMARY\n","Loss = 25.0\n"," Result of propagation [[ 7. 11.]]\n"," Shape of y_true [[2 3]]\n","Loss = 44.5\n"],"name":"stdout"}]}]}