{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"IML_302_A Neural Network_Layer.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ygg6Tym5c7F_","colab_type":"text"},"source":["# Formal Notation\n"]},{"cell_type":"markdown","metadata":{"id":"j75BMqpuveOv","colab_type":"text"},"source":["##  Concept\n","A Layer is just sequence $Q_1, ... Q_K$ of K neurons, where all neurons are set up with\n","- **identical** values $D$ for the incoming activations   \n","- **identical** activation functions $\\sigma$\n","\n","Before doing a formal definition of a layer, we define the activation of a Layer L for an incoming activation. This will help us to find a sharp definition."]},{"cell_type":"markdown","metadata":{"id":"7eN-SG7ZvVHA","colab_type":"text"},"source":["## Activation of a Layer\n","The activation $L(a)$ of a layer $L$ for an incoming activation $a \\in \\mathbb{R}^D$ is defined as\n","\n","$$\n","L : \\mathbb{R}^{D} \\rightarrow \\mathbb{R^k}\n","\\quad \\text{with} \\quad\n","L(a)    =   \\begin{bmatrix}\n","           Q_{1}(a) \\\\\n","           Q_{2}(a) \\\\\n","           \\vdots \\\\\n","           Q_{K}(a)\n","         \\end{bmatrix}\n","$$\n","\n","Let us rewrite this\n","\n","$$\n","\\begin{align}\n","L(a)    =   \\begin{bmatrix}\n","           Q_{1}(a) \\\\\n","           Q_{2}(a) \\\\\n","           \\vdots \\\\\n","           Q_{K}(a)\n","         \\end{bmatrix}\n","        = \\begin{bmatrix}\n","           \\sigma( w_1^Ta +b_1) \\\\\n","           \\sigma( w_2^Ta +b_2) \\\\\n","           \\vdots \\\\\n","           \\sigma( w_K^Ta +b_k)\n","         \\end{bmatrix}       \n","        = \\sigma( W a + b) \\\\[50pt]\n","\\end{align}\n","$$\n","\n","where the weight matrix $W$ and the bias-vector $b$ are now defined as\n","\n","$$ \n","W    = \\begin{bmatrix}\n","          w_1^T \\\\\n","           w_2^T \\\\\n","           \\vdots \\\\\n","           w_K^T\n","         \\end{bmatrix} \n","         \\in \\mathbb{R}^{K \\times D}\n","         \\quad \\text{and} \\quad\n","b    = \\begin{bmatrix}\n","          b_1 \\\\\n","           b_2 \\\\\n","           \\vdots \\\\\n","           b_k\n","         \\end{bmatrix} \n","         \\in \\mathbb{R}^{K}\n","$$\n","\n","Finally for any vector $v$ we define $\\sigma(v)$ as result of applying  $\\sigma$ to each element of $v$. \n","\n","Note, that the activation of layer now boils down to\n","- multiply the weight matrix $W$ with the incoming activation\n","- add the bias\n","- apply sigma.\n","\n","No too complicated! Take some time to understand the equations and definitions - it will payoff later!\n","\n","Now this gives us a good ground for a definition of a layer, that we can use directly in out algorithm."]},{"cell_type":"markdown","metadata":{"id":"vYDjhzkP58Z3","colab_type":"text"},"source":["## Definition of a Layer\n","A Layer L is defined by four elements $d, k, W, b, \\sigma$, where\n","- $D$ is the dimension of the incoming activation vector (**input dimension**)\n","- $K$ is the dimension of the outgoing activation vector (**output dimension**)\n","- $W \\in \\mathbb{R}^{K \\times D}$ is the **weight matrix** \n","- $b \\in \\mathbb{R}^{K}$ is the **bias** of the layer\n","- $\\sigma$ is the **activation function**\n","\n","Note, that we do not use the concept of a neutron any longer, but of course it remains the core of our neural network and will be imbeded in our definition."]},{"cell_type":"markdown","metadata":{"id":"8irc28L7rmBY","colab_type":"text"},"source":["# Neural Network Layer (LANET 0.1)"]},{"cell_type":"code","metadata":{"id":"HMyWpNp9MMeX","colab_type":"code","colab":{}},"source":["import numpy as np \n","\n","def sigma(z, act_func):\n","    global _activation\n","    if act_func == 'relu':\n","       return np.maximum(z, np.zeros(z.shape))\n","    \n","    elif act_func == 'sigmoid':\n","      return 1.0/(1.0 + np.exp( -z ))\n","\n","    elif act_func == 'linear':\n","        return z\n","    else:\n","        raise Exception('Activation function is not defined.')\n","\n","class Layer:\n","    def __init__(self,input_dim, output_dim, activation_function='linear'):    \n","        self.activation = activation_function\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim \n","        if input_dim > 0:\n","            #self.b = np.random.randn( output_dim, 1 )       \n","            #self.W = np.random.randn( output_dim, input_dim )\n","            #self.dW = np.random.randn( output_dim, input_dim )\n","            #self.db = np.random.randn( output_dim, 1 )\n","            self.b  = np.ones( (output_dim, 1) )       \n","            self.W  = np.ones( (output_dim, input_dim) )\n","            self.dW = np.ones( (output_dim, input_dim) )\n","            self.db = np.ones( (output_dim, 1) )\n","        self.a = np.zeros( (output_dim,1) )\n","        self.history = []\n","    \n","    def set_weight(self, W ):\n","        self.W = W\n","      \n","    def set_bias(self, b ):\n","        self.b = b\n","  \n","    def compute_activation(self, a ): \n","        self.z =  np.add( np.dot(self.W, a), self.b)\n","        self.a =  sigma(self.z, self.activation)\n","    \n","    def print( self ):\n","        print(f\"\\n====== Layer Info =======\")\n","        print(f\"a    = {self.a}\") \n","        print(f\"W   =  {self.W}\")          \n","        print(f\"b   =  {self.b}\")    \n","  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pR9OV0HRrrBX","colab_type":"text"},"source":["# Test"]},{"cell_type":"markdown","metadata":{"id":"HVaVTM5jrsqm","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"iSiApxbnrtlZ","colab_type":"code","outputId":"1681f145-540a-43ad-beac-6d5e614686cd","executionInfo":{"status":"ok","timestamp":1568298124830,"user_tz":-120,"elapsed":534,"user":{"displayName":"Dieter Greipl","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCQkt4LlXZTbr43bodLOYJA_AMigYzivO_n1PBIcw=s64","userId":"09839720196215486633"}},"colab":{"base_uri":"https://localhost:8080/","height":185}},"source":["# Create a Layer\n","my_layer = Layer(3,2,'relu')\n","my_layer.set_weight([[1,2], [2,-4]])\n","my_layer.set_bias([[1],[1]])\n","\n","myLayer.print()\n","\n","# Evaluate\n","aIn  = [[1], [1]]\n","my_layer.compute_activation( aIn )\n","print(f\"Activation : {my_layer.a}\")\n","\n","# This neuron works also for several inputs\n","aIn  = [[1,2], [2,3]]\n","my_layer.compute_activation( aIn )\n","print(f\"Activation : {my_layer.a}\")\n"],"execution_count":16,"outputs":[{"output_type":"stream","text":["\n","====== Layer Info =======\n","a    = [[0.]\n"," [0.]]\n","W   =  [[1, 2], [2, -4]]\n","b   =  [[1], [1]]\n","Activation : [[4.]\n"," [0.]]\n","Activation : [[6. 9.]\n"," [0. 0.]]\n"],"name":"stdout"}]}]}