{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"IML_305_Adding  Backpropagation.ipynb","version":"0.3.2","provenance":[{"file_id":"1qO3v9wS3vNQRhzyWfw2MJGAuX3TtVw0J","timestamp":1566987247685},{"file_id":"1S9bdyiAEoSoFnub1i90tniJjGu86Y4wu","timestamp":1566986399582}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"nRXZ2JSFmXlI","colab_type":"text"},"source":["We add a loss function."]},{"cell_type":"markdown","metadata":{"id":"ScZkAb4Qtr6Z","colab_type":"text"},"source":["# Adding  Backpropagation"]},{"cell_type":"code","metadata":{"id":"B_27n1bKPiJf","colab_type":"code","colab":{}},"source":["import numpy as np \n","from sklearn import metrics\n","\n","def activation(z, act_func):\n","    global _activation\n","    if act_func == 'relu':\n","       return np.maximum(z, np.zeros(z.shape))\n","    \n","    elif act_func == 'sigmoid':\n","      return 1.0/(1.0 + np.exp( -z ))\n","\n","    elif act_func == 'linear':\n","        return z\n","    else:\n","        raise Exception('Activation function is not defined.')\n","\n","#added    \n","def get_dactivation(A, act_func):\n","    if act_func == 'relu':\n","        return np.maximum(np.sign(A), np.zeros(A.shape)) # 1 if backward input >0, 0 otherwise; then diaganolize\n","\n","    elif act_func == 'sigmoid':\n","        h = activation(A, 'sigmoid')\n","        return h *(1-h)\n","\n","    elif act_func == 'linear':\n","        return np.ones(A.shape)\n","\n","    else:\n","        raise Exception('Activation function is not defined.')\n","        \n","def loss(y_true, y_predicted, loss_function='mse'):\n","   if loss_function == 'mse':\n","      return metrics.mean_squared_error( y_true, y_predicted)\n","   else:\n","      raise Exception('Loss metric is not defined.')\n","\n","#added\n","def get_dZ_from_loss(y, y_predicted, metric):\n","    if metric == 'mse':\n","        return y_predicted - y\n","    else:\n","        raise Exception('Loss metric is not defined.')\n","\n","        \n","class layer:\n","  def __init__(self,input_dim, output_dim, activation='relu'):    \n","    self.activation = activation\n","    self.input_dim = input_dim\n","    self.output_dim = output_dim # is this needed?? TODO\n","    if input_dim > 0:\n","      self.b = np.ones( (output_dim,1) ) \n","      self.W = np.ones( (output_dim, input_dim) )\n","    \n","    self.A = np.zeros( (output_dim,1) ) # added: we temp. store for A\n","  \n","  def setWeight(self, W ):\n","    self.W = W\n","    \n","  def setBias(self, b ):\n","    self.b = b\n","    \n","  def setActivation(self, A ): \n","    self.Z =  np.add( np.dot(self.W, A), self.b)\n","    self.A =  activation(self.Z, self.activation)\n","  \n","  \n","  def print(self, layer_name=\"\"):\n","    print(f\"Konfiguration of Layer {layer_name} ------\")\n","    if self.input_dim > 0:\n","      print(f\"input_dim = {self.input_dim}\")\n","      print(f\"output_dim = {self.output_dim}\")\n","      print(f\"Activation = {self.activation}\")\n","      print(f\"W = \")\n","      print(self.W)\n","      print(f\"b = \")\n","      print(self.b)\n","    else:\n","      print(\"This is an input layer..... \")\n","    #print(\"-----Finished Layer Config.\")\n","  \n","\n","class ModelNet:\n","  def __init__(self, input_dim):  \n","    \n","    self.neural_net = []\n","    self.neural_net.append(layer(0 , input_dim, 'irrelevant'))\n","    \n","  def addLayer(self, nr_neurons, activation='relu'):    \n","    layer_index = len(self.neural_net)\n","    input_dim = self.neural_net[layer_index - 1].output_dim\n","    new_layer = layer( input_dim, nr_neurons, activation)\n","    self.neural_net.append( new_layer)\n","    \n","  \n","  def forward_propagation(self, input_vec ):\n","    self.neural_net[0].A = input_vec\n","    for layer_index in range(1,len(self.neural_net)):    \n","      _A_Prev = self.neural_net[layer_index-1].A                       \n","      self.neural_net[layer_index].setActivation( _A_Prev )\n","    return  self.neural_net[layer_index].A\n","    \n","    \n","  # added \n","  def backward_propagation(self, y, y_predicted, num_train_datum, metric='mse', verbose=False):\n","    nr_layers = len(self.neural_net)\n","    for layer_index in range(nr_layers-1,0,-1):\n","        if layer_index+1 == nr_layers: # if output layer\n","            dZ = get_dZ_from_loss(y, y_predicted, metric)\n","        else: \n","            dZ = np.multiply(\n","                   np.dot(\n","                       self.neural_net[layer_index+1].W.T, \n","                       dZ), \n","                   get_dactivation(\n","                         self.neural_net[layer_index].A, \n","                         self.neural_net[layer_index].activation)\n","                   )\n","           \n","        \n","        dW = np.dot(dZ, self.neural_net[layer_index-1].A.T) / num_train_datum\n","        db = np.sum(dZ, axis=1, keepdims=True) / num_train_datum\n","        \n","        self.neural_net[layer_index].dW = dW\n","        self.neural_net[layer_index].db = db\n","        if verbose:\n","          print(f\"\\n\\n====== Backward Propagation Layer {layer_index} =======\")\n","          print(f\"dZ      =  {dZ}\");          \n","          print(f\"dW      =  {dW}\");\n","          print(f\"\\nb     =  {db}\");\n","             \n","  # added\n","  def update( self, learning_rate ):\n","    nr_layers = len(self.neural_net)\n","    for layer_index in range(1,nr_layers):        # update (W,b)\n","      self.neural_net[layer_index].W = self.neural_net[layer_index].W - learning_rate * self.neural_net[layer_index].dW  \n","      self.neural_net[layer_index].b = self.neural_net[layer_index].b - learning_rate * self.neural_net[layer_index].db\n","\n","  def summary(self):      \n","      for layer_index in range(len(self.neural_net)):        \n","        self.neural_net[layer_index].print(layer_index)\n","        "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ydk6xZXwtudS","colab_type":"text"},"source":["# Test"]},{"cell_type":"code","metadata":{"id":"7cToGoJPtxnl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":672},"outputId":"37b68f42-5c09-4770-d48c-59ce23eb6b59","executionInfo":{"status":"ok","timestamp":1567876447224,"user_tz":-120,"elapsed":579,"user":{"displayName":"Dieter Greipl","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCQkt4LlXZTbr43bodLOYJA_AMigYzivO_n1PBIcw=s64","userId":"09839720196215486633"}}},"source":["#Testing        \n","input_dim = 2\n","output_dim = 1\n","model = ModelNet( input_dim )\n","model.addLayer( 2, 'relu' )\n","model.addLayer( output_dim, 'linear' )\n","\n","\n","X  = np.array( [[1,2], [1,2]] ) \n","y_true =np.array( [[2, 3]] )\n","\n","y_predicted = model.forward_propagation( X )\n","print(f\" Predicted value {y_predicted}\")\n","model.backward_propagation(y_true, y_predicted, 1, verbose=True)\n","model.update( 0.1 )\n","model.summary()\n","\n","y_predicted = model.forward_propagation( X )\n","print(f\" Predicted value after one update (=learning) cycle {y_predicted}\")\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":[" Predicted value [[ 7. 11.]]\n","\n","\n","====== Backward Propagation Layer 2 =======\n","dZ      =  [[5. 8.]]\n","dW      =  [[55. 55.]]\n","\n","b     =  [[13.]]\n","\n","\n","====== Backward Propagation Layer 1 =======\n","dZ      =  [[5. 8.]\n"," [5. 8.]]\n","dW      =  [[21. 21.]\n"," [21. 21.]]\n","\n","b     =  [[13.]\n"," [13.]]\n","Konfiguration of Layer 0 ------\n","This is an input layer..... \n","Konfiguration of Layer 1 ------\n","input_dim = 2\n","output_dim = 2\n","Activation = relu\n","W = \n","[[-1.1 -1.1]\n"," [-1.1 -1.1]]\n","b = \n","[[-0.3]\n"," [-0.3]]\n","Konfiguration of Layer 2 ------\n","input_dim = 2\n","output_dim = 1\n","Activation = linear\n","W = \n","[[-4.5 -4.5]]\n","b = \n","[[-0.3]]\n"," Predicted value after one update (=learning) cycle [[-0.3 -0.3]]\n"],"name":"stdout"}]}]}