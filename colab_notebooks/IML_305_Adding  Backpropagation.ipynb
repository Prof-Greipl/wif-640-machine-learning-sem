{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"IML_305_Adding  Backpropagation.ipynb","version":"0.3.2","provenance":[{"file_id":"1qO3v9wS3vNQRhzyWfw2MJGAuX3TtVw0J","timestamp":1566987247685},{"file_id":"1S9bdyiAEoSoFnub1i90tniJjGu86Y4wu","timestamp":1566986399582}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"nRXZ2JSFmXlI","colab_type":"text"},"source":["We add a loss function."]},{"cell_type":"markdown","metadata":{"id":"ScZkAb4Qtr6Z","colab_type":"text"},"source":["# Adding  Backpropagation"]},{"cell_type":"code","metadata":{"id":"B_27n1bKPiJf","colab_type":"code","colab":{}},"source":["import numpy as np \n","from sklearn import metrics\n","\n","import numpy as np \n","from sklearn import metrics\n","\n","def loss(y_true, y_predicted, loss_function='mse'):\n","    if loss_function == 'mse':       \n","        return metrics.mean_squared_error( y_true, y_predicted)\n","    else:\n","        raise Exception('Loss metric is not defined.')\n","\n","def get_dz_from_loss(y, y_predicted, metric):\n","    if metric == 'mse':\n","        return y_predicted - y\n","    else:\n","        raise Exception('Loss metric is not defined.')\n","\n","def sigma(z, act_func):\n","    global _activation\n","    if act_func == 'relu':\n","       return np.maximum(z, np.zeros(z.shape))\n","    \n","    elif act_func == 'sigmoid':\n","      return 1.0/(1.0 + np.exp( -z ))\n","\n","    elif act_func == 'linear':\n","        return z\n","    else:\n","        raise Exception('Activation function is not defined.')\n","\n","def sigma_prime(z, act_func):\n","    if act_func == 'relu':\n","        return np.maximum(np.sign(z), np.zeros(z.shape)) # 1 if backward input >0, 0 otherwise; then diaganolize\n","\n","    elif act_func == 'sigmoid':\n","        h = sigma(z, 'sigmoid')\n","        return h *(1-h)\n","\n","    elif act_func == 'linear':\n","        return np.ones(z.shape)\n","\n","    else:\n","        raise Exception('Activation function is not defined.')\n","\n","class Layer:\n","    def __init__(self,input_dim, output_dim, activation_function='linear'):    \n","        self.activation = activation_function\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim \n","        if input_dim > 0:\n","            #self.b = np.random.randn( output_dim, 1 )       \n","            #self.W = np.random.randn( output_dim, input_dim )\n","            #self.dW = np.random.randn( output_dim, input_dim )\n","            #self.db = np.random.randn( output_dim, 1 )\n","            self.b  = np.ones( (output_dim, 1) )       \n","            self.W  = np.ones( (output_dim, input_dim) )\n","            self.dW = np.ones( (output_dim, input_dim) )\n","            self.db = np.ones( (output_dim, 1) )\n","        self.a = np.zeros( (output_dim,1) )\n","\n","    \n","    def set_weight(self, W ):\n","        self.W = W\n","      \n","    def set_bias(self, b ):\n","        self.b = b\n","  \n","    def compute_activation(self, a ): \n","        self.z =  np.add( np.dot(self.W, a), self.b)\n","        self.a =  sigma(self.z, self.activation)\n","    \n","    \n","    def print( self ):      \n","        print(f\"\\n====== Layer Info =======\")\n","        print(f\"a    = {self.a}\")\n","        if self.input_dim > 0: \n","          print(f\"W   =  {self.W}\")          \n","          print(f\"b   =  {self.b}\")  \n","    \n","\n","class Model:\n","    def __init__(self, input_dim):  \n","        self.neural_net = []\n","        self.neural_net.append(Layer(0 , input_dim, 'irrelevant'))    \n","\n","\n","    def add_layer(self, nr_neurons, activation='relu'):    \n","        layer_index = len(self.neural_net)\n","        input_dim = self.neural_net[layer_index - 1].output_dim\n","        new_layer = Layer( input_dim, nr_neurons, activation)\n","        self.neural_net.append( new_layer )\n","\n","\n","    def forward_propagation(self, input_vec ):\n","        self.neural_net[0].a = input_vec\n","        for layer_index in range(1,len(self.neural_net)):    \n","            _A_Prev = self.neural_net[layer_index-1].a                       \n","            self.neural_net[layer_index].compute_activation( _A_Prev )\n","        return  self.neural_net[layer_index].a\n","  \n","    def backward_propagation(self, y, y_predicted, num_train_datum, metric='mse', verbose=0):   \n","            nr_layers = len(self.neural_net)\n","            for layer_index in range(nr_layers-1,0,-1):\n","                if layer_index+1 == nr_layers: # if output layer\n","\n","                    dz = np.multiply(get_dz_from_loss(y, y_predicted, metric), \n","                                    sigma_prime(\n","                                        self.neural_net[layer_index].a, \n","                                        self.neural_net[layer_index].activation)\n","                    )        \n","                else: \n","                    dz = np.multiply(\n","                          np.dot(\n","                              self.neural_net[layer_index+1].W.T, \n","                              dz), \n","                          sigma_prime(\n","                                self.neural_net[layer_index].a, \n","                                self.neural_net[layer_index].activation)\n","                          )         \n","                dW = np.dot(dz, self.neural_net[layer_index-1].a.T) / num_train_datum\n","                db = np.sum(dz, axis=1, keepdims=True) / num_train_datum\n","\n","                # Update gradients\n","                self.neural_net[layer_index].dW = dW \n","                self.neural_net[layer_index].db = db \n","\n","                if (verbose > 0):\n","                  print(f\"\\n\\n====== Backward Propagation Layer {layer_index} =======\")\n","                  print(f\"dZ      =  {dz}\")          \n","                  print(f\"dW      =  {dW}\")\n","                  print(f\"db      =  {db}\")\n","                  print(f\"A           = {self.neural_net[layer_index].a}\") \n","                  print(f\"A prev lay  = {self.neural_net[layer_index-1].a}\") \n","                  \n","\n","    def update( self, learning_rate ):\n","        nr_layers = len(self.neural_net)\n","        for layer_index in range(1,nr_layers):        \n","            self.neural_net[layer_index].set_weight( self.neural_net[layer_index].W - learning_rate * self.neural_net[layer_index].dW )\n","            self.neural_net[layer_index].set_bias(  self.neural_net[layer_index].b  - learning_rate * self.neural_net[layer_index].db  )\n","    \n","\n","    def summary(self):\n","        print(\"MODEL SUMMARY\")\n","        for layer_index in range(len(self.neural_net)):        \n","          self.neural_net[layer_index].print()\n","          \n","        print(\"FINISHED MODEL SUMMARY\")\n","        "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ydk6xZXwtudS","colab_type":"text"},"source":["# Test"]},{"cell_type":"code","metadata":{"id":"7cToGoJPtxnl","colab_type":"code","outputId":"c015c305-0622-4f3c-9ed0-3ebd87288e10","executionInfo":{"status":"ok","timestamp":1568300482812,"user_tz":-120,"elapsed":517,"user":{"displayName":"Dieter Greipl","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCQkt4LlXZTbr43bodLOYJA_AMigYzivO_n1PBIcw=s64","userId":"09839720196215486633"}},"colab":{"base_uri":"https://localhost:8080/","height":907}},"source":["#Testing        \n","input_dim = 2\n","output_dim = 1\n","model = Model( input_dim )\n","model.add_layer( 2, 'relu' )\n","model.add_layer( output_dim, 'linear' )\n","\n","\n","X  = np.array( [[1,1], [2,2]] ) \n","y  =np.array( [[2, 3]] )\n","\n","y_predicted = model.forward_propagation( X.T )\n","print(f\" Predicted value {y_predicted}\")\n","print(f\" Predicted value after one update (=learning) cycle {y_predicted}\")\n","print(f\" Starting cost  : { loss(y_predicted, y)}\")\n","\n","model.backward_propagation(y, y_predicted, 1, verbose=True)\n","model.update( 0.1 )\n","y_predicted = model.forward_propagation( X )\n","print(f\" Predicted value {y_predicted}\")\n","print(f\" Predicted value after one update (=learning) cycle {y_predicted}\")\n","print(f\"Cost after first learning cycle : { loss(y_predicted, y)}\")\n","\n","model.backward_propagation(y_true, y_predicted, 1, verbose=True)\n","model.update( 0.1 )\n","y_predicted = model.forward_propagation( X )\n","print(f\" Predicted value {y_predicted}\")\n","print(f\" Predicted value after one update (=learning) cycle {y_predicted}\")\n","print(f\"Cost after second learning cycle : { loss(y_predicted, y)}\")\n","\n"],"execution_count":14,"outputs":[{"output_type":"stream","text":[" Predicted value [[ 7. 11.]]\n"," Predicted value after one update (=learning) cycle [[ 7. 11.]]\n"," Starting cost  : 44.5\n","\n","\n","====== Backward Propagation Layer 2 =======\n","dZ      =  [[5. 8.]]\n","dW      =  [[55. 55.]]\n","db      =  [[13.]]\n","A           = [[ 7. 11.]]\n","A prev lay  = [[3. 5.]\n"," [3. 5.]]\n","\n","\n","====== Backward Propagation Layer 1 =======\n","dZ      =  [[5. 8.]\n"," [5. 8.]]\n","dW      =  [[21. 21.]\n"," [21. 21.]]\n","db      =  [[13.]\n"," [13.]]\n","A           = [[3. 5.]\n"," [3. 5.]]\n","A prev lay  = [[1 2]\n"," [1 2]]\n"," Predicted value [[-0.3 -0.3]]\n"," Predicted value after one update (=learning) cycle [[-0.3 -0.3]]\n","Cost after first learning cycle : 8.09\n","\n","\n","====== Backward Propagation Layer 2 =======\n","dZ      =  [[-2.3 -3.3]]\n","dW      =  [[0. 0.]]\n","db      =  [[-5.6]]\n","A           = [[-0.3 -0.3]]\n","A prev lay  = [[0. 0.]\n"," [0. 0.]]\n","\n","\n","====== Backward Propagation Layer 1 =======\n","dZ      =  [[0. 0.]\n"," [0. 0.]]\n","dW      =  [[0. 0.]\n"," [0. 0.]]\n","db      =  [[0.]\n"," [0.]]\n","A           = [[0. 0.]\n"," [0. 0.]]\n","A prev lay  = [[1 1]\n"," [2 2]]\n"," Predicted value [[0.26 0.26]]\n"," Predicted value after one update (=learning) cycle [[0.26 0.26]]\n","Cost after second learning cycle : 5.267600000000001\n"],"name":"stdout"}]}]}