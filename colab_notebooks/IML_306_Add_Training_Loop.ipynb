{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"IML_306_Add_Training_Loop.ipynb","version":"0.3.2","provenance":[{"file_id":"1610xqRo2kOx8mfvzK3fWft-wNrwBBGPL","timestamp":1566992017998},{"file_id":"1qO3v9wS3vNQRhzyWfw2MJGAuX3TtVw0J","timestamp":1566987247685},{"file_id":"1S9bdyiAEoSoFnub1i90tniJjGu86Y4wu","timestamp":1566986399582}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"nRXZ2JSFmXlI","colab_type":"text"},"source":["# Add a Training Loop"]},{"cell_type":"code","metadata":{"id":"B_27n1bKPiJf","colab_type":"code","colab":{}},"source":["import numpy as np \n","from sklearn import metrics\n","\n","import numpy as np \n","from sklearn import metrics\n","\n","def loss(y_true, y_predicted, loss_function='mse'):\n","    if loss_function == 'mse':       \n","        return metrics.mean_squared_error( y_true, y_predicted)\n","    else:\n","        raise Exception('Loss metric is not defined.')\n","\n","def get_dz_from_loss(y, y_predicted, metric):\n","    if metric == 'mse':\n","        return y_predicted - y\n","    else:\n","        raise Exception('Loss metric is not defined.')\n","\n","def sigma(z, act_func):\n","    global _activation\n","    if act_func == 'relu':\n","       return np.maximum(z, np.zeros(z.shape))\n","    \n","    elif act_func == 'sigmoid':\n","      return 1.0/(1.0 + np.exp( -z ))\n","\n","    elif act_func == 'linear':\n","        return z\n","    else:\n","        raise Exception('Activation function is not defined.')\n","\n","def sigma_prime(z, act_func):\n","    if act_func == 'relu':\n","        return np.maximum(np.sign(z), np.zeros(z.shape)) # 1 if backward input >0, 0 otherwise; then diaganolize\n","\n","    elif act_func == 'sigmoid':\n","        h = sigma(z, 'sigmoid')\n","        return h *(1-h)\n","\n","    elif act_func == 'linear':\n","        return np.ones(z.shape)\n","\n","    else:\n","        raise Exception('Activation function is not defined.')\n","\n","class Layer:\n","    def __init__(self,input_dim, output_dim, activation_function='linear'):    \n","        self.activation = activation_function\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim \n","        if input_dim > 0:\n","            #self.b = np.random.randn( output_dim, 1 )       \n","            #self.W = np.random.randn( output_dim, input_dim )\n","            #self.dW = np.random.randn( output_dim, input_dim )\n","            #self.db = np.random.randn( output_dim, 1 )\n","            self.b  = np.ones( (output_dim, 1) )       \n","            self.W  = np.ones( (output_dim, input_dim) )\n","            self.dW = np.ones( (output_dim, input_dim) )\n","            self.db = np.ones( (output_dim, 1) )\n","        self.a = np.zeros( (output_dim,1) )\n","\n","    \n","    def set_weight(self, W ):\n","        self.W = W\n","      \n","    def set_bias(self, b ):\n","        self.b = b\n","  \n","    def compute_activation(self, a ): \n","        self.z =  np.add( np.dot(self.W, a), self.b)\n","        self.a =  sigma(self.z, self.activation)\n","    \n","    \n","    def print( self ):      \n","        print(f\"\\n====== Layer Info =======\")\n","        print(f\"a    = {self.a}\")\n","        if self.input_dim > 0: \n","          print(f\"W   =  {self.W}\")          \n","          print(f\"b   =  {self.b}\")  \n","    \n","\n","class Model:\n","    def __init__(self, input_dim):  \n","        self.neural_net = []\n","        self.neural_net.append(Layer(0 , input_dim, 'irrelevant'))    \n","\n","    def fit(self, training_data, epochs, learning_rate=0.001, verbose=1 ):\n","          self.learning_rate = learning_rate\n","          \n","          X_train, y_train = training_data\n","          num_train_datum = X_train.shape[1]\n","\n","          print(f\"\\n\\nStart training for following parameters :\")\n","          print(f\" N              =  {num_train_datum}\")\n","          print(f\" Shape of X     =  {X_train.shape}\")\n","          print(f\" Shape of y     =  {y_train.shape}\")\n","          print(f\" epochs         = {epochs}\")\n","          print(f\" learning_rate  =  {learning_rate}\")\n","    \n","          \n","          # Training Loop\n","          for epoch in range(1,epochs+1):        \n","              y_train_predicted = model.forward_propagation( X_train )\n","              self.backward_propagation( y_train, y_train_predicted, num_train_datum, verbose = verbose - 1 )\n","              self.update( learning_rate )\n","                            \n","              ##  - Backpropagation for whole training set is finished\n","              \n","              # After backpropagation\n","              # ... calculate the training loss\n","              y_train_predicted = model.forward_propagation( X_train )\n","              training_loss   = loss(y_train, y_train_predicted)\n","                            \n","              # If requested, print result of this round\n","              if (verbose > 0):\n","                print(f\"Epoch {epoch}: Train.-Loss   = { training_loss  }\")\n","\n","          ##### end of training loop \n","          return training_loss\n","\n","\n","    def add_layer(self, nr_neurons, activation='relu'):    \n","        layer_index = len(self.neural_net)\n","        input_dim = self.neural_net[layer_index - 1].output_dim\n","        new_layer = Layer( input_dim, nr_neurons, activation)\n","        self.neural_net.append( new_layer )\n","\n","\n","    def forward_propagation(self, input_vec ):\n","        self.neural_net[0].a = input_vec\n","        for layer_index in range(1,len(self.neural_net)):    \n","            _A_Prev = self.neural_net[layer_index-1].a                       \n","            self.neural_net[layer_index].compute_activation( _A_Prev )\n","        return  self.neural_net[layer_index].a\n","  \n","    def backward_propagation(self, y, y_predicted, num_train_datum, metric='mse', verbose=0):   \n","            nr_layers = len(self.neural_net)\n","            for layer_index in range(nr_layers-1,0,-1):\n","                if layer_index+1 == nr_layers: # if output layer\n","\n","                    dz = np.multiply(get_dz_from_loss(y, y_predicted, metric), \n","                                    sigma_prime(\n","                                        self.neural_net[layer_index].a, \n","                                        self.neural_net[layer_index].activation)\n","                    )        \n","                else: \n","                    dz = np.multiply(\n","                          np.dot(\n","                              self.neural_net[layer_index+1].W.T, \n","                              dz), \n","                          sigma_prime(\n","                                self.neural_net[layer_index].a, \n","                                self.neural_net[layer_index].activation)\n","                          )         \n","                dW = np.dot(dz, self.neural_net[layer_index-1].a.T) / num_train_datum\n","                db = np.sum(dz, axis=1, keepdims=True) / num_train_datum\n","\n","                # Update gradients\n","                self.neural_net[layer_index].dW = dW \n","                self.neural_net[layer_index].db = db \n","\n","                if (verbose > 0):\n","                  print(f\"\\n\\n====== Backward Propagation Layer {layer_index} =======\")\n","                  print(f\"dZ      =  {dz}\")          \n","                  print(f\"dW      =  {dW}\")\n","                  print(f\"db      =  {db}\")\n","                  print(f\"A           = {self.neural_net[layer_index].a}\") \n","                  print(f\"A prev lay  = {self.neural_net[layer_index-1].a}\") \n","                  \n","\n","    def update( self, learning_rate ):\n","        nr_layers = len(self.neural_net)\n","        for layer_index in range(1,nr_layers):        \n","            self.neural_net[layer_index].set_weight( self.neural_net[layer_index].W - learning_rate * self.neural_net[layer_index].dW )\n","            self.neural_net[layer_index].set_bias(  self.neural_net[layer_index].b  - learning_rate * self.neural_net[layer_index].db  )\n","    \n","\n","    def summary(self):\n","        print(\"MODEL SUMMARY\")\n","        for layer_index in range(len(self.neural_net)):        \n","          self.neural_net[layer_index].print()\n","          \n","        print(\"FINISHED MODEL SUMMARY\")\n","            \n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N8Zq6ItsuX8g","colab_type":"text"},"source":["# Test"]},{"cell_type":"code","metadata":{"id":"7hAmsf2BuaHq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"a8e28a33-c125-4bc6-9732-660f43181197","executionInfo":{"status":"ok","timestamp":1568301144659,"user_tz":-120,"elapsed":1089,"user":{"displayName":"Dieter Greipl","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCQkt4LlXZTbr43bodLOYJA_AMigYzivO_n1PBIcw=s64","userId":"09839720196215486633"}}},"source":["#Testing---------------------------------        \n","input_dim = 2\n","output_dim = 1\n","model = Model( input_dim )\n","#model.add_layer( 2, 'linear' )\n","model.add_layer( output_dim, 'linear' )\n","\n","# Play with different feature set lengths..\n","# N=1\n","X  = np.array([[1,1]])\n","y  = np.array( [[22]] )\n","\n","# N=2\n","#X  = np.array( [[0.,1.], [0.,1.]] ) \n","#y_true = np.array( [[0., 7.]] )\n","\n","\n","# N=3\n","#X  = np.array( [[0.,1., 2.0], [0.,1., 2.0]] ) \n","#y_true = np.array( [[-10., -11., -15.0]] )\n","\n","print( y.shape )\n","model.fit( (X.T, y) , 1000, verbose=1)\n"],"execution_count":9,"outputs":[{"output_type":"stream","text":["(1, 1)\n","\n","\n","Start training for following parameters :\n"," N              =  1\n"," Shape of X     =  (2, 1)\n"," Shape of y     =  (1, 1)\n"," epochs         = 1000\n"," learning_rate  =  0.001\n","Epoch 1: Train.-Loss   = 358.83724900000004\n","Epoch 2: Train.-Loss   = 356.68745504124104\n","Epoch 3: Train.-Loss   = 354.550540498089\n","Epoch 4: Train.-Loss   = 352.42642820996497\n","Epoch 5: Train.-Loss   = 350.31504147855895\n","Epoch 6: Train.-Loss   = 348.21630406506097\n","Epoch 7: Train.-Loss   = 346.1301401874072\n","Epoch 8: Train.-Loss   = 344.0564745175444\n","Epoch 9: Train.-Loss   = 341.99523217870984\n","Epoch 10: Train.-Loss   = 339.94633874272716\n","Epoch 11: Train.-Loss   = 337.90972022731955\n","Epoch 12: Train.-Loss   = 335.88530309343764\n","Epoch 13: Train.-Loss   = 333.8730142426049\n","Epoch 14: Train.-Loss   = 331.8727810142774\n","Epoch 15: Train.-Loss   = 329.8845311832209\n","Epoch 16: Train.-Loss   = 327.90819295690216\n","Epoch 17: Train.-Loss   = 325.9436949728974\n","Epoch 18: Train.-Loss   = 323.9909662963148\n","Epoch 19: Train.-Loss   = 322.0499364172336\n","Epoch 20: Train.-Loss   = 320.12053524815786\n","Epoch 21: Train.-Loss   = 318.2026931214861\n","Epoch 22: Train.-Loss   = 316.2963407869953\n","Epoch 23: Train.-Loss   = 314.40140940934043\n","Epoch 24: Train.-Loss   = 312.5178305655691\n","Epoch 25: Train.-Loss   = 310.6455362426507\n","Epoch 26: Train.-Loss   = 308.784458835021\n","Epoch 27: Train.-Loss   = 306.9345311421404\n","Epoch 28: Train.-Loss   = 305.0956863660679\n","Epoch 29: Train.-Loss   = 303.2678581090487\n","Epoch 30: Train.-Loss   = 301.4509803711175\n","Epoch 31: Train.-Loss   = 299.64498754771404\n","Epoch 32: Train.-Loss   = 297.84981442731566\n","Epoch 33: Train.-Loss   = 296.0653961890816\n","Epoch 34: Train.-Loss   = 294.2916684005129\n","Epoch 35: Train.-Loss   = 292.52856701512536\n","Epoch 36: Train.-Loss   = 290.7760283701378\n","Epoch 37: Train.-Loss   = 289.0339891841722\n","Epoch 38: Train.-Loss   = 287.3023865549699\n","Epoch 39: Train.-Loss   = 285.581157957119\n","Epoch 40: Train.-Loss   = 283.870241239798\n","Epoch 41: Train.-Loss   = 282.16957462453036\n","Epoch 42: Train.-Loss   = 280.4790967029548\n","Epoch 43: Train.-Loss   = 278.7987464346074\n","Epoch 44: Train.-Loss   = 277.12846314471767\n","Epoch 45: Train.-Loss   = 275.4681865220176\n","Epoch 46: Train.-Loss   = 273.8178566165642\n","Epoch 47: Train.-Loss   = 272.17741383757436\n","Epoch 48: Train.-Loss   = 270.5467989512735\n","Epoch 49: Train.-Loss   = 268.9259530787564\n","Epoch 50: Train.-Loss   = 267.31481769386164\n","Epoch 51: Train.-Loss   = 265.71333462105764\n","Epoch 52: Train.-Loss   = 264.12144603334286\n","Epoch 53: Train.-Loss   = 262.53909445015717\n","Epoch 54: Train.-Loss   = 260.96622273530625\n","Epoch 55: Train.-Loss   = 259.402774094899\n","Epoch 56: Train.-Loss   = 257.84869207529647\n","Epoch 57: Train.-Loss   = 256.30392056107337\n","Epoch 58: Train.-Loss   = 254.76840377299203\n","Epoch 59: Train.-Loss   = 253.24208626598804\n","Epoch 60: Train.-Loss   = 251.72491292716845\n","Epoch 61: Train.-Loss   = 250.21682897382178\n","Epoch 62: Train.-Loss   = 248.7177799514396\n","Epoch 63: Train.-Loss   = 247.22771173175056\n","Epoch 64: Train.-Loss   = 245.7465705107656\n","Epoch 65: Train.-Loss   = 244.2743028068356\n","Epoch 66: Train.-Loss   = 242.81085545871986\n","Epoch 67: Train.-Loss   = 241.35617562366667\n","Epoch 68: Train.-Loss   = 239.91021077550528\n","Epoch 69: Train.-Loss   = 238.47290870274924\n","Epoch 70: Train.-Loss   = 237.04421750671108\n","Epoch 71: Train.-Loss   = 235.6240855996284\n","Epoch 72: Train.-Loss   = 234.212461702801\n","Epoch 73: Train.-Loss   = 232.80929484473953\n","Epoch 74: Train.-Loss   = 231.4145343593247\n","Epoch 75: Train.-Loss   = 230.02812988397798\n","Epoch 76: Train.-Loss   = 228.65003135784306\n","Epoch 77: Train.-Loss   = 227.28018901997822\n","Epoch 78: Train.-Loss   = 225.91855340755956\n","Epoch 79: Train.-Loss   = 224.56507535409486\n","Epoch 80: Train.-Loss   = 223.21970598764847\n","Epoch 81: Train.-Loss   = 221.8823967290765\n","Epoch 82: Train.-Loss   = 220.55309929027257\n","Epoch 83: Train.-Loss   = 219.23176567242453\n","Epoch 84: Train.-Loss   = 217.91834816428104\n","Epoch 85: Train.-Loss   = 216.6127993404288\n","Epoch 86: Train.-Loss   = 215.31507205958033\n","Epoch 87: Train.-Loss   = 214.02511946287137\n","Epoch 88: Train.-Loss   = 212.74289497216932\n","Epoch 89: Train.-Loss   = 211.46835228839106\n","Epoch 90: Train.-Loss   = 210.20144538983135\n","Epoch 91: Train.-Loss   = 208.94212853050084\n","Epoch 92: Train.-Loss   = 207.69035623847464\n","Epoch 93: Train.-Loss   = 206.44608331424996\n","Epoch 94: Train.-Loss   = 205.2092648291143\n","Epoch 95: Train.-Loss   = 203.9798561235231\n","Epoch 96: Train.-Loss   = 202.75781280548705\n","Epoch 97: Train.-Loss   = 201.54309074896935\n","Epoch 98: Train.-Loss   = 200.33564609229225\n","Epoch 99: Train.-Loss   = 199.13543523655335\n","Epoch 100: Train.-Loss   = 197.94241484405117\n","Epoch 101: Train.-Loss   = 196.7565418367205\n","Epoch 102: Train.-Loss   = 195.57777339457667\n","Epoch 103: Train.-Loss   = 194.40606695416977\n","Epoch 104: Train.-Loss   = 193.24138020704737\n","Epoch 105: Train.-Loss   = 192.083671098227\n","Epoch 106: Train.-Loss   = 190.93289782467747\n","Epoch 107: Train.-Loss   = 189.78901883380982\n","Epoch 108: Train.-Loss   = 188.65199282197648\n","Epoch 109: Train.-Loss   = 187.52177873297998\n","Epoch 110: Train.-Loss   = 186.39833575659074\n","Epoch 111: Train.-Loss   = 185.28162332707296\n","Epoch 112: Train.-Loss   = 184.1716011217205\n","Epoch 113: Train.-Loss   = 183.06822905940024\n","Epoch 114: Train.-Loss   = 181.97146729910543\n","Epoch 115: Train.-Loss   = 180.88127623851645\n","Epoch 116: Train.-Loss   = 179.79761651257152\n","Epoch 117: Train.-Loss   = 178.7204489920447\n","Epoch 118: Train.-Loss   = 177.64973478213335\n","Epoch 119: Train.-Loss   = 176.58543522105364\n","Epoch 120: Train.-Loss   = 175.5275118786443\n","Epoch 121: Train.-Loss   = 174.47592655497937\n","Epoch 122: Train.-Loss   = 173.43064127898847\n","Epoch 123: Train.-Loss   = 172.39161830708608\n","Epoch 124: Train.-Loss   = 171.3588201218083\n","Epoch 125: Train.-Loss   = 170.33220943045853\n","Epoch 126: Train.-Loss   = 169.31174916376068\n","Epoch 127: Train.-Loss   = 168.2974024745206\n","Epoch 128: Train.-Loss   = 167.28913273629573\n","Epoch 129: Train.-Loss   = 166.28690354207257\n","Epoch 130: Train.-Loss   = 165.29067870295202\n","Epoch 131: Train.-Loss   = 164.3004222468427\n","Epoch 132: Train.-Loss   = 163.31609841716184\n","Epoch 133: Train.-Loss   = 162.3376716715446\n","Epoch 134: Train.-Loss   = 161.36510668056036\n","Epoch 135: Train.-Loss   = 160.39836832643712\n","Epoch 136: Train.-Loss   = 159.43742170179345\n","Epoch 137: Train.-Loss   = 158.482232108378\n","Epoch 138: Train.-Loss   = 157.5327650558167\n","Epoch 139: Train.-Loss   = 156.5889862603673\n","Epoch 140: Train.-Loss   = 155.65086164368148\n","Epoch 141: Train.-Loss   = 154.71835733157414\n","Epoch 142: Train.-Loss   = 153.7914396528007\n","Epoch 143: Train.-Loss   = 152.87007513784076\n","Epoch 144: Train.-Loss   = 151.95423051768995\n","Epoch 145: Train.-Loss   = 151.04387272265845\n","Epoch 146: Train.-Loss   = 150.13896888117702\n","Epoch 147: Train.-Loss   = 149.2394863186099\n","Epoch 148: Train.-Loss   = 148.34539255607507\n","Epoch 149: Train.-Loss   = 147.45665530927164\n","Epoch 150: Train.-Loss   = 146.57324248731382\n","Epoch 151: Train.-Loss   = 145.6951221915723\n","Epoch 152: Train.-Loss   = 144.8222627145226\n","Epoch 153: Train.-Loss   = 143.9546325385999\n","Epoch 154: Train.-Loss   = 143.09220033506116\n","Epoch 155: Train.-Loss   = 142.23493496285383\n","Epoch 156: Train.-Loss   = 141.38280546749135\n","Epoch 157: Train.-Loss   = 140.53578107993562\n","Epoch 158: Train.-Loss   = 139.69383121548572\n","Epoch 159: Train.-Loss   = 138.85692547267374\n","Epoch 160: Train.-Loss   = 138.02503363216695\n","Epoch 161: Train.-Loss   = 137.19812565567662\n","Epoch 162: Train.-Loss   = 136.37617168487344\n","Epoch 163: Train.-Loss   = 135.55914204030935\n","Epoch 164: Train.-Loss   = 134.74700722034586\n","Epoch 165: Train.-Loss   = 133.9397379000888\n","Epoch 166: Train.-Loss   = 133.13730493032935\n","Epoch 167: Train.-Loss   = 132.3396793364917\n","Epoch 168: Train.-Loss   = 131.5468323175868\n","Epoch 169: Train.-Loss   = 130.75873524517212\n","Epoch 170: Train.-Loss   = 129.97535966231834\n","Epoch 171: Train.-Loss   = 129.19667728258136\n","Epoch 172: Train.-Loss   = 128.42265998898142\n","Epoch 173: Train.-Loss   = 127.65327983298745\n","Epoch 174: Train.-Loss   = 126.888509033508\n","Epoch 175: Train.-Loss   = 126.12831997588827\n","Epoch 176: Train.-Loss   = 125.3726852109127\n","Epoch 177: Train.-Loss   = 124.62157745381417\n","Epoch 178: Train.-Loss   = 123.87496958328839\n","Epoch 179: Train.-Loss   = 123.13283464051487\n","Epoch 180: Train.-Loss   = 122.39514582818356\n","Epoch 181: Train.-Loss   = 121.66187650952693\n","Epoch 182: Train.-Loss   = 120.93300020735832\n","Epoch 183: Train.-Loss   = 120.20849060311603\n","Epoch 184: Train.-Loss   = 119.48832153591276\n","Epoch 185: Train.-Loss   = 118.77246700159111\n","Epoch 186: Train.-Loss   = 118.06090115178456\n","Epoch 187: Train.-Loss   = 117.35359829298422\n","Epoch 188: Train.-Loss   = 116.65053288561091\n","Epoch 189: Train.-Loss   = 115.95167954309322\n","Epoch 190: Train.-Loss   = 115.25701303095057\n","Epoch 191: Train.-Loss   = 114.56650826588215\n","Epoch 192: Train.-Loss   = 113.88014031486124\n","Epoch 193: Train.-Loss   = 113.19788439423492\n","Epoch 194: Train.-Loss   = 112.51971586882905\n","Epoch 195: Train.-Loss   = 111.84561025105889\n","Epoch 196: Train.-Loss   = 111.17554320004479\n","Epoch 197: Train.-Loss   = 110.50949052073332\n","Epoch 198: Train.-Loss   = 109.84742816302362\n","Epoch 199: Train.-Loss   = 109.1893322208989\n","Epoch 200: Train.-Loss   = 108.53517893156351\n","Epoch 201: Train.-Loss   = 107.88494467458452\n","Epoch 202: Train.-Loss   = 107.23860597103914\n","Epoch 203: Train.-Loss   = 106.59613948266664\n","Epoch 204: Train.-Loss   = 105.95752201102596\n","Epoch 205: Train.-Loss   = 105.32273049665791\n","Epoch 206: Train.-Loss   = 104.69174201825244\n","Epoch 207: Train.-Loss   = 104.06453379182106\n","Epoch 208: Train.-Loss   = 103.44108316987429\n","Epoch 209: Train.-Loss   = 102.82136764060355\n","Epoch 210: Train.-Loss   = 102.20536482706869\n","Epoch 211: Train.-Loss   = 101.59305248638974\n","Epoch 212: Train.-Loss   = 100.98440850894377\n","Epoch 213: Train.-Loss   = 100.37941091756667\n","Epoch 214: Train.-Loss   = 99.77803786675956\n","Epoch 215: Train.-Loss   = 99.18026764189982\n","Epoch 216: Train.-Loss   = 98.5860786584572\n","Epoch 217: Train.-Loss   = 97.99544946121436\n","Epoch 218: Train.-Loss   = 97.40835872349226\n","Epoch 219: Train.-Loss   = 96.8247852463798\n","Epoch 220: Train.-Loss   = 96.2447079579687\n","Epoch 221: Train.-Loss   = 95.66810591259251\n","Epoch 222: Train.-Loss   = 95.09495829007021\n","Epoch 223: Train.-Loss   = 94.52524439495444\n","Epoch 224: Train.-Loss   = 93.95894365578427\n","Epoch 225: Train.-Loss   = 93.39603562434245\n","Epoch 226: Train.-Loss   = 92.83649997491702\n","Epoch 227: Train.-Loss   = 92.28031650356726\n","Epoch 228: Train.-Loss   = 91.72746512739442\n","Epoch 229: Train.-Loss   = 91.1779258838162\n","Epoch 230: Train.-Loss   = 90.63167892984629\n","Epoch 231: Train.-Loss   = 90.08870454137757\n","Epoch 232: Train.-Loss   = 89.54898311247018\n","Epoch 233: Train.-Loss   = 89.01249515464336\n","Epoch 234: Train.-Loss   = 88.47922129617186\n","Epoch 235: Train.-Loss   = 87.94914228138653\n","Epoch 236: Train.-Loss   = 87.42223896997868\n","Epoch 237: Train.-Loss   = 86.89849233630957\n","Epoch 238: Train.-Loss   = 86.37788346872274\n","Epoch 239: Train.-Loss   = 85.86039356886162\n","Epoch 240: Train.-Loss   = 85.34600395099059\n","Epoch 241: Train.-Loss   = 84.83469604132016\n","Epoch 242: Train.-Loss   = 84.32645137733661\n","Epoch 243: Train.-Loss   = 83.821251607135\n","Epoch 244: Train.-Loss   = 83.3190784887566\n","Epoch 245: Train.-Loss   = 82.8199138895305\n","Epoch 246: Train.-Loss   = 82.32373978541835\n","Epoch 247: Train.-Loss   = 81.8305382603639\n","Epoch 248: Train.-Loss   = 81.34029150564602\n","Epoch 249: Train.-Loss   = 80.8529818192357\n","Epoch 250: Train.-Loss   = 80.36859160515668\n","Epoch 251: Train.-Loss   = 79.88710337285018\n","Epoch 252: Train.-Loss   = 79.4084997365434\n","Epoch 253: Train.-Loss   = 78.9327634146218\n","Epoch 254: Train.-Loss   = 78.4598772290048\n","Epoch 255: Train.-Loss   = 77.98982410452581\n","Epoch 256: Train.-Loss   = 77.5225870683156\n","Epoch 257: Train.-Loss   = 77.05814924918934\n","Epoch 258: Train.-Loss   = 76.59649387703743\n","Epoch 259: Train.-Loss   = 76.13760428222008\n","Epoch 260: Train.-Loss   = 75.6814638949653\n","Epoch 261: Train.-Loss   = 75.22805624477057\n","Epoch 262: Train.-Loss   = 74.77736495980814\n","Epoch 263: Train.-Loss   = 74.32937376633392\n","Epoch 264: Train.-Loss   = 73.88406648809983\n","Epoch 265: Train.-Loss   = 73.44142704576961\n","Epoch 266: Train.-Loss   = 73.0014394563384\n","Epoch 267: Train.-Loss   = 72.56408783255546\n","Epoch 268: Train.-Loss   = 72.12935638235062\n","Epoch 269: Train.-Loss   = 71.69722940826395\n","Epoch 270: Train.-Loss   = 71.26769130687902\n","Epoch 271: Train.-Loss   = 70.84072656825951\n","Epoch 272: Train.-Loss   = 70.41631977538907\n","Epoch 273: Train.-Loss   = 69.9944556036147\n","Epoch 274: Train.-Loss   = 69.57511882009342\n","Epoch 275: Train.-Loss   = 69.15829428324221\n","Epoch 276: Train.-Loss   = 68.7439669421913\n","Epoch 277: Train.-Loss   = 68.33212183624065\n","Epoch 278: Train.-Loss   = 67.92274409431974\n","Epoch 279: Train.-Loss   = 67.51581893445068\n","Epoch 280: Train.-Loss   = 67.11133166321437\n","Epoch 281: Train.-Loss   = 66.70926767522003\n","Epoch 282: Train.-Loss   = 66.30961245257778\n","Epoch 283: Train.-Loss   = 65.9123515643744\n","Epoch 284: Train.-Loss   = 65.51747066615228\n","Epoch 285: Train.-Loss   = 65.12495549939133\n","Epoch 286: Train.-Loss   = 64.73479189099446\n","Epoch 287: Train.-Loss   = 64.34696575277552\n","Epoch 288: Train.-Loss   = 63.96146308095065\n","Epoch 289: Train.-Loss   = 63.57826995563269\n","Epoch 290: Train.-Loss   = 63.197372540328516\n","Epoch 291: Train.-Loss   = 62.8187570814394\n","Epoch 292: Train.-Loss   = 62.44240990776448\n","Epoch 293: Train.-Loss   = 62.06831743000708\n","Epoch 294: Train.-Loss   = 61.69646614028393\n","Epoch 295: Train.-Loss   = 61.32684261163751\n","Epoch 296: Train.-Loss   = 60.959433497551174\n","Epoch 297: Train.-Loss   = 60.59422553146734\n","Epoch 298: Train.-Loss   = 60.231205526308315\n","Epoch 299: Train.-Loss   = 59.87036037400018\n","Epoch 300: Train.-Loss   = 59.511677044999566\n","Epoch 301: Train.-Loss   = 59.15514258782298\n","Epoch 302: Train.-Loss   = 58.80074412857932\n","Epoch 303: Train.-Loss   = 58.44846887050503\n","Epoch 304: Train.-Loss   = 58.09830409350182\n","Epoch 305: Train.-Loss   = 57.75023715367765\n","Epoch 306: Train.-Loss   = 57.40425548288996\n","Epoch 307: Train.-Loss   = 57.06034658829194\n","Epoch 308: Train.-Loss   = 56.71849805188148\n","Epoch 309: Train.-Loss   = 56.37869753005267\n","Epoch 310: Train.-Loss   = 56.04093275315011\n","Epoch 311: Train.-Loss   = 55.705191525025974\n","Epoch 312: Train.-Loss   = 55.37146172259955\n","Epoch 313: Train.-Loss   = 55.03973129541945\n","Epoch 314: Train.-Loss   = 54.7099882652286\n","Epoch 315: Train.-Loss   = 54.38222072553163\n","Epoch 316: Train.-Loss   = 54.05641684116495\n","Epoch 317: Train.-Loss   = 53.732564847869526\n","Epoch 318: Train.-Loss   = 53.410653051865964\n","Epoch 319: Train.-Loss   = 53.090669829432215\n","Epoch 320: Train.-Loss   = 52.77260362648409\n","Epoch 321: Train.-Loss   = 52.456442958157794\n","Epoch 322: Train.-Loss   = 52.14217640839547\n","Epoch 323: Train.-Loss   = 51.82979262953276\n","Epoch 324: Train.-Loss   = 51.51928034188925\n","Epoch 325: Train.-Loss   = 51.21062833336099\n","Epoch 326: Train.-Loss   = 50.90382545901581\n","Epoch 327: Train.-Loss   = 50.59886064069085\n","Epoch 328: Train.-Loss   = 50.29572286659249\n","Epoch 329: Train.-Loss   = 49.9944011908987\n","Epoch 330: Train.-Loss   = 49.694884733364\n","Epoch 331: Train.-Loss   = 49.39716267892646\n","Epoch 332: Train.-Loss   = 49.10122427731701\n","Epoch 333: Train.-Loss   = 48.80705884267161\n","Epoch 334: Train.-Loss   = 48.51465575314518\n","Epoch 335: Train.-Loss   = 48.22400445052808\n","Epoch 336: Train.-Loss   = 47.935094439864976\n","Epoch 337: Train.-Loss   = 47.64791528907574\n","Epoch 338: Train.-Loss   = 47.36245662857889\n","Epoch 339: Train.-Loss   = 47.07870815091709\n","Epoch 340: Train.-Loss   = 46.79665961038496\n","Epoch 341: Train.-Loss   = 46.516300822659154\n","Epoch 342: Train.-Loss   = 46.23762166443062\n","Epoch 343: Train.-Loss   = 45.96061207303901\n","Epoch 344: Train.-Loss   = 45.68526204610943\n","Epoch 345: Train.-Loss   = 45.41156164119121\n","Epoch 346: Train.-Loss   = 45.13950097539884\n","Epoch 347: Train.-Loss   = 44.869070225055246\n","Epoch 348: Train.-Loss   = 44.600259625336946\n","Epoch 349: Train.-Loss   = 44.33305946992154\n","Epoch 350: Train.-Loss   = 44.067460110637256\n","Epoch 351: Train.-Loss   = 43.80345195711444\n","Epoch 352: Train.-Loss   = 43.541025476439344\n","Epoch 353: Train.-Loss   = 43.28017119281\n","Epoch 354: Train.-Loss   = 43.02087968719386\n","Epoch 355: Train.-Loss   = 42.76314159698788\n","Epoch 356: Train.-Loss   = 42.50694761568033\n","Epoch 357: Train.-Loss   = 42.252288492514786\n","Epoch 358: Train.-Loss   = 41.999155032156146\n","Epoch 359: Train.-Loss   = 41.7475380943585\n","Epoch 360: Train.-Loss   = 41.497428593635185\n","Epoch 361: Train.-Loss   = 41.248817498930734\n","Epoch 362: Train.-Loss   = 41.00169583329462\n","Epoch 363: Train.-Loss   = 40.75605467355736\n","Epoch 364: Train.-Loss   = 40.51188515000808\n","Epoch 365: Train.-Loss   = 40.26917844607438\n","Epoch 366: Train.-Loss   = 40.02792579800394\n","Epoch 367: Train.-Loss   = 39.788118494548065\n","Epoch 368: Train.-Loss   = 39.54974787664724\n","Epoch 369: Train.-Loss   = 39.31280533711825\n","Epoch 370: Train.-Loss   = 39.0772823203436\n","Epoch 371: Train.-Loss   = 38.84317032196241\n","Epoch 372: Train.-Loss   = 38.610460888563516\n","Epoch 373: Train.-Loss   = 38.379145617380146\n","Epoch 374: Train.-Loss   = 38.149216155986416\n","Epoch 375: Train.-Loss   = 37.920664201995905\n","Epoch 376: Train.-Loss   = 37.69348150276173\n","Epoch 377: Train.-Loss   = 37.467659855078686\n","Epoch 378: Train.-Loss   = 37.243191104886904\n","Epoch 379: Train.-Loss   = 37.020067146977546\n","Epoch 380: Train.-Loss   = 36.798279924700005\n","Epoch 381: Train.-Loss   = 36.57782142967113\n","Epoch 382: Train.-Loss   = 36.35868370148599\n","Epoch 383: Train.-Loss   = 36.14085882743041\n","Epoch 384: Train.-Loss   = 35.92433894219526\n","Epoch 385: Train.-Loss   = 35.70911622759256\n","Epoch 386: Train.-Loss   = 35.495182912273066\n","Epoch 387: Train.-Loss   = 35.282531271445656\n","Epoch 388: Train.-Loss   = 35.07115362659843\n","Epoch 389: Train.-Loss   = 34.861042345221485\n","Epoch 390: Train.-Loss   = 34.652189840531264\n","Epoch 391: Train.-Loss   = 34.44458857119662\n","Epoch 392: Train.-Loss   = 34.238231041066555\n","Epoch 393: Train.-Loss   = 34.03310979889955\n","Epoch 394: Train.-Loss   = 33.82921743809433\n","Epoch 395: Train.-Loss   = 33.626546596422706\n","Epoch 396: Train.-Loss   = 33.42508995576357\n","Epoch 397: Train.-Loss   = 33.224840241838585\n","Epoch 398: Train.-Loss   = 33.025790223949706\n","Epoch 399: Train.-Loss   = 32.827932714718024\n","Epoch 400: Train.-Loss   = 32.63126056982417\n","Epoch 401: Train.-Loss   = 32.43576668775034\n","Epoch 402: Train.-Loss   = 32.241444009524024\n","Epoch 403: Train.-Loss   = 32.04828551846296\n","Epoch 404: Train.-Loss   = 31.856284239921848\n","Epoch 405: Train.-Loss   = 31.66543324104045\n","Epoch 406: Train.-Loss   = 31.4757256304934\n","Epoch 407: Train.-Loss   = 31.287154558241074\n","Epoch 408: Train.-Loss   = 31.099713215282655\n","Epoch 409: Train.-Loss   = 30.91339483340992\n","Epoch 410: Train.-Loss   = 30.728192684962973\n","Epoch 411: Train.-Loss   = 30.544100082587367\n","Epoch 412: Train.-Loss   = 30.361110378992567\n","Epoch 413: Train.-Loss   = 30.179216966712\n","Epoch 414: Train.-Loss   = 29.99841327786445\n","Epoch 415: Train.-Loss   = 29.81869278391676\n","Epoch 416: Train.-Loss   = 29.640048995448314\n","Epoch 417: Train.-Loss   = 29.462475461916576\n","Epoch 418: Train.-Loss   = 29.285965771424213\n","Epoch 419: Train.-Loss   = 29.110513550487624\n","Epoch 420: Train.-Loss   = 28.93611246380665\n","Epoch 421: Train.-Loss   = 28.762756214035985\n","Epoch 422: Train.-Loss   = 28.590438541557695\n","Epoch 423: Train.-Loss   = 28.41915322425523\n","Epoch 424: Train.-Loss   = 28.248894077288714\n","Epoch 425: Train.-Loss   = 28.079654952871657\n","Epoch 426: Train.-Loss   = 27.911429740049012\n","Epoch 427: Train.-Loss   = 27.744212364476372\n","Epoch 428: Train.-Loss   = 27.577996788200792\n","Epoch 429: Train.-Loss   = 27.412777009442703\n","Epoch 430: Train.-Loss   = 27.248547062379135\n","Epoch 431: Train.-Loss   = 27.085301016928405\n","Epoch 432: Train.-Loss   = 26.923032978535982\n","Epoch 433: Train.-Loss   = 26.761737087961595\n","Epoch 434: Train.-Loss   = 26.601407521067614\n","Epoch 435: Train.-Loss   = 26.442038488608876\n","Epoch 436: Train.-Loss   = 26.283624236023638\n","Epoch 437: Train.-Loss   = 26.126159043225606\n","Epoch 438: Train.-Loss   = 25.969637224397633\n","Epoch 439: Train.-Loss   = 25.81405312778631\n","Epoch 440: Train.-Loss   = 25.65940113549771\n","Epoch 441: Train.-Loss   = 25.505675663294916\n","Epoch 442: Train.-Loss   = 25.352871160396134\n","Epoch 443: Train.-Loss   = 25.200982109274204\n","Epoch 444: Train.-Loss   = 25.05000302545752\n","Epoch 445: Train.-Loss   = 24.899928457331995\n","Epoch 446: Train.-Loss   = 24.750752985944114\n","Epoch 447: Train.-Loss   = 24.602471224805342\n","Epoch 448: Train.-Loss   = 24.4550778196975\n","Epoch 449: Train.-Loss   = 24.30856744847971\n","Epoch 450: Train.-Loss   = 24.16293482089585\n","Epoch 451: Train.-Loss   = 24.018174678383872\n","Epoch 452: Train.-Loss   = 23.87428179388567\n","Epoch 453: Train.-Loss   = 23.73125097165852\n","Epoch 454: Train.-Loss   = 23.589077047087287\n","Epoch 455: Train.-Loss   = 23.447754886498185\n","Epoch 456: Train.-Loss   = 23.307279386973185\n","Epoch 457: Train.-Loss   = 23.167645476165834\n","Epoch 458: Train.-Loss   = 23.02884811211812\n","Epoch 459: Train.-Loss   = 22.89088228307841\n","Epoch 460: Train.-Loss   = 22.753743007320463\n","Epoch 461: Train.-Loss   = 22.617425332963617\n","Epoch 462: Train.-Loss   = 22.481924337793835\n","Epoch 463: Train.-Loss   = 22.347235129086098\n","Epoch 464: Train.-Loss   = 22.213352843427753\n","Epoch 465: Train.-Loss   = 22.08027264654275\n","Epoch 466: Train.-Loss   = 21.947989733117314\n","Epoch 467: Train.-Loss   = 21.81649932662623\n","Epoch 468: Train.-Loss   = 21.685796679160386\n","Epoch 469: Train.-Loss   = 21.555877071255544\n","Epoch 470: Train.-Loss   = 21.42673581172168\n","Epoch 471: Train.-Loss   = 21.29836823747363\n","Epoch 472: Train.-Loss   = 21.17076971336294\n","Epoch 473: Train.-Loss   = 21.043935632010175\n","Epoch 474: Train.-Loss   = 20.91786141363882\n","Epoch 475: Train.-Loss   = 20.7925425059097\n","Epoch 476: Train.-Loss   = 20.667974383756768\n","Epoch 477: Train.-Loss   = 20.54415254922372\n","Epoch 478: Train.-Loss   = 20.421072531301327\n","Epoch 479: Train.-Loss   = 20.298729885766306\n","Epoch 480: Train.-Loss   = 20.17712019502065\n","Epoch 481: Train.-Loss   = 20.056239067932296\n","Epoch 482: Train.-Loss   = 19.93608213967631\n","Epoch 483: Train.-Loss   = 19.8166450715775\n","Epoch 484: Train.-Loss   = 19.697923550953696\n","Epoch 485: Train.-Loss   = 19.57991329095995\n","Epoch 486: Train.-Loss   = 19.462610030433794\n","Epoch 487: Train.-Loss   = 19.346009533741448\n","Epoch 488: Train.-Loss   = 19.23010759062481\n","Epoch 489: Train.-Loss   = 19.1149000160494\n","Epoch 490: Train.-Loss   = 19.000382650053247\n","Epoch 491: Train.-Loss   = 18.886551357596787\n","Epoch 492: Train.-Loss   = 18.773402028413404\n","Epoch 493: Train.-Loss   = 18.660930576861162\n","Epoch 494: Train.-Loss   = 18.549132941775184\n","Epoch 495: Train.-Loss   = 18.438005086321017\n","Epoch 496: Train.-Loss   = 18.32754299784888\n","Epoch 497: Train.-Loss   = 18.21774268774875\n","Epoch 498: Train.-Loss   = 18.108600191306444\n","Epoch 499: Train.-Loss   = 18.000111567560335\n","Epoch 500: Train.-Loss   = 17.892272899159085\n","Epoch 501: Train.-Loss   = 17.785080292220233\n","Epoch 502: Train.-Loss   = 17.67852987618953\n","Epoch 503: Train.-Loss   = 17.57261780370127\n","Epoch 504: Train.-Loss   = 17.467340250439275\n","Epoch 505: Train.-Loss   = 17.36269341499892\n","Epoch 506: Train.-Loss   = 17.258673518749664\n","Epoch 507: Train.-Loss   = 17.155276805698836\n","Epoch 508: Train.-Loss   = 17.052499542355864\n","Epoch 509: Train.-Loss   = 16.95033801759763\n","Epoch 510: Train.-Loss   = 16.8487885425342\n","Epoch 511: Train.-Loss   = 16.747847450375875\n","Epoch 512: Train.-Loss   = 16.64751109630067\n","Epoch 513: Train.-Loss   = 16.54777585732274\n","Epoch 514: Train.-Loss   = 16.44863813216152\n","Epoch 515: Train.-Loss   = 16.350094341111713\n","Epoch 516: Train.-Loss   = 16.25214092591412\n","Epoch 517: Train.-Loss   = 16.154774349626965\n","Epoch 518: Train.-Loss   = 16.05799109649836\n","Epoch 519: Train.-Loss   = 15.961787671839215\n","Epoch 520: Train.-Loss   = 15.866160601897224\n","Epoch 521: Train.-Loss   = 15.771106433731278\n","Epoch 522: Train.-Loss   = 15.676621735086803\n","Epoch 523: Train.-Loss   = 15.582703094271894\n","Epoch 524: Train.-Loss   = 15.489347120034106\n","Epoch 525: Train.-Loss   = 15.396550441437979\n","Epoch 526: Train.-Loss   = 15.304309707743336\n","Epoch 527: Train.-Loss   = 15.21262158828424\n","Epoch 528: Train.-Loss   = 15.121482772348834\n","Epoch 529: Train.-Loss   = 15.030889969059691\n","Epoch 530: Train.-Loss   = 14.940839907255054\n","Epoch 531: Train.-Loss   = 14.85132933537068\n","Epoch 532: Train.-Loss   = 14.76235502132248\n","Epoch 533: Train.-Loss   = 14.673913752389725\n","Epoch 534: Train.-Loss   = 14.586002335099167\n","Epoch 535: Train.-Loss   = 14.498617595109616\n","Epoch 536: Train.-Loss   = 14.411756377097314\n","Epoch 537: Train.-Loss   = 14.325415544642107\n","Epoch 538: Train.-Loss   = 14.239591980114163\n","Epoch 539: Train.-Loss   = 14.154282584561296\n","Epoch 540: Train.-Loss   = 14.069484277597192\n","Epoch 541: Train.-Loss   = 13.98519399729012\n","Epoch 542: Train.-Loss   = 13.901408700052356\n","Epoch 543: Train.-Loss   = 13.818125360530326\n","Epoch 544: Train.-Loss   = 13.735340971495393\n","Epoch 545: Train.-Loss   = 13.653052543735162\n","Epoch 546: Train.-Loss   = 13.571257105945657\n","Epoch 547: Train.-Loss   = 13.489951704623937\n","Epoch 548: Train.-Loss   = 13.409133403961535\n","Epoch 549: Train.-Loss   = 13.328799285738405\n","Epoch 550: Train.-Loss   = 13.24894644921755\n","Epoch 551: Train.-Loss   = 13.169572011040277\n","Epoch 552: Train.-Loss   = 13.09067310512214\n","Epoch 553: Train.-Loss   = 13.012246882549357\n","Epoch 554: Train.-Loss   = 12.934290511475997\n","Epoch 555: Train.-Loss   = 12.856801177021755\n","Epoch 556: Train.-Loss   = 12.77977608117021\n","Epoch 557: Train.-Loss   = 12.703212442667922\n","Epoch 558: Train.-Loss   = 12.627107496923896\n","Epoch 559: Train.-Loss   = 12.551458495909836\n","Epoch 560: Train.-Loss   = 12.476262708060826\n","Epoch 561: Train.-Loss   = 12.401517418176832\n","Epoch 562: Train.-Loss   = 12.327219927324528\n","Epoch 563: Train.-Loss   = 12.253367552739949\n","Epoch 564: Train.-Loss   = 12.179957627731472\n","Epoch 565: Train.-Loss   = 12.106987501583728\n","Epoch 566: Train.-Loss   = 12.03445453946174\n","Epoch 567: Train.-Loss   = 11.962356122315834\n","Epoch 568: Train.-Loss   = 11.890689646787042\n","Epoch 569: Train.-Loss   = 11.819452525113121\n","Epoch 570: Train.-Loss   = 11.748642185035182\n","Epoch 571: Train.-Loss   = 11.678256069704627\n","Epoch 572: Train.-Loss   = 11.608291637591007\n","Epoch 573: Train.-Loss   = 11.538746362390222\n","Epoch 574: Train.-Loss   = 11.46961773293314\n","Epoch 575: Train.-Loss   = 11.400903253095137\n","Epoch 576: Train.-Loss   = 11.332600441705836\n","Epoch 577: Train.-Loss   = 11.264706832459572\n","Epoch 578: Train.-Loss   = 11.197219973826304\n","Epoch 579: Train.-Loss   = 11.130137428963108\n","Epoch 580: Train.-Loss   = 11.06345677562619\n","Epoch 581: Train.-Loss   = 10.997175606083422\n","Epoch 582: Train.-Loss   = 10.931291527027373\n","Epoch 583: Train.-Loss   = 10.865802159488956\n","Epoch 584: Train.-Loss   = 10.800705138751463\n","Epoch 585: Train.-Loss   = 10.735998114265191\n","Epoch 586: Train.-Loss   = 10.67167874956261\n","Epoch 587: Train.-Loss   = 10.607744722173992\n","Epoch 588: Train.-Loss   = 10.544193723543438\n","Epoch 589: Train.-Loss   = 10.481023458945707\n","Epoch 590: Train.-Loss   = 10.418231647403164\n","Epoch 591: Train.-Loss   = 10.355816021603559\n","Epoch 592: Train.-Loss   = 10.29377432781813\n","Epoch 593: Train.-Loss   = 10.232104325820162\n","Epoch 594: Train.-Loss   = 10.170803788804172\n","Epoch 595: Train.-Loss   = 10.109870503305448\n","Epoch 596: Train.-Loss   = 10.049302269120158\n","Epoch 597: Train.-Loss   = 9.989096899225842\n","Epoch 598: Train.-Loss   = 9.929252219702592\n","Epoch 599: Train.-Loss   = 9.869766069654359\n","Epoch 600: Train.-Loss   = 9.810636301131055\n","Epoch 601: Train.-Loss   = 9.751860779050977\n","Epoch 602: Train.-Loss   = 9.693437381123685\n","Epoch 603: Train.-Loss   = 9.635363997773371\n","Epoch 604: Train.-Loss   = 9.577638532062693\n","Epoch 605: Train.-Loss   = 9.520258899617119\n","Epoch 606: Train.-Loss   = 9.463223028549505\n","Epoch 607: Train.-Loss   = 9.406528859385466\n","Epoch 608: Train.-Loss   = 9.350174344988888\n","Epoch 609: Train.-Loss   = 9.294157450488049\n","Epoch 610: Train.-Loss   = 9.238476153202187\n","Epoch 611: Train.-Loss   = 9.183128442568337\n","Epoch 612: Train.-Loss   = 9.128112320068917\n","Epoch 613: Train.-Loss   = 9.07342579915939\n","Epoch 614: Train.-Loss   = 9.019066905196617\n","Epoch 615: Train.-Loss   = 8.965033675367595\n","Epoch 616: Train.-Loss   = 8.911324158618458\n","Epoch 617: Train.-Loss   = 8.857936415584168\n","Epoch 618: Train.-Loss   = 8.804868518518408\n","Epoch 619: Train.-Loss   = 8.752118551223978\n","Epoch 620: Train.-Loss   = 8.699684608983585\n","Epoch 621: Train.-Loss   = 8.647564798491166\n","Epoch 622: Train.-Loss   = 8.5957572377834\n","Epoch 623: Train.-Loss   = 8.544260056171828\n","Epoch 624: Train.-Loss   = 8.493071394175317\n","Epoch 625: Train.-Loss   = 8.442189403452806\n","Epoch 626: Train.-Loss   = 8.391612246736731\n","Epoch 627: Train.-Loss   = 8.341338097766519\n","Epoch 628: Train.-Loss   = 8.291365141222816\n","Epoch 629: Train.-Loss   = 8.241691572661749\n","Epoch 630: Train.-Loss   = 8.192315598449927\n","Epoch 631: Train.-Loss   = 8.143235435699616\n","Epoch 632: Train.-Loss   = 8.094449312204352\n","Epoch 633: Train.-Loss   = 8.045955466374929\n","Epoch 634: Train.-Loss   = 7.9977521471758815\n","Epoch 635: Train.-Loss   = 7.9498376140621465\n","Epoch 636: Train.-Loss   = 7.9022101369162865\n","Epoch 637: Train.-Loss   = 7.85486799598604\n","Epoch 638: Train.-Loss   = 7.8078094818220745\n","Epoch 639: Train.-Loss   = 7.761032895216504\n","Epoch 640: Train.-Loss   = 7.714536547141249\n","Epoch 641: Train.-Loss   = 7.668318758687337\n","Epoch 642: Train.-Loss   = 7.622377861004036\n","Epoch 643: Train.-Loss   = 7.5767121952387715\n","Epoch 644: Train.-Loss   = 7.531320112477082\n","Epoch 645: Train.-Loss   = 7.486199973683238\n","Epoch 646: Train.-Loss   = 7.441350149640909\n","Epoch 647: Train.-Loss   = 7.396769020894411\n","Epoch 648: Train.-Loss   = 7.352454977690234\n","Epoch 649: Train.-Loss   = 7.308406419918894\n","Epoch 650: Train.-Loss   = 7.264621757057156\n","Epoch 651: Train.-Loss   = 7.221099408110633\n","Epoch 652: Train.-Loss   = 7.177837801556644\n","Epoch 653: Train.-Loss   = 7.1348353752875155\n","Epoch 654: Train.-Loss   = 7.092090576554173\n","Epoch 655: Train.-Loss   = 7.049601861910041\n","Epoch 656: Train.-Loss   = 7.007367697155333\n","Epoch 657: Train.-Loss   = 6.96538655728169\n","Epoch 658: Train.-Loss   = 6.923656926416998\n","Epoch 659: Train.-Loss   = 6.882177297770846\n","Epoch 660: Train.-Loss   = 6.8409461735798995\n","Epoch 661: Train.-Loss   = 6.799962065053976\n","Epoch 662: Train.-Loss   = 6.759223492322233\n","Epoch 663: Train.-Loss   = 6.718728984379738\n","Epoch 664: Train.-Loss   = 6.678477079034321\n","Epoch 665: Train.-Loss   = 6.638466322853832\n","Epoch 666: Train.-Loss   = 6.598695271113609\n","Epoch 667: Train.-Loss   = 6.559162487744366\n","Epoch 668: Train.-Loss   = 6.519866545280289\n","Epoch 669: Train.-Loss   = 6.480806024807519\n","Epoch 670: Train.-Loss   = 6.441979515912895\n","Epoch 671: Train.-Loss   = 6.40338561663305\n","Epoch 672: Train.-Loss   = 6.365022933403809\n","Epoch 673: Train.-Loss   = 6.326890081009779\n","Epoch 674: Train.-Loss   = 6.288985682534452\n","Epoch 675: Train.-Loss   = 6.251308369310398\n","Epoch 676: Train.-Loss   = 6.213856780869873\n","Epoch 677: Train.-Loss   = 6.176629564895657\n","Epoch 678: Train.-Loss   = 6.139625377172385\n","Epoch 679: Train.-Loss   = 6.1028428815377325\n","Epoch 680: Train.-Loss   = 6.066280749834436\n","Epoch 681: Train.-Loss   = 6.029937661862183\n","Epoch 682: Train.-Loss   = 5.993812305329963\n","Epoch 683: Train.-Loss   = 5.957903375808741\n","Epoch 684: Train.-Loss   = 5.922209576684269\n","Epoch 685: Train.-Loss   = 5.886729619110347\n","Epoch 686: Train.-Loss   = 5.8514622219622625\n","Epoch 687: Train.-Loss   = 5.816406111790493\n","Epoch 688: Train.-Loss   = 5.7815600227747534\n","Epoch 689: Train.-Loss   = 5.746922696678314\n","Epoch 690: Train.-Loss   = 5.712492882802501\n","Epoch 691: Train.-Loss   = 5.678269337941642\n","Epoch 692: Train.-Loss   = 5.644250826338027\n","Epoch 693: Train.-Loss   = 5.610436119637426\n","Epoch 694: Train.-Loss   = 5.576823996844687\n","Epoch 695: Train.-Loss   = 5.543413244279586\n","Epoch 696: Train.-Loss   = 5.510202655533124\n","Epoch 697: Train.-Loss   = 5.477191031423818\n","Epoch 698: Train.-Loss   = 5.444377179954555\n","Epoch 699: Train.-Loss   = 5.411759916269442\n","Epoch 700: Train.-Loss   = 5.379338062611072\n","Epoch 701: Train.-Loss   = 5.347110448277975\n","Epoch 702: Train.-Loss   = 5.3150759095823465\n","Epoch 703: Train.-Loss   = 5.283233289808036\n","Epoch 704: Train.-Loss   = 5.2515814391687865\n","Epoch 705: Train.-Loss   = 5.220119214766741\n","Epoch 706: Train.-Loss   = 5.188845480551061\n","Epoch 707: Train.-Loss   = 5.157759107277085\n","Epoch 708: Train.-Loss   = 5.126858972465391\n","Epoch 709: Train.-Loss   = 5.096143960361342\n","Epoch 710: Train.-Loss   = 5.065612961894817\n","Epoch 711: Train.-Loss   = 5.035264874640118\n","Epoch 712: Train.-Loss   = 5.005098602776149\n","Epoch 713: Train.-Loss   = 4.975113057046911\n","Epoch 714: Train.-Loss   = 4.945307154722156\n","Epoch 715: Train.-Loss   = 4.9156798195582025\n","Epoch 716: Train.-Loss   = 4.886229981759239\n","Epoch 717: Train.-Loss   = 4.856956577938518\n","Epoch 718: Train.-Loss   = 4.827858551080093\n","Epoch 719: Train.-Loss   = 4.798934850500566\n","Epoch 720: Train.-Loss   = 4.77018443181123\n","Epoch 721: Train.-Loss   = 4.74160625688024\n","Epoch 722: Train.-Loss   = 4.713199293795261\n","Epoch 723: Train.-Loss   = 4.684962516826139\n","Epoch 724: Train.-Loss   = 4.656894906387822\n","Epoch 725: Train.-Loss   = 4.628995449003652\n","Epoch 726: Train.-Loss   = 4.601263137268685\n","Epoch 727: Train.-Loss   = 4.573696969813295\n","Epoch 728: Train.-Loss   = 4.546295951267157\n","Epoch 729: Train.-Loss   = 4.519059092223111\n","Epoch 730: Train.-Loss   = 4.491985409201597\n","Epoch 731: Train.-Loss   = 4.46507392461507\n","Epoch 732: Train.-Loss   = 4.438323666732699\n","Epoch 733: Train.-Loss   = 4.411733669645302\n","Epoch 734: Train.-Loss   = 4.385302973230459\n","Epoch 735: Train.-Loss   = 4.359030623117832\n","Epoch 736: Train.-Loss   = 4.332915670654737\n","Epoch 737: Train.-Loss   = 4.306957172871835\n","Epoch 738: Train.-Loss   = 4.281154192449151\n","Epoch 739: Train.-Loss   = 4.255505797682204\n","Epoch 740: Train.-Loss   = 4.230011062448286\n","Epoch 741: Train.-Loss   = 4.204669066173159\n","Epoch 742: Train.-Loss   = 4.1794788937977145\n","Epoch 743: Train.-Loss   = 4.154439635744978\n","Epoch 744: Train.-Loss   = 4.129550387887231\n","Epoch 745: Train.-Loss   = 4.104810251513394\n","Epoch 746: Train.-Loss   = 4.080218333296587\n","Epoch 747: Train.-Loss   = 4.055773745261797\n","Epoch 748: Train.-Loss   = 4.031475604753942\n","Epoch 749: Train.-Loss   = 4.0073230344058555\n","Epoch 750: Train.-Loss   = 3.9833151621067313\n","Epoch 751: Train.-Loss   = 3.959451120970556\n","Epoch 752: Train.-Loss   = 3.9357300493048295\n","Epoch 753: Train.-Loss   = 3.9121510905794428\n","Epoch 754: Train.-Loss   = 3.8887133933957765\n","Epoch 755: Train.-Loss   = 3.8654161114559495\n","Epoch 756: Train.-Loss   = 3.842258403532224\n","Epoch 757: Train.-Loss   = 3.819239433436663\n","Epoch 758: Train.-Loss   = 3.7963583699909442\n","Epoch 759: Train.-Loss   = 3.773614386996318\n","Epoch 760: Train.-Loss   = 3.751006663203834\n","Epoch 761: Train.-Loss   = 3.7285343822845802\n","Epoch 762: Train.-Loss   = 3.7061967328003114\n","Epoch 763: Train.-Loss   = 3.6839929081741114\n","Epoch 764: Train.-Loss   = 3.661922106661228\n","Epoch 765: Train.-Loss   = 3.6399835313202353\n","Epoch 766: Train.-Loss   = 3.6181763899840846\n","Epoch 767: Train.-Loss   = 3.5964998952316862\n","Epoch 768: Train.-Loss   = 3.5749532643593556\n","Epoch 769: Train.-Loss   = 3.553535719352567\n","Epoch 770: Train.-Loss   = 3.5322464868579324\n","Epoch 771: Train.-Loss   = 3.511084798155171\n","Epoch 772: Train.-Loss   = 3.4900498891294176\n","Epoch 773: Train.-Loss   = 3.469141000243645\n","Epoch 774: Train.-Loss   = 3.448357376511188\n","Epoch 775: Train.-Loss   = 3.4276982674685\n","Epoch 776: Train.-Loss   = 3.407162927148099\n","Epoch 777: Train.-Loss   = 3.3867506140515617\n","Epoch 778: Train.-Loss   = 3.3664605911227805\n","Epoch 779: Train.-Loss   = 3.346292125721364\n","Epoch 780: Train.-Loss   = 3.326244489596164\n","Epoch 781: Train.-Loss   = 3.3063169588589956\n","Epoch 782: Train.-Loss   = 3.2865088139584717\n","Epoch 783: Train.-Loss   = 3.2668193396540524\n","Epoch 784: Train.-Loss   = 3.247247824990184\n","Epoch 785: Train.-Loss   = 3.227793563270672\n","Epoch 786: Train.-Loss   = 3.208455852033109\n","Epoch 787: Train.-Loss   = 3.1892339930235805\n","Epoch 788: Train.-Loss   = 3.170127292171377\n","Epoch 789: Train.-Loss   = 3.151135059563969\n","Epoch 790: Train.-Loss   = 3.132256609422121\n","Epoch 791: Train.-Loss   = 3.1134912600750737\n","Epoch 792: Train.-Loss   = 3.094838333935954\n","Epoch 793: Train.-Loss   = 3.0762971574773585\n","Epoch 794: Train.-Loss   = 3.057867061206912\n","Epoch 795: Train.-Loss   = 3.03954737964322\n","Epoch 796: Train.-Loss   = 3.0213374512917732\n","Epoch 797: Train.-Loss   = 3.003236618621088\n","Epoch 798: Train.-Loss   = 2.9852442280389297\n","Epoch 799: Train.-Loss   = 2.9673596298687444\n","Epoch 800: Train.-Loss   = 2.9495821783262013\n","Epoch 801: Train.-Loss   = 2.9319112314958495\n","Epoch 802: Train.-Loss   = 2.914346151307971\n","Epoch 803: Train.-Loss   = 2.8968863035154815\n","Epoch 804: Train.-Loss   = 2.879531057671124\n","Epoch 805: Train.-Loss   = 2.862279787104614\n","Epoch 806: Train.-Loss   = 2.8451318689000713\n","Epoch 807: Train.-Loss   = 2.8280866838734906\n","Epoch 808: Train.-Loss   = 2.8111436165504116\n","Epoch 809: Train.-Loss   = 2.794302055143648\n","Epoch 810: Train.-Loss   = 2.7775613915312904\n","Epoch 811: Train.-Loss   = 2.7609210212346307\n","Epoch 812: Train.-Loss   = 2.74438034339641\n","Epoch 813: Train.-Loss   = 2.7279387607591286\n","Epoch 814: Train.-Loss   = 2.7115956796434135\n","Epoch 815: Train.-Loss   = 2.695350509926676\n","Epoch 816: Train.-Loss   = 2.679202665021705\n","Epoch 817: Train.-Loss   = 2.6631515618555484\n","Epoch 818: Train.-Loss   = 2.6471966208484834\n","Epoch 819: Train.-Loss   = 2.6313372658929657\n","Epoch 820: Train.-Loss   = 2.615572924333007\n","Epoch 821: Train.-Loss   = 2.5999030269433336\n","Epoch 822: Train.-Loss   = 2.5843270079089096\n","Epoch 823: Train.-Loss   = 2.568844304804532\n","Epoch 824: Train.-Loss   = 2.553454358574453\n","Epoch 825: Train.-Loss   = 2.538156613512235\n","Epoch 826: Train.-Loss   = 2.5229505172406776\n","Epoch 827: Train.-Loss   = 2.5078355206918923\n","Epoch 828: Train.-Loss   = 2.4928110780874184\n","Epoch 829: Train.-Loss   = 2.4778766469186064\n","Epoch 830: Train.-Loss   = 2.463031687926908\n","Epoch 831: Train.-Loss   = 2.4482756650845316\n","Epoch 832: Train.-Loss   = 2.4336080455750158\n","Epoch 833: Train.-Loss   = 2.419028299773976\n","Epoch 834: Train.-Loss   = 2.404535901230035\n","Epoch 835: Train.-Loss   = 2.39013032664576\n","Epoch 836: Train.-Loss   = 2.3758110558588346\n","Epoch 837: Train.-Loss   = 2.3615775718231857\n","Epoch 838: Train.-Loss   = 2.3474293605903935\n","Epoch 839: Train.-Loss   = 2.333365911291092\n","Epoch 840: Train.-Loss   = 2.3193867161165516\n","Epoch 841: Train.-Loss   = 2.3054912703003025\n","Epoch 842: Train.-Loss   = 2.2916790720999245\n","Epoch 843: Train.-Loss   = 2.2779496227789777\n","Epoch 844: Train.-Loss   = 2.264302426588914\n","Epoch 845: Train.-Loss   = 2.2507369907512222\n","Epoch 846: Train.-Loss   = 2.2372528254396324\n","Epoch 847: Train.-Loss   = 2.2238494437624245\n","Epoch 848: Train.-Loss   = 2.2105263617448383\n","Epoch 849: Train.-Loss   = 2.19728309831163\n","Epoch 850: Train.-Loss   = 2.1841191752696436\n","Epoch 851: Train.-Loss   = 2.171034117290601\n","Epoch 852: Train.-Loss   = 2.1580274518939158\n","Epoch 853: Train.-Loss   = 2.1450987094296217\n","Epoch 854: Train.-Loss   = 2.132247423061427\n","Epoch 855: Train.-Loss   = 2.1194731287498683\n","Epoch 856: Train.-Loss   = 2.106775365235524\n","Epoch 857: Train.-Loss   = 2.0941536740224036\n","Epoch 858: Train.-Loss   = 2.0816075993613317\n","Epoch 859: Train.-Loss   = 2.0691366882335633\n","Epoch 860: Train.-Loss   = 2.0567404903343496\n","Epoch 861: Train.-Loss   = 2.044418558056753\n","Epoch 862: Train.-Loss   = 2.0321704464754426\n","Epoch 863: Train.-Loss   = 2.0199957133306063\n","Epoch 864: Train.-Loss   = 2.0078939190120457\n","Epoch 865: Train.-Loss   = 1.9958646265432431\n","Epoch 866: Train.-Loss   = 1.9839074015656206\n","Epoch 867: Train.-Loss   = 1.9720218123228421\n","Epoch 868: Train.-Loss   = 1.9602074296452145\n","Epoch 869: Train.-Loss   = 1.9484638269342105\n","Epoch 870: Train.-Loss   = 1.9367905801470509\n","Epoch 871: Train.-Loss   = 1.9251872677813877\n","Epoch 872: Train.-Loss   = 1.9136534708601127\n","Epoch 873: Train.-Loss   = 1.902188772916189\n","Epoch 874: Train.-Loss   = 1.8907927599776455\n","Epoch 875: Train.-Loss   = 1.8794650205526224\n","Epoch 876: Train.-Loss   = 1.8682051456144915\n","Epoch 877: Train.-Loss   = 1.8570127285871127\n","Epoch 878: Train.-Loss   = 1.8458873653301475\n","Epoch 879: Train.-Loss   = 1.8348286541244558\n","Epoch 880: Train.-Loss   = 1.8238361956575884\n","Epoch 881: Train.-Loss   = 1.8129095930094026\n","Epoch 882: Train.-Loss   = 1.8020484516376842\n","Epoch 883: Train.-Loss   = 1.7912523793639281\n","Epoch 884: Train.-Loss   = 1.7805209863591562\n","Epoch 885: Train.-Loss   = 1.7698538851298729\n","Epoch 886: Train.-Loss   = 1.759250690504064\n","Epoch 887: Train.-Loss   = 1.7487110196172506\n","Epoch 888: Train.-Loss   = 1.7382344918987254\n","Epoch 889: Train.-Loss   = 1.7278207290577658\n","Epoch 890: Train.-Loss   = 1.7174693550699758\n","Epoch 891: Train.-Loss   = 1.7071799961637515\n","Epoch 892: Train.-Loss   = 1.696952280806733\n","Epoch 893: Train.-Loss   = 1.6867858396924191\n","Epoch 894: Train.-Loss   = 1.676680305726822\n","Epoch 895: Train.-Loss   = 1.666635314015217\n","Epoch 896: Train.-Loss   = 1.6566505018489532\n","Epoch 897: Train.-Loss   = 1.6467255086923753\n","Epoch 898: Train.-Loss   = 1.6368599761698033\n","Epoch 899: Train.-Loss   = 1.6270535480525703\n","Epoch 900: Train.-Loss   = 1.6173058702461902\n","Epoch 901: Train.-Loss   = 1.6076165907775442\n","Epoch 902: Train.-Loss   = 1.5979853597822056\n","Epoch 903: Train.-Loss   = 1.5884118294917495\n","Epoch 904: Train.-Loss   = 1.5788956542212593\n","Epoch 905: Train.-Loss   = 1.5694364903568179\n","Epoch 906: Train.-Loss   = 1.5600339963430891\n","Epoch 907: Train.-Loss   = 1.5506878326709976\n","Epoch 908: Train.-Loss   = 1.541397661865472\n","Epoch 909: Train.-Loss   = 1.5321631484732305\n","Epoch 910: Train.-Loss   = 1.52298395905073\n","Epoch 911: Train.-Loss   = 1.5138597621520564\n","Epoch 912: Train.-Loss   = 1.5047902283170047\n","Epoch 913: Train.-Loss   = 1.4957750300591537\n","Epoch 914: Train.-Loss   = 1.4868138418540735\n","Epoch 915: Train.-Loss   = 1.4779063401275239\n","Epoch 916: Train.-Loss   = 1.4690522032438136\n","Epoch 917: Train.-Loss   = 1.4602511114941783\n","Epoch 918: Train.-Loss   = 1.4515027470852238\n","Epoch 919: Train.-Loss   = 1.4428067941274298\n","Epoch 920: Train.-Loss   = 1.434162938623813\n","Epoch 921: Train.-Loss   = 1.4255708684585193\n","Epoch 922: Train.-Loss   = 1.4170302733855775\n","Epoch 923: Train.-Loss   = 1.408540845017734\n","Epoch 924: Train.-Loss   = 1.400102276815233\n","Epoch 925: Train.-Loss   = 1.3917142640748328\n","Epoch 926: Train.-Loss   = 1.3833765039187669\n","Epoch 927: Train.-Loss   = 1.3750886952837849\n","Epoch 928: Train.-Loss   = 1.3668505389103345\n","Epoch 929: Train.-Loss   = 1.358661737331729\n","Epoch 930: Train.-Loss   = 1.350521994863371\n","Epoch 931: Train.-Loss   = 1.3424310175921381\n","Epoch 932: Train.-Loss   = 1.3343885133657454\n","Epoch 933: Train.-Loss   = 1.3263941917821647\n","Epoch 934: Train.-Loss   = 1.3184477641792014\n","Epoch 935: Train.-Loss   = 1.310548943624001\n","Epoch 936: Train.-Loss   = 1.302697444902749\n","Epoch 937: Train.-Loss   = 1.294892984510339\n","Epoch 938: Train.-Loss   = 1.2871352806401335\n","Epoch 939: Train.-Loss   = 1.2794240531738177\n","Epoch 940: Train.-Loss   = 1.271759023671255\n","Epoch 941: Train.-Loss   = 1.2641399153604411\n","Epoch 942: Train.-Loss   = 1.2565664531275171\n","Epoch 943: Train.-Loss   = 1.2490383635068325\n","Epoch 944: Train.-Loss   = 1.2415553746710657\n","Epoch 945: Train.-Loss   = 1.2341172164214096\n","Epoch 946: Train.-Loss   = 1.2267236201778304\n","Epoch 947: Train.-Loss   = 1.2193743189693433\n","Epoch 948: Train.-Loss   = 1.212069047424392\n","Epoch 949: Train.-Loss   = 1.2048075417612683\n","Epoch 950: Train.-Loss   = 1.197589539778578\n","Epoch 951: Train.-Loss   = 1.1904147808457661\n","Epoch 952: Train.-Loss   = 1.1832830058937158\n","Epoch 953: Train.-Loss   = 1.1761939574054119\n","Epoch 954: Train.-Loss   = 1.1691473794065896\n","Epoch 955: Train.-Loss   = 1.1621430174565675\n","Epoch 956: Train.-Loss   = 1.1551806186389795\n","Epoch 957: Train.-Loss   = 1.1482599315527156\n","Epoch 958: Train.-Loss   = 1.1413807063027905\n","Epoch 959: Train.-Loss   = 1.1345426944913302\n","Epoch 960: Train.-Loss   = 1.127745649208636\n","Epoch 961: Train.-Loss   = 1.1209893250242289\n","Epoch 962: Train.-Loss   = 1.1142734779780048\n","Epoch 963: Train.-Loss   = 1.1075978655714405\n","Epoch 964: Train.-Loss   = 1.1009622467588067\n","Epoch 965: Train.-Loss   = 1.094366381938469\n","Epoch 966: Train.-Loss   = 1.0878100329442766\n","Epoch 967: Train.-Loss   = 1.0812929630369048\n","Epoch 968: Train.-Loss   = 1.0748149368953503\n","Epoch 969: Train.-Loss   = 1.068375720608411\n","Epoch 970: Train.-Loss   = 1.0619750816662439\n","Epoch 971: Train.-Loss   = 1.0556127889519786\n","Epoch 972: Train.-Loss   = 1.0492886127333743\n","Epoch 973: Train.-Loss   = 1.0430023246544855\n","Epoch 974: Train.-Loss   = 1.0367536977274818\n","Epoch 975: Train.-Loss   = 1.0305425063243956\n","Epoch 976: Train.-Loss   = 1.024368526169\n","Epoch 977: Train.-Loss   = 1.018231534328724\n","Epoch 978: Train.-Loss   = 1.0121313092065527\n","Epoch 979: Train.-Loss   = 1.006067630533094\n","Epoch 980: Train.-Loss   = 1.0000402793585714\n","Epoch 981: Train.-Loss   = 0.9940490380449343\n","Epoch 982: Train.-Loss   = 0.9880936902580054\n","Epoch 983: Train.-Loss   = 0.9821740209596765\n","Epoch 984: Train.-Loss   = 0.9762898164001048\n","Epoch 985: Train.-Loss   = 0.9704408641100472\n","Epoch 986: Train.-Loss   = 0.9646269528931681\n","Epoch 987: Train.-Loss   = 0.9588478728183865\n","Epoch 988: Train.-Loss   = 0.9531034152123348\n","Epoch 989: Train.-Loss   = 0.9473933726517941\n","Epoch 990: Train.-Loss   = 0.9417175389562347\n","Epoch 991: Train.-Loss   = 0.9360757091803444\n","Epoch 992: Train.-Loss   = 0.9304676796066494\n","Epoch 993: Train.-Loss   = 0.9248932477381218\n","Epoch 994: Train.-Loss   = 0.9193522122909269\n","Epoch 995: Train.-Loss   = 0.9138443731870896\n","Epoch 996: Train.-Loss   = 0.9083695315473252\n","Epoch 997: Train.-Loss   = 0.9029274896838312\n","Epoch 998: Train.-Loss   = 0.8975180510931268\n","Epoch 999: Train.-Loss   = 0.8921410204490294\n","Epoch 1000: Train.-Loss   = 0.8867962035955237\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["0.8867962035955237"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"GcAhaql2Rz8e","colab_type":"text"},"source":["# Check with Keras (optional todo move somewhere else)"]},{"cell_type":"code","metadata":{"id":"mEnv2_pNSV2R","colab_type":"code","outputId":"a7735716-a800-46ac-b7b5-48b4b20ef67f","executionInfo":{"status":"ok","timestamp":1567863394917,"user_tz":-120,"elapsed":21760,"user":{"displayName":"Dieter Greipl","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCQkt4LlXZTbr43bodLOYJA_AMigYzivO_n1PBIcw=s64","userId":"09839720196215486633"}},"colab":{"base_uri":"https://localhost:8080/","height":602}},"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Activation\n","from tensorflow.keras.callbacks import EarlyStopping\n","\n","import pandas as pd\n","import numpy as np\n","\n","\n","import matplotlib.pyplot as plt\n","\n","def plot_history(history):\n","  hist = pd.DataFrame(history.history)\n","  hist['epoch'] = history.epoch\n","\n","  plt.figure()\n","  plt.xlabel('Epoch')\n","  plt.ylabel('Mean Square Error [MSE]')\n","  plt.plot(hist['epoch'], hist['loss'],\n","           label='Train Error')\n","\n","  plt.ylim([0,1])\n","  plt.legend()\n","  plt.show()\n","\n","\n","X  = np.array( [[0.,1., 2.0], [0.,1., 2.0]] ).T \n","y_true = np.array( [[-10., -11., -15.0]] ).T\n","print(f\"X Shape : { X.shape}\" )\n","print(f\"Y_true Shape : { y_true.shape}\" )\n","\n","\n","model = Sequential()\n","model.add(Dense(2, input_dim=2, activation='relu')) # Hidden 1\n","model.add(Dense(1)) # Output\n","model.compile(loss='mean_squared_error', optimizer='adam')\n","model.summary()\n","\n","w0 = []\n","w0.append(np.array([[1,1],[1,1]]))  # Weight Vectors in rows\n","w0.append(np.array( [1,1] ))        # Bias\n","#model.layers[0].set_weights( w0 )\n","\n","w1 = []\n","w1.append(np.array([[1],[1]]))\n","w1.append(np.array( [1] ))  \n","#model.layers[1].set_weights( w1 )\n","\n","print( model.layers[1].get_weights() )\n","\n","\n","history = model.fit(X,y_true,verbose=0,epochs=20000)\n","\n","plot_history( history )\n","\n","y = model.predict( X )\n","print( y )\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["X Shape : (3, 2)\n","Y_true Shape : (3, 1)\n","Model: \"sequential_3\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_6 (Dense)              (None, 2)                 6         \n","_________________________________________________________________\n","dense_7 (Dense)              (None, 1)                 3         \n","=================================================================\n","Total params: 9\n","Trainable params: 9\n","Non-trainable params: 0\n","_________________________________________________________________\n","[array([[-0.6689986],\n","       [ 1.2921003]], dtype=float32), array([0.], dtype=float32)]\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJEVJQUREUv4/SQDmpkyDsAwAAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["[[ -9.5]\n"," [-12. ]\n"," [-14.5]]\n"],"name":"stdout"}]}]}